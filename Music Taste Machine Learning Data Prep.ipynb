{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d392a2d-8247-40dc-9431-ae29941dc3ea",
   "metadata": {},
   "source": [
    "# Music Taste Prediction Model: New Music Friday Recommender üíøüéßüëçüëé\n",
    "In this model, I use my liked songs playlist, my recently loved and not loved albums, to train my regression model on what kind of music I do and don't like. At the end my test model will be the new music friday albums from the most recent Friday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e436444-4aff-431a-aedc-e7871c247d1e",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160a9a1f-e9f9-4fb7-b0e3-dcc706dba067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Third-Party Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# Fuzzy Matching\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Network Analysis\n",
    "import networkx as nx\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "from nbconvert import HTMLExporter\n",
    "import nbformat\n",
    "\n",
    "# Streamlit \n",
    "import streamlit as st\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42454624-15b3-4b80-bab9-a8ecbf9d299b",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8067f0c-b06b-4426-bad9-69bd28e3aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liked = pd.read_csv(\"data/liked.csv\")  # Liked playlist on Spotify\n",
    "df_fav_albums = pd.read_csv(\"data/liked_albums.csv\")  # Albums I've Liked in Recent Years\n",
    "df_not_liked = pd.read_csv(\"data/did_not_like.csv\")  # Albums I've not liked in Recent Years\n",
    "df_nmf = pd.read_csv(\"data/nmf.csv\")  # The most recent New Music Friday Playlist\n",
    "df_mid = pd.read_csv(\"data/mid.csv\") #Albums I neither enjoyed or hated, but were simply Mid\n",
    "df_liked_similar = pd.read_csv('data/liked_artists_only_similar.csv') #Similar Artisits to My Liked Artists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b112f9a-264e-4da1-87fd-ffe68974dd78",
   "metadata": {},
   "source": [
    "## A Check for New Artists / Pull Their Similar Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c1adc-0020-425f-89fa-56dee6f455e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LastFM API client with rate limiting and request limits\n",
    "# This ensures we don't overload the API and stay within usage guidelines ‚è≥\n",
    "class LastFMAPI:\n",
    "    def __init__(self, api_key: str, rate_limit_delay: float = 0.25, limit: int = 8):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"http://ws.audioscrobbler.com/2.0/\"\n",
    "        self.rate_limit_delay = rate_limit_delay\n",
    "        self.limit = limit\n",
    "\n",
    "    # Fetch similar artists for a given artist from LastFM API\n",
    "    # This is the core function that interacts with the API üé§\n",
    "    def get_similar_artists(self, artist_name: str) -> List[str]:\n",
    "        params = {\n",
    "            'method': 'artist.getSimilar',\n",
    "            'artist': artist_name,\n",
    "            'api_key': self.api_key,\n",
    "            'limit': self.limit,  # Limit the number of similar artists returned üéØ\n",
    "            'format': 'json'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(self.base_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Handle rate limiting to avoid API bans ‚ö†Ô∏è\n",
    "            if 'X-RateLimit-Remaining' in response.headers:\n",
    "                remaining = int(response.headers['X-RateLimit-Remaining'])\n",
    "                if remaining == 0:\n",
    "                    sleep(self.rate_limit_delay)\n",
    "            \n",
    "            # Extract and return similar artists from the API response üé∂\n",
    "            data = response.json()\n",
    "            if 'similarartists' in data and 'artist' in data['similarartists']:\n",
    "                return [artist['name'] for artist in data['similarartists']['artist'][:self.limit]]\n",
    "            return []\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching similar artists for {artist_name}: {e}\")\n",
    "            return []\n",
    "\n",
    "# Extract the primary artist name from a string\n",
    "# This handles cases where multiple artists are listed (e.g., \"Artist A, Artist B\") üé§\n",
    "def extract_primary_artist(artist_string: str) -> str:\n",
    "    if pd.isna(artist_string):\n",
    "        return \"\"\n",
    "    return artist_string.split(\",\")[0].strip()\n",
    "\n",
    "# Update the similar artists database with new artists from liked playlists\n",
    "# This function ensures the database stays up-to-date with my latest music preferences üîÑ\n",
    "def update_similar_artists(liked_path: str, \n",
    "                         albums_path: str, \n",
    "                         output_path: str, \n",
    "                         api_key: str) -> pd.DataFrame:\n",
    "    print(\"Loading existing and new data...\")\n",
    "    \n",
    "    # Load existing similar artists data\n",
    "    # This ensures we don't duplicate work for artists already in the database üìÇ\n",
    "    existing_data: Dict[str, List[str]] = {}\n",
    "    if os.path.exists(output_path):\n",
    "        existing_df = pd.read_csv(output_path)\n",
    "        existing_data = dict(zip(existing_df['Artist'], existing_df['Similar Artists']))\n",
    "        print(f\"Loaded {len(existing_data)} existing artists from database\")\n",
    "    \n",
    "    # Load and process current playlists\n",
    "    # This combines liked songs and albums into a single set of artists üéß\n",
    "    df_liked = pd.read_csv(liked_path)\n",
    "    df_albums = pd.read_csv(albums_path)\n",
    "    \n",
    "    # Extract and combine primary artists\n",
    "    current_artists = set(\n",
    "        pd.concat([\n",
    "            df_liked['Artist Name(s)'].apply(extract_primary_artist),\n",
    "            df_albums['Artist Name(s)'].apply(extract_primary_artist)\n",
    "        ]).unique()\n",
    "    )\n",
    "    current_artists.discard(\"\")  # Remove empty strings\n",
    "    \n",
    "    # Find new artists not in existing data\n",
    "    # This ensures we only process artists we haven't seen before ÔøΩ\n",
    "    new_artists = current_artists - set(existing_data.keys())\n",
    "    print(f\"Found {len(new_artists)} new artists to process\")\n",
    "    \n",
    "    if not new_artists:\n",
    "        print(\"No new artists to process. Database is up to date!\")\n",
    "        # Create and return DataFrame even if no updates\n",
    "        return pd.DataFrame({\n",
    "            'Artist': list(existing_data.keys()),\n",
    "            'Similar Artists': list(existing_data.values())\n",
    "        })\n",
    "    \n",
    "    # Initialize LastFM API client\n",
    "    # This is where the magic happens ‚ú®\n",
    "    api = LastFMAPI(api_key)\n",
    "    \n",
    "    # Process artists with concurrent requests\n",
    "    # This speeds up the process by fetching data in parallel ‚ö°\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_artist = {\n",
    "            executor.submit(api.get_similar_artists, artist): artist \n",
    "            for artist in new_artists\n",
    "        }\n",
    "        \n",
    "        # Show progress bar while processing\n",
    "        # This keeps you informed about the progress of the task üìä\n",
    "        for future in tqdm(as_completed(future_to_artist), \n",
    "                         total=len(future_to_artist),\n",
    "                         desc=\"Fetching similar artists\"):\n",
    "            artist = future_to_artist[future]\n",
    "            similar_artists = future.result()\n",
    "            results[artist] = ', '.join(similar_artists)\n",
    "    \n",
    "    # Combine existing and new data\n",
    "    # This ensures the final dataset includes all artists, old and new üîó\n",
    "    combined_data = {**existing_data, **results}\n",
    "    \n",
    "    # Create DataFrame\n",
    "    # This formats the data for easy saving and future use üìÑ\n",
    "    output_df = pd.DataFrame({\n",
    "        'Artist': list(combined_data.keys()),\n",
    "        'Similar Artists': list(combined_data.values())\n",
    "    })\n",
    "    \n",
    "    # Save updated data\n",
    "    # This ensures the database is persisted for future runs üíæ\n",
    "    output_df.to_csv(output_path, index=False)\n",
    "    print(f\"Successfully updated database with {len(new_artists)} new artists\")\n",
    "    print(f\"Total artists in database: {len(combined_data)}\")\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "# Main execution block\n",
    "# This runs the update process when the script is executed directly üöÄ\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    API_KEY = \"74a510ecc9fc62bf3e0edc6adc2e99f9\"\n",
    "    LIKED_PATH = \"data/liked.csv\"\n",
    "    ALBUMS_PATH = \"data/liked_albums.csv\"\n",
    "    OUTPUT_PATH = \"data/liked_artists_only_similar.csv\"\n",
    "    \n",
    "    # Run the update and get the DataFrame\n",
    "    df_liked_similar = update_similar_artists(\n",
    "        LIKED_PATH, \n",
    "        ALBUMS_PATH, \n",
    "        OUTPUT_PATH, \n",
    "        API_KEY\n",
    "    )\n",
    "    \n",
    "# Now df_liked_similar is ready to use\n",
    "df_liked_similar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392df38d-c607-4196-bf48-ab735338849e",
   "metadata": {},
   "source": [
    "## Quick Glance at our Refreshed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ec508-7fe2-43be-ac08-f08bb640aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80b875a-ebab-472e-82b5-85568f036a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liked Albums in Recent Years\n",
    "df_fav_albums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d6345-80d1-4b33-a4c5-10b1e138bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albums Not Liked in Recent Years\n",
    "df_not_liked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4a16f-a5d1-4a67-8091-49d5f29084b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Albums I neither enjoyed or hated, but were simply Mid\n",
    "df_mid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190ec91-73e0-40c5-95b4-fc212abd1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Music Friday Playlist\n",
    "df_nmf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bfbf76-1649-4542-b001-2fedd527ecd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Similar Artists to Recently Played Artists (Last.fm)\n",
    "\n",
    "df_liked_similar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c30386-5197-4245-9871-cffbe4952a33",
   "metadata": {},
   "source": [
    "> A quick reminder of the standard columns of a spotify export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f0c33-76f1-4d7b-95c3-988b1ca941a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liked.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694749b-c05c-45c8-8c17-6f30a6ffc54d",
   "metadata": {},
   "source": [
    "> What's available in the Similar dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe763b3-50c4-474d-8363-a5c41461ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liked_similar.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c25a6b9-e488-4c3a-9e27-4cf15e57c504",
   "metadata": {},
   "source": [
    "### Add Target Labels for Training Feature\n",
    "We need to assign a score to songs I've faved on spotify (100), albums I've enjoyed in recent years (65), albums that were mid (45), and albums that I have not enjoyed in recent years (30) to train the model on the types of songs I don't like, like, and love. 'liked' will be our target variable, later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a802d4-ff06-4dab-886d-059ed0ee2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign liked scores before combining\n",
    "df_liked['liked'] = 100\n",
    "df_fav_albums['liked'] = 65\n",
    "df_mid['liked'] = 45\n",
    "df_not_liked['liked'] = 30\n",
    "df_nmf['liked'] = np.nan \n",
    "\n",
    "# Add playlist_origin column before combining\n",
    "df_liked['playlist_origin'] = 'df_liked'\n",
    "df_fav_albums['playlist_origin'] = 'df_fav_albums'\n",
    "df_not_liked['playlist_origin'] = 'df_not_liked'\n",
    "df_mid['playlist_origin'] = 'df_mid'\n",
    "df_nmf['playlist_origin'] = 'df_nmf'\n",
    "df_liked_similar['source'] = 'liked_similar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b10e0ba-5881-4cd4-91e0-630063a72dd8",
   "metadata": {},
   "source": [
    "### Check application of the target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add9e6b-4a3c-46f0-bc43-4bb4215cd97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liked[['liked', 'playlist_origin']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cb7c2-b08e-4d00-9cd7-fcea6eec1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fav_albums[['liked', 'playlist_origin']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca431aa-e88b-4857-8d9a-4d09a71b3866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mid[['liked', 'playlist_origin']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b72fb6-dac5-4d75-b6fd-b3ca1d8973f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_liked[['liked', 'playlist_origin']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75164a9-8182-463f-a450-2534f52aeb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmf[['liked', 'playlist_origin']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40fa4a1-a55a-42c6-b3b1-8994b44c3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liked_similar[['Artist', 'Similar Artists', 'source']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4bb9c-a4ea-4286-bf9b-9a7575d9ffcf",
   "metadata": {},
   "source": [
    "## Data Cleaning üßπ\n",
    "Before diving into modeling, I need to clean and prepare the data to ensure it‚Äôs ready for analysis. This section focuses on merging my datasets, handling duplicates, filling missing values, and making sure the data is in the best possible shape. Here‚Äôs what I‚Äôll tackle:\n",
    "\n",
    "1. **Merging Datasets**: I‚Äôll combine my liked songs, favorite albums, not-liked albums, and New Music Friday tracks into a single dataframe.\n",
    "2. **Removing Duplicates**: I‚Äôll ensure each track is unique, prioritizing higher \"liked\" scores for duplicates.\n",
    "3. **Handling Missing Values**: I‚Äôll fill gaps in genres using Last.fm‚Äôs richer data and save a backup of missing artists for manual review later.\n",
    "4. **Dropping Irrelevant Columns**: I‚Äôll remove columns that won‚Äôt contribute to the model.\n",
    "5. **Final Checks**: I‚Äôll verify the dataset‚Äôs integrity and distribution before moving to modeling.\n",
    "\n",
    "By the end of this section, I‚Äôll have a clean, unified dataset ready for feature engineering and model training. Let‚Äôs get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44856ee-a472-41f3-8622-01d76e370849",
   "metadata": {},
   "source": [
    "## Merge The Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd66a5-2ed8-4eda-b8df-09a5feaa782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_liked, df_fav_albums, df_mid, df_not_liked, df_nmf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b1007-6bfe-429b-bf10-5de0241f9e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#How Large is the Dataset, Now?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035bf83-b7da-4fbb-bc26-f1ed816a5798",
   "metadata": {},
   "source": [
    "#### Remove the Duplicates üî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861de4ff-4fb7-421b-8923-419d750172bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates: Keep the highest 'liked' score (100 > 65)\n",
    "df = df.sort_values(by='liked', ascending=False)  # Ensures 100-rated songs come first\n",
    "df = df.drop_duplicates(subset=['Track Name', 'Artist Name(s)'], keep='first')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b2580a-e10a-4d8f-a9fc-258a2777d37f",
   "metadata": {},
   "source": [
    "#### Drop columns that won't help the model üí£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1551b-d6a6-4ff4-b5f3-f333f2e4e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Added By', 'Added At', 'Time Signature'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bff07c-0b26-47bf-af09-6c87b741ba13",
   "metadata": {},
   "source": [
    "#### Handle missing values (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7d185-71c3-4aa3-8b81-8920a76aa070",
   "metadata": {},
   "outputs": [],
   "source": [
    " df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41658750-d956-442d-99ed-2dca4323ec47",
   "metadata": {},
   "source": [
    "## Getting New Genre Data From Last.fm üü¢üéµ\n",
    "Since half of the genres are missing from Spotify songs, I decided to use the richer data from Last.fm's crowd-sourced genre tags. There are some silly tags in there, so I added a list of ignored tags that I update weekly based on what I see in the data. This ensures the genres are meaningful and relevant for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b250a-eb92-4eb4-a0a1-c20f0550eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastFMGenreFetcher:\n",
    "    def __init__(self, api_key: str, rate_limit_delay: float = 0.25):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"http://ws.audioscrobbler.com/2.0/\"\n",
    "        self.rate_limit_delay = rate_limit_delay\n",
    "        \n",
    "        # Define tags to ignore\n",
    "        # These are the crowd-sourced tags that are more about personal taste than actual genres\n",
    "        self.ignored_tags = {\n",
    "            'seen live',\n",
    "            'albums i own',\n",
    "            'favorite',\n",
    "            'favourites',\n",
    "            'favourite',\n",
    "            'my playlist',\n",
    "            'spotify',\n",
    "            'pandora',\n",
    "            'wish i had seen live',\n",
    "            'awesome',\n",
    "            'love at first listen',\n",
    "            'love',\n",
    "            'amazing',\n",
    "            'listened',\n",
    "            'personal',\n",
    "            'my music'\n",
    "        }\n",
    "        \n",
    "        # Define variations of female vocalist to standardize\n",
    "        # Because Last.fm users love to tag the same thing in 10 different ways üé§\n",
    "        self.female_vocalist_variants = {\n",
    "            'female vocalists',\n",
    "            'female vocalist',\n",
    "            'female vocals',\n",
    "            'female fronted',\n",
    "            'female voices',\n",
    "            'female voice',\n",
    "            'female singers',\n",
    "            'female singer'\n",
    "        }\n",
    "        \n",
    "        # Genre groups that typically appear together\n",
    "        self.genre_compatibility = {\n",
    "            'indie': ['indie pop', 'indie rock', 'indie folk', 'alternative', 'singer-songwriter'],\n",
    "            'rock': ['alternative rock', 'indie rock', 'hard rock', 'classic rock', 'punk'],\n",
    "            'electronic': ['techno', 'house', 'edm', 'ambient', 'idm', 'electronica'],\n",
    "            'metal': ['heavy metal', 'death metal', 'black metal', 'thrash metal', 'doom metal'],\n",
    "            'folk': ['indie folk', 'folk rock', 'americana', 'singer-songwriter', 'acoustic'],\n",
    "            'pop': ['indie pop', 'synth pop', 'dream pop', 'electropop', 'pop rock'],\n",
    "            'hip-hop': ['rap', 'trap', 'r&b', 'urban', 'grime'],\n",
    "            'jazz': ['fusion', 'bebop', 'smooth jazz', 'soul', 'funk'],\n",
    "            'classical': ['orchestral', 'chamber music', 'piano', 'instrumental', 'contemporary classical']\n",
    "        }\n",
    "        \n",
    "        # This identifies genres that rarely appear together\n",
    "        self.genre_incompatibility = {\n",
    "            'classical': ['metal', 'rap', 'edm', 'techno', 'dubstep'],\n",
    "            'death metal': ['pop', 'r&b', 'jazz', 'folk', 'ambient'],\n",
    "            'christian': ['black metal', 'satanic', 'pagan'],\n",
    "            'country': ['techno', 'edm', 'black metal', 'death metal'],\n",
    "            'jazz': ['black metal', 'death metal', 'hardcore', 'screamo'],\n",
    "            'k-pop': ['death metal', 'black metal', 'doom metal']\n",
    "        }\n",
    "        \n",
    "    def _make_request(self, params: Dict)  -> Dict:\n",
    "        try:\n",
    "            response = requests.get(self.base_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            sleep(self.rate_limit_delay)  # Basic rate limiting to avoid angering the Last.fm API gods\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"API request failed for params {params}: {e}\")\n",
    "            if getattr(response, 'status_code', None) == 429:\n",
    "                print(\"Rate limit exceeded, increasing delay. Patience is a virtue!\")\n",
    "                self.rate_limit_delay *= 2\n",
    "                sleep(5)  # Wait before retry\n",
    "            return None\n",
    "\n",
    "    def get_artist_tags(self, artist_name: str) -> List[str]:\n",
    "        params = {\n",
    "            'method': 'artist.getTopTags',\n",
    "            'artist': artist_name,\n",
    "            'api_key': self.api_key,\n",
    "            'format': 'json'\n",
    "        }\n",
    "        data = self._make_request(params)\n",
    "        \n",
    "        if not data or 'toptags' not in data:\n",
    "            return []  # Sometimes Last.fm just doesn't have the data ü§∑‚Äç‚ôÇÔ∏è\n",
    "            \n",
    "        tags = []\n",
    "        seen_tags = set()  # Track unique tags to avoid duplicates\n",
    "        has_female_vocalist = False\n",
    "        \n",
    "        for tag in data['toptags'].get('tag', []):\n",
    "            tag_name = tag['name'].lower()\n",
    "            \n",
    "            # Skip if tag is in ignored list\n",
    "            if tag_name in self.ignored_tags:\n",
    "                continue  # Bye-bye, irrelevant tags!\n",
    "                \n",
    "            # Handle female vocalist variations\n",
    "            if tag_name in self.female_vocalist_variants:\n",
    "                if not has_female_vocalist:\n",
    "                    tags.insert(0, 'female vocalist')  # Standardize and prioritize this tag\n",
    "                    has_female_vocalist = True\n",
    "                continue\n",
    "            \n",
    "            # Only add tag if we haven't seen it before\n",
    "            if tag_name not in seen_tags:\n",
    "                tags.append(tag_name)\n",
    "                seen_tags.add(tag_name)\n",
    "                \n",
    "        return tags[:10]  # Return up to 10 tags because less is more, right?\n",
    "        \n",
    "    def get_album_tags(self, artist_name: str, album_name: str, release_date: str = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get tags specifically for an album, with verification against artist tags.\n",
    "        \"\"\"\n",
    "        # Try to get album-specific tags first\n",
    "        params = {\n",
    "            'method': 'album.getTopTags',\n",
    "            'artist': artist_name,\n",
    "            'album': album_name,\n",
    "            'api_key': self.api_key,\n",
    "            'format': 'json'\n",
    "        }\n",
    "        album_data = self._make_request(params)\n",
    "        \n",
    "        # Get artist tags for comparison\n",
    "        artist_tags = self.get_artist_tags(artist_name)\n",
    "        \n",
    "        # Extract album tags if available\n",
    "        album_tags = []\n",
    "        if album_data and 'toptags' in album_data and 'tag' in album_data['toptags']:\n",
    "            for tag in album_data['toptags'].get('tag', []):\n",
    "                tag_name = tag['name'].lower()\n",
    "                if tag_name not in self.ignored_tags:\n",
    "                    album_tags.append(tag_name)\n",
    "        \n",
    "        # This verifies tags when we have both album and artist data\n",
    "        if album_tags and artist_tags:\n",
    "            verified_tags = self._verify_tags(album_tags, artist_tags, release_date)\n",
    "            return verified_tags[:10]\n",
    "        elif album_tags:\n",
    "            return album_tags[:10]\n",
    "        else:\n",
    "            return artist_tags[:10]\n",
    "    \n",
    "    def _verify_tags(self, album_tags: List[str], artist_tags: List[str], release_date: str = None) -> List[str]:\n",
    "        # Convert to sets for easier comparison\n",
    "        album_tag_set = set(album_tags)\n",
    "        artist_tag_set = set(artist_tags)\n",
    "        \n",
    "        # Calculate overlap between tags\n",
    "        overlap = album_tag_set.intersection(artist_tag_set)\n",
    "        overlap_ratio = len(overlap) / len(album_tag_set) if album_tag_set else 0\n",
    "        \n",
    "        # This means we have good tag agreement\n",
    "        if overlap_ratio >= 0.3:\n",
    "            return album_tags\n",
    "        \n",
    "        # Check for incompatible genres\n",
    "        for tag in album_tags:\n",
    "            for incompatible_genre, incompatible_tags in self.genre_incompatibility.items():\n",
    "                if tag == incompatible_genre:\n",
    "                    for artist_tag in artist_tags:\n",
    "                        if artist_tag in incompatible_tags:\n",
    "                            # This indicates a likely mismatch\n",
    "                            return artist_tags\n",
    "        \n",
    "        # Use release date as tiebreaker if available\n",
    "        if release_date:\n",
    "            try:\n",
    "                year = int(release_date.split('-')[0])\n",
    "                current_year = datetime.now().year\n",
    "                \n",
    "                # This prioritizes album tags for recent releases\n",
    "                if year >= current_year - 2:\n",
    "                    merged_tags = album_tags[:7] + [tag for tag in artist_tags if tag not in album_tag_set][:3]\n",
    "                    return merged_tags\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Merge tags with preference to album tags\n",
    "        merged_tags = album_tags[:7] + [tag for tag in artist_tags if tag not in album_tag_set][:3]\n",
    "        return merged_tags\n",
    "\n",
    "def update_genre_data(api_key: str, dataframes: List[pd.DataFrame], output_file: str = 'data/ten_genres.csv') -> None:\n",
    "    api = LastFMGenreFetcher(api_key)\n",
    "    \n",
    "    # Cache for artist tags\n",
    "    artist_tags_cache = {}\n",
    "\n",
    "    def get_album_tags_cached(artist_name, album_name, release_date=None):\n",
    "        if artist_name not in artist_tags_cache:\n",
    "            artist_tags_cache[artist_name] = api.get_artist_tags(artist_name)\n",
    "        artist_tags = artist_tags_cache[artist_name]\n",
    "\n",
    "        params = {\n",
    "            'method': 'album.getTopTags',\n",
    "            'artist': artist_name,\n",
    "            'album': album_name,\n",
    "            'api_key': api.api_key,\n",
    "            'format': 'json'\n",
    "        }\n",
    "        album_data = api._make_request(params)\n",
    "        \n",
    "        album_tags = []\n",
    "        if album_data and 'toptags' in album_data and 'tag' in album_data['toptags']:\n",
    "            for tag in album_data['toptags']['tag']:\n",
    "                tag_name = tag['name'].lower()\n",
    "                if tag_name not in api.ignored_tags:\n",
    "                    album_tags.append(tag_name)\n",
    "        \n",
    "        if album_tags:\n",
    "            return api._verify_tags(album_tags, artist_tags, release_date)[:10]\n",
    "        else:\n",
    "            return artist_tags[:10]\n",
    "\n",
    "    # Gather all unique artist-album pairs\n",
    "    artist_album_pairs = []\n",
    "    for df in dataframes:\n",
    "        if 'Artist Name(s)' in df.columns and 'Album Name' in df.columns and 'Release Date' in df.columns:\n",
    "            pairs = df[['Artist Name(s)', 'Album Name', 'Release Date']].drop_duplicates()\n",
    "            for _, row in pairs.iterrows():\n",
    "                artist = row['Artist Name(s)'].split(',')[0].strip() if pd.notna(row['Artist Name(s)']) else None\n",
    "                album = row['Album Name'] if pd.notna(row['Album Name']) else None\n",
    "                release_date = row['Release Date'] if pd.notna(row['Release Date']) else None\n",
    "                if artist and album:\n",
    "                    artist_album_pairs.append((artist, album, release_date))\n",
    "\n",
    "    # Load existing data\n",
    "    existing_data = {}\n",
    "    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "        try:\n",
    "            existing_data = pd.read_csv(output_file).set_index(['Artist', 'Album'])['Genres'].to_dict()\n",
    "        except (pd.errors.EmptyDataError, KeyError):\n",
    "            print(\"No existing genre data found or file is empty. Starting fresh.\")\n",
    "\n",
    "    # Load obscure artist genres\n",
    "    obscure_artists = pd.read_csv(\"data/obscure_artists_mike_likes.csv\", quotechar='\"')\n",
    "    obscure_artists_dict = { (row['Artist'], 'ANY_ALBUM'): row['Genres'] for _, row in obscure_artists.iterrows() }\n",
    "    existing_data.update(obscure_artists_dict)\n",
    "\n",
    "    # Identify new artist-album pairs\n",
    "    new_pairs = [(artist, album, release_date) for artist, album, release_date in artist_album_pairs]\n",
    "\n",
    "    if not new_pairs:\n",
    "        print(\"No new artist-album pairs to process!\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(new_pairs)} new artist-album pairs to process.\")\n",
    "\n",
    "    # Process new pairs in parallel\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=25) as executor:\n",
    "        future_to_pair = {\n",
    "            executor.submit(get_album_tags_cached, artist, album, release_date): (artist, album)\n",
    "            for artist, album, release_date in new_pairs\n",
    "        }\n",
    "\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_pair):\n",
    "            artist, album = future_to_pair[future]\n",
    "            try:\n",
    "                tags = future.result()\n",
    "                if tags:\n",
    "                    results[(artist, album)] = ', '.join(tags)\n",
    "                completed += 1\n",
    "                if completed % 50 == 0 or (len(new_pairs) < 50 and completed % 10 == 0):\n",
    "                    print(f\"Processed {completed}/{len(new_pairs)} artist-album pairs.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {artist} - {album}: {e}\")\n",
    "\n",
    "    # Combine existing and new data\n",
    "    all_data = {**existing_data, **results}\n",
    "\n",
    "    # Save to CSV\n",
    "    df_output = pd.DataFrame(\n",
    "        [{'Artist': artist, 'Album': album, 'Genres': genres} for (artist, album), genres in all_data.items()]\n",
    "    )\n",
    "    df_output.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSuccessfully updated {output_file} with {len(results)} new artist-album pairs!\")\n",
    "\n",
    "\n",
    "# Define genre cleaning function\n",
    "def clean_genres(genre_string):\n",
    "    \"\"\"\n",
    "    Clean genre strings by:\n",
    "    1. Removing duplicates\n",
    "    2. Removing '|' bars\n",
    "    3. Filtering out genres with numbers\n",
    "    4. Filtering out genres longer than 3 words\n",
    "    \"\"\"\n",
    "    if pd.isna(genre_string):\n",
    "        return genre_string\n",
    "        \n",
    "    # Split by | and flatten all genres into one list\n",
    "    all_parts = [part.strip() for part in genre_string.split('|')]\n",
    "    all_genres = []\n",
    "    for part in all_parts:\n",
    "        genres = [g.strip() for g in part.split(',')]\n",
    "        all_genres.extend(genres)\n",
    "    \n",
    "    # Remove duplicates, numbers, and long genres while preserving order\n",
    "    seen = set()\n",
    "    unique_genres = []\n",
    "    for genre in all_genres:\n",
    "        # Skip if empty\n",
    "        if not genre:\n",
    "            continue\n",
    "        \n",
    "        # Skip if contains digits\n",
    "        if any(char.isdigit() for char in genre):\n",
    "            continue\n",
    "            \n",
    "        # Skip if longer than 3 words\n",
    "        if len(genre.split()) > 3:\n",
    "            continue\n",
    "            \n",
    "        # Add if not seen before (case-insensitive check)\n",
    "        if genre.lower() not in seen:\n",
    "            unique_genres.append(genre)\n",
    "            seen.add(genre.lower())\n",
    "    \n",
    "    return ', '.join(unique_genres)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = '74a510ecc9fc62bf3e0edc6adc2e99f9'\n",
    "\n",
    "    cache_file = \"data/ten_genres.csv\"\n",
    "    obscure_file = \"data/obscure_artists_mike_likes.csv\"\n",
    "\n",
    "    # Load cached ten_genres\n",
    "    if os.path.exists(cache_file) and os.path.getsize(cache_file) > 0:\n",
    "        cached_df = pd.read_csv(cache_file)\n",
    "        cached_df['Primary Artist'] = cached_df['Artist'].str.split(',').str[0].str.strip()\n",
    "        cached_pairs_set = set(zip(cached_df['Primary Artist'], cached_df['Album']))\n",
    "        cached_any_album_set = set(cached_df['Primary Artist'])\n",
    "    else:\n",
    "        cached_pairs_set = set()\n",
    "        cached_any_album_set = set()\n",
    "\n",
    "    # Load obscure artists\n",
    "    if os.path.exists(obscure_file):\n",
    "        obscure_df = pd.read_csv(obscure_file, quotechar='\"')\n",
    "        obscure_df['Primary Artist'] = obscure_df['Artist'].str.split(',').str[0].str.strip()\n",
    "        obscure_any_album_set = set(obscure_df['Primary Artist'])\n",
    "    else:\n",
    "        obscure_any_album_set = set()\n",
    "\n",
    "    # Extract primary artist from main df\n",
    "    df['Primary Artist'] = df['Artist Name(s)'].str.split(',').str[0].str.strip()\n",
    "\n",
    "    # Identify truly new artist-album pairs\n",
    "    df['Primary Artist'] = df['Artist Name(s)'].str.split(',').str[0].str.strip()\n",
    "\n",
    "    # Vectorized filtering\n",
    "    is_new_pair = ~df.apply(lambda r: (r['Primary Artist'], r['Album Name']) in cached_pairs_set, axis=1)\n",
    "    is_not_in_cached_any = ~df['Primary Artist'].isin(cached_any_album_set)\n",
    "    is_not_in_obscure = ~df['Primary Artist'].isin(obscure_any_album_set)\n",
    "\n",
    "    df_new = df[is_new_pair & is_not_in_cached_any & is_not_in_obscure]\n",
    "\n",
    "    print(f\"Processing {len(df_new)} new artist-album pairs.\")\n",
    "    \n",
    "    # Update genre data\n",
    "    update_genre_data(API_KEY, [df_new], 'data/ten_genres.csv')\n",
    "\n",
    "    # Load the ten_genres.csv file\n",
    "    ten_genres = pd.read_csv(\"data/ten_genres.csv\")\n",
    "    \n",
    "    # Clean genres in the ten_genres DataFrame\n",
    "    ten_genres[\"Genres\"] = ten_genres[\"Genres\"].apply(clean_genres)\n",
    "    \n",
    "    # Save the cleaned ten_genres back to CSV\n",
    "    ten_genres.to_csv(\"data/ten_genres.csv\", index=False)\n",
    "\n",
    "    # Extract the primary artist (before the comma)\n",
    "    df[\"Primary Artist\"] = df[\"Artist Name(s)\"].str.split(\",\").str[0]\n",
    "    ten_genres[\"Primary Artist\"] = ten_genres[\"Artist\"].str.split(\",\").str[0]\n",
    "\n",
    "    # Merge, prioritizing ten_genres\n",
    "    df = df.merge(ten_genres[[\"Primary Artist\", \"Genres\"]], on=\"Primary Artist\", how=\"left\", suffixes=(\"\", \"_ten\"))\n",
    "\n",
    "    # If ten_genres has a match, use it; otherwise, keep the original\n",
    "    df[\"Genres\"] = df[\"Genres_ten\"].combine_first(df[\"Genres\"])\n",
    "\n",
    "    # Apply the cleaning function to clean up genre data\n",
    "    df[\"Genres\"] = df[\"Genres\"].apply(clean_genres)\n",
    "\n",
    "    # Drop the extra columns\n",
    "    df.drop(columns=[\"Genres_ten\", \"Primary Artist\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8f067-0f96-4c4c-8d63-7296b631a751",
   "metadata": {},
   "source": [
    "### Save a Copy of the Missing Genre Artists (Just in Case I Want to Fuss With It Later!) üïµÔ∏è‚Äç‚ôÇÔ∏èüé∂\n",
    "Even with Last.fm‚Äôs extensive data, some artists still slip through the cracks. To handle these edge cases, I‚Äôm saving a list of artists with missing genre data to a separate file. This way, I can manually review and assign genres later if needed. Think of it as a backup plan for those hard-to-categorize artists! üéß‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada40922-e667-48b7-aa4f-a87e06a9112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ten_genres.csv file\n",
    "ten_genres = pd.read_csv(\"data/ten_genres.csv\")\n",
    "\n",
    "# Extract the primary artist from both datasets\n",
    "# For artists with collaborators, we focus on the first name listed üé§\n",
    "df[\"Primary Artist\"] = df[\"Artist Name(s)\"].str.split(\",\").str[0].str.strip()\n",
    "ten_genres[\"Primary Artist\"] = ten_genres[\"Artist\"].str.split(\",\").str[0].str.strip()\n",
    "\n",
    "# Identify missing artists\n",
    "# These are the artists not found in the ten_genres.csv file üé∏\n",
    "missing_artists = df[~df[\"Primary Artist\"].isin(ten_genres[\"Primary Artist\"])]\n",
    "\n",
    "# Save the missing artists to a CSV file\n",
    "# This allows for manual review and genre assignment later üïµÔ∏è‚Äç‚ôÇÔ∏è\n",
    "missing_artists[[\"Primary Artist\"]].drop_duplicates().to_csv(\"data/missing_artists.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(missing_artists)} missing artists to 'data/missing_artists.csv'.\")\n",
    "\n",
    "# The missing artists are now stored in data/missing_artists.csv.\n",
    "# If needed, I can manually assign genres and add them to\n",
    "# obscure_artists_mike_likes.csv for future runs. A little manual effort goes a long way! üéõÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64562b7f-ea92-400d-b33d-845770388d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many songs still have missing genre data?\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e0e6e-7b06-452d-8999-c96b5fcd8c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with nulls in any column except 'liked' because those belong to playlist_origin = df_nmf\n",
    "df = df[df.drop(columns=['liked']).notna().all(axis=1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d89341-0210-4136-bd93-66070a896e54",
   "metadata": {},
   "outputs": [],
   "source": [
    " df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c803a7f-4ea3-431f-8c62-54b6801f8416",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4860391-1274-4020-bd0a-8cca3608576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many tracks belong to each 'playlist_origin' in the dataset\n",
    "# This helps us understand the distribution of tracks across different sources üéµ\n",
    "playlist_origin_counts = df['playlist_origin'].value_counts()\n",
    "\n",
    "# Print the results\n",
    "print(\"Playlist Origin Counts:\")\n",
    "playlist_origin_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec9c04-e135-49c3-a901-c51f9911c7d1",
   "metadata": {},
   "source": [
    "## Finding Top 30 Genres (Temporarily, for new feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac5264-0f36-43e4-ac63-ef46d30cdc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Explode the Genres column\n",
    "df_exploded = df.assign(Genres=df['Genres'].str.split(', ')).explode('Genres')\n",
    "\n",
    "# Step 2: Filter out genres that appear fewer than 10 times\n",
    "genre_counts = df_exploded['Genres'].value_counts()\n",
    "frequent_genres = genre_counts[genre_counts >= 40].index  # Genres with at least 10 occurrences\n",
    "\n",
    "# Filter the exploded DataFrame to only include frequent genres\n",
    "df_frequent_genres = df_exploded[df_exploded['Genres'].isin(frequent_genres)]\n",
    "\n",
    "# Step 3: Group by Genre and calculate the mean of the 'liked' column\n",
    "genre_liked_avg = df_frequent_genres.groupby('Genres')['liked'].mean().reset_index()\n",
    "\n",
    "# Step 4: Sort by the average 'liked' score in descending order and get the top 30\n",
    "top_30_genres = genre_liked_avg.sort_values(by='liked', ascending=False).head(30)\n",
    "\n",
    "# Display the top 30 genres\n",
    "print(\"Top 30 Genres by Average Liked Score (Filtered for Frequent Genres):\")\n",
    "print(top_30_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99e430-4c09-4dc2-bb20-445d1ebe6a16",
   "metadata": {},
   "source": [
    "## Feature Engineering / Further Selecting üëå"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef584c69-80fb-4a3c-9631-e0e9edcb042a",
   "metadata": {},
   "source": [
    "### 'Record Label Frequency Encoded' üè∑Ô∏è\n",
    "\n",
    "To improve the model‚Äôs ability to generalize, I‚Äôm creating a new feature: **Record Label Frequency Encoded**. This feature represents how frequently each record label appears in the dataset, but with a twist‚Äîwe only consider labels from tracks I‚Äôve liked or marked as favorite albums. This ensures the encoding reflects my preferences and ignores labels from tracks I don‚Äôt care about.\n",
    "\n",
    "#### How It Works:\n",
    "1. **Training Data Preparation**: We filter the combined dataframe (`df`) by `playlist_origin` to get subsets for `df_liked` and `df_fav_albums`. This ensures we‚Äôre working with the most up-to-date data.\n",
    "2. **Frequency Calculation**: We count how often each record label appears in the training data (i.e., liked and favorite albums).\n",
    "3. **Handling Unseen Labels**: For labels not present in the training data (e.g., those in the NMF test set), we use the **mean frequency** of all labels as a fallback. This ensures the model can handle new or rare labels gracefully.\n",
    "4. **Application**: We apply the encoding to both the training and test sets, ensuring consistency across the data.\n",
    "\n",
    "#### Why This Matters:\n",
    "- **Relevance**: By focusing only on labels from tracks I like, the encoding better reflects my preferences.\n",
    "- **Robustness**: Using the mean frequency for unseen labels ensures new artists from rare labels won't be penalized.\n",
    "- **Simplicity**: Frequency encoding avoids the complexity of one-hot encoding while still capturing meaningful information.\n",
    "\n",
    "This approach ensures the model learns from the distribution of labels I care about while remaining robust to new or rare labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6075b5b-3b34-4b10-b304-ef671e86f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 'Record Label Frequency Encoded' üè∑Ô∏è\n",
    "\n",
    "# Filter the combined dataframe by playlist_origin to get the training subset\n",
    "# We focus on liked and favorite albums for training, ignoring disliked tracks and NMF üö´\n",
    "df_train_only = df[df['playlist_origin'].isin(['df_liked', 'df_fav_albums'])]\n",
    "\n",
    "# Calculate frequency encoding using only liked and favorite albums\n",
    "# This ensures the encoding reflects the distribution of labels I care about üè∑Ô∏è\n",
    "freq_encoding = df_train_only['Record Label'].value_counts()\n",
    "\n",
    "# Get the mean frequency to use for unseen labels\n",
    "# This acts as a fallback for labels not present in the training data üõ†Ô∏è\n",
    "mean_freq = freq_encoding.quantile(0.25)\n",
    "\n",
    "# Apply encoding to the training data\n",
    "df.loc[df['playlist_origin'].isin(['df_liked', 'df_fav_albums']), 'Record Label Frequency Encoded'] = (\n",
    "    df_train_only['Record Label'].map(freq_encoding).fillna(mean_freq)\n",
    ")\n",
    "\n",
    "# Apply encoding to the NMF test data separately\n",
    "# This avoids data leakage by using only the training data's frequency encoding üöÄ\n",
    "df_nmf_subset = df[df['playlist_origin'] == 'df_nmf']\n",
    "df_nmf_subset['Record Label Frequency Encoded'] = df_nmf_subset['Record Label'].map(freq_encoding).fillna(mean_freq)\n",
    "df.loc[df['playlist_origin'] == 'df_nmf', 'Record Label Frequency Encoded'] = df_nmf_subset['Record Label Frequency Encoded']\n",
    "\n",
    "# Apply encoding to the disliked albums\n",
    "df_not_liked_subset = df[df['playlist_origin'] == 'df_not_liked']\n",
    "df_not_liked_subset['Record Label Frequency Encoded'] = df_not_liked_subset['Record Label'].map(freq_encoding).fillna(mean_freq)\n",
    "df.loc[df['playlist_origin'] == 'df_not_liked', 'Record Label Frequency Encoded'] = df_not_liked_subset['Record Label Frequency Encoded']\n",
    "\n",
    "# Apply encoding to the mid albums\n",
    "df_mid_subset = df[df['playlist_origin'] == 'df_mid']\n",
    "df_mid_subset['Record Label Frequency Encoded'] = df_mid_subset['Record Label'].map(freq_encoding).fillna(mean_freq)\n",
    "df.loc[df['playlist_origin'] == 'df_mid', 'Record Label Frequency Encoded'] = df_mid_subset['Record Label Frequency Encoded']\n",
    "\n",
    "\n",
    "# Check the result by displaying 20 Unique Record Labels and Their Encoding!\n",
    "df[['Record Label', 'Record Label Frequency Encoded']].drop_duplicates().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6275ff43-c4a4-4a7a-acb7-d5c74a80285e",
   "metadata": {},
   "source": [
    "### Target Encode Genres üéö\n",
    "\n",
    "To capture the relationship between genres and the target variable (liked), I use target encoding. This technique replaces each genre (or combination of genres) with the mean target value for that genre, smoothed to handle rare categories. This gives me a \"fingerprint\" of how much I tend to like songs from each genre, which helps the model make better predictions.\n",
    "\n",
    "#### Key Features:\n",
    "- **Handling Multi-Genre Tracks**: Tracks with multiple genres are split, encoded individually, and then aggregated.\n",
    "- **Smoothing**: I smooth the encoding to prevent overfitting by balancing genre-specific means with the global mean.\n",
    "- **Rare Genres**: Genres that appear fewer than 28 times are grouped into a common \"Rare_Genre\" category.\n",
    "- **Unknown Genres**: Tracks with \"Unknown\" genres are handled separately, using the global mean or a fallback value for NMF rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1f54f-259b-46aa-826e-df7fa3d3596c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define explicitly disliked genres with severity weights (1-10 scale, 10 being most disliked)\n",
    "disliked_genres = {\n",
    "    'drone': 9, 'psychedelic': 7, 'improv': 8, 'ambient': 6, 'experimental': 7,\n",
    "    'instrumental': 5, 'classical': 5, 'hardcore': 8, 'downtempo': 6, 'slowcore': 6,\n",
    "    'noise': 9, 'satanic': 10, 'pagan': 8, 'metalcore': 9, 'deathcore': 10, \n",
    "    'death metal': 10, 'metal': 8, 'metallic hardcore': 9, 'beatdown deathcore': 10, \n",
    "    'nydm': 9, 'soundscape': 7, 'alternative metal': 8, 'horror punk': 8, \n",
    "    'sludge metal': 9, 'thrash metal': 9, 'death thrash metal': 10, \n",
    "    'heavy metal': 8, 'black metal': 10, 'doom metal': 9, 'death doom metal': 10,\n",
    "    'techno': 8, 'hard techno': 9, 'tech house': 7, 'minimal techno': 8,\n",
    "    'acid techno': 8, 'industrial techno': 9,  \n",
    "    'psychedelic rock': 7, 'psychedelic pop': 7, 'neo-psychedelic': 7, \n",
    "    'psychedelic folk': 7, 'psychedelia': 7, 'psych': 7, 'psych rock': 7, \n",
    "    'psych pop': 7, 'psychedelic soul': 7, 'acid rock': 7\n",
    "}\n",
    "\n",
    "preferred_genres = {\n",
    "    'alternative r&b': 10, 'chamber pop': 9, 'bedroom pop': 8.5,\n",
    "    'indie folk': 8, 'post punk': 7, 'indie': 6.5, 'jangle pop': 6.3,\n",
    "    'retro soul': 5, 'folk pop': 3.5, 'indie rock': 3.5, 'indie pop': 3.5,\n",
    "    'baroque pop': 3.5, 'americana': 3\n",
    "}\n",
    "\n",
    "def target_encode_multi_genre(df, genre_column, target, smoothing=1, aggregation_method='mean', min_count=28):\n",
    "    # Make a copy of the dataframe to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Target encode a multi-genre column with improved handling of disliked genres\n",
    "    df_train = df[df['playlist_origin'] != 'df_nmf'].copy()\n",
    "    \n",
    "    # Check if we have training data\n",
    "    if len(df_train) == 0:\n",
    "        raise ValueError(\"No training data found (df_train is empty)\")\n",
    "    \n",
    "    global_mean = df_train[target].mean()\n",
    "    \n",
    "    # Split genres into lists\n",
    "    df_train['split_genres'] = df_train[genre_column].fillna('Unknown').str.split(', ').apply(\n",
    "        lambda x: [genre.strip() for genre in x if genre not in ['seen live', 'Unknown', '']] \n",
    "        if isinstance(x, list) else ['Unknown']\n",
    "    )\n",
    "    \n",
    "    # Explode genres for encoding calculation\n",
    "    exploded_genres = df_train.explode('split_genres')\n",
    "    \n",
    "    # Calculate genre statistics\n",
    "    label_means = exploded_genres.groupby('split_genres')[target].mean()\n",
    "    label_counts = exploded_genres['split_genres'].value_counts()\n",
    "    rare_genres = label_counts[label_counts < min_count].index.tolist()\n",
    "    \n",
    "    # Apply smoothing\n",
    "    smoothed_values = (label_means * label_counts + global_mean * smoothing) / (label_counts + smoothing)\n",
    "    \n",
    "    # Adjust rare genres\n",
    "    for genre in rare_genres:\n",
    "        if genre in smoothed_values:\n",
    "            smoothed_values[genre] = (smoothed_values[genre] + global_mean * 2) / 3\n",
    "    \n",
    "    genre_encoding_map = smoothed_values.to_dict()\n",
    "    \n",
    "    def encode_genres(genres_str):\n",
    "        if pd.isna(genres_str) or genres_str == 'Unknown':\n",
    "            return global_mean\n",
    "        \n",
    "        genres = str(genres_str).split(', ')\n",
    "        genres = [g.strip() for g in genres if g not in ['seen live', 'Unknown', '']]\n",
    "        \n",
    "        # Handle genres without commas\n",
    "        if len(genres) == 1 and ' ' in genres[0]:\n",
    "            genre_lower = genres[0].lower()\n",
    "            # First check preferred genres\n",
    "            for preferred_term in preferred_genres:\n",
    "                if preferred_term in genre_lower:\n",
    "                    return min(100, global_mean * 1.5)  # Boost preferred genres\n",
    "            # Then check disliked terms\n",
    "            for disliked_term in ['metal', 'satanic', 'gore', 'brutal']:\n",
    "                if disliked_term in genre_lower:\n",
    "                    return 30.0\n",
    "        \n",
    "        if not genres:\n",
    "            return global_mean\n",
    "        \n",
    "        genre_values = []\n",
    "        genre_weights = []\n",
    "        \n",
    "        for genre in genres:\n",
    "            value = genre_encoding_map.get(genre, global_mean)\n",
    "            weight = 1.0\n",
    "            \n",
    "            # Apply blessing for preferred genres\n",
    "            genre_lower = genre.lower()\n",
    "            if genre_lower in preferred_genres:\n",
    "                blessing_factor = 1 + (preferred_genres[genre_lower] / 10.0)\n",
    "                value = min(100, value * blessing_factor)  # Cap at 100\n",
    "                weight = 1.5  # Higher weight for preferred genres\n",
    "            elif genre_lower in disliked_genres:\n",
    "                severity = disliked_genres.get(genre_lower, 6)\n",
    "                penalty_factor = 1.0 - (severity / 10.0)\n",
    "                value = value * penalty_factor\n",
    "                weight = 1.0 + (severity / 10.0)\n",
    "            elif genre in label_means and label_means[genre] < global_mean - 10:\n",
    "                value = value * 0.75\n",
    "                \n",
    "            genre_values.append(value)\n",
    "            genre_weights.append(weight)\n",
    "        \n",
    "        return sum(v * w for v, w in zip(genre_values, genre_weights)) / sum(genre_weights)\n",
    "    \n",
    "    # Apply encoding to ALL rows including NMF\n",
    "    df[genre_column + '_encoded'] = df[genre_column].apply(encode_genres)\n",
    "    return df\n",
    "\n",
    "# Run the improved target encoding function \n",
    "try:\n",
    "    df = target_encode_multi_genre(df, 'Genres', 'liked', smoothing=35, aggregation_method='mean')\n",
    "except Exception as e:\n",
    "    print(f\"Error during encoding: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Check NMF rows encoding stats\n",
    "if df is not None:\n",
    "    nmf_encoding_check = df[df['playlist_origin'] == 'df_nmf'][['Track Name', 'Artist Name(s)', 'Album Name', 'Genres', 'Genres_encoded']]\n",
    "\n",
    "    print(f\"NMF rows total: {len(nmf_encoding_check)}\")\n",
    "    print(f\"NMF rows with genre encoding: {nmf_encoding_check['Genres_encoded'].notna().sum()}\")\n",
    "    print(f\"NMF rows missing genre encoding: {nmf_encoding_check['Genres_encoded'].isna().sum()}\")\n",
    "\n",
    "    if nmf_encoding_check['Genres_encoded'].notna().any():\n",
    "        print(\"\\nEncoding value stats:\")\n",
    "        print(f\"Min: {nmf_encoding_check['Genres_encoded'].min():.2f}\")\n",
    "        print(f\"Max: {nmf_encoding_check['Genres_encoded'].max():.2f}\")\n",
    "        print(f\"Mean: {nmf_encoding_check['Genres_encoded'].mean():.2f}\")\n",
    "        print(f\"Median: {nmf_encoding_check['Genres_encoded'].median():.2f}\")\n",
    "        \n",
    "        diverse_samples = nmf_encoding_check.drop_duplicates(['Artist Name(s)'])[:10]\n",
    "        print(\"\\n10 tracks from different artists:\")\n",
    "        for i, row in diverse_samples.iterrows():\n",
    "            print(f\"{row['Artist Name(s)']} - {row['Track Name']} | Genres: {row['Genres']} | Score: {row['Genres_encoded']:.2f}\")\n",
    "        \n",
    "        print(\"\\nTop 5 highest genre scores:\")\n",
    "        high_scores = nmf_encoding_check.nlargest(5, 'Genres_encoded')\n",
    "        for i, row in high_scores.iterrows():\n",
    "            print(f\"{row['Artist Name(s)']} - {row['Track Name']} | Genres: {row['Genres']} | Score: {row['Genres_encoded']:.2f}\")\n",
    "        \n",
    "        print(\"\\nBottom 5 lowest genre scores:\")\n",
    "        low_scores = nmf_encoding_check.nsmallest(5, 'Genres_encoded')\n",
    "        for i, row in low_scores.iterrows():\n",
    "            print(f\"{row['Artist Name(s)']} - {row['Track Name']} | Genres: {row['Genres']} | Score: {row['Genres_encoded']:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nNo encoded values found in NMF rows.\")\n",
    "\n",
    "    # Save results\n",
    "    df[df['playlist_origin'] == 'df_nmf'][['Track Name', 'Artist Name(s)', 'Album Name', 'Genres', 'Genres_encoded']].to_csv('genre_encoding_results.csv', index=False)\n",
    "    print(\"\\nGenre encoding results saved to 'genre_encoding_results.csv'\")\n",
    "else:\n",
    "    print(\"Dataframe is None - encoding failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df9122-2225-49b9-b63c-3f9f15f433dc",
   "metadata": {},
   "source": [
    "# Finding How Central an Artist is to My Music Taste üéØ\n",
    "\n",
    "To understand how central an artist is to my music taste, we're building a network of artists based on which ones I've liked and which are similar to them. Using **PageRank**, a method that measures the importance of nodes in a network, we calculate each artist's \"centrality\" score. This score reflects how influential an artist is within the network of my liked and similar artists.\n",
    "\n",
    "## Key Points:\n",
    "* **Liked Artists**: Artists from my liked songs and favorite albums.\n",
    "* **Similar Artists**: Artists similar to my liked artists (from `df_liked_similar`).\n",
    "* **No Data Leakage**: The `df_nmf` (New Music Friday) artists are excluded from the network to avoid bias.\n",
    "* **Scaled Scores**: Centrality scores are normalized to a 0-100 range for easier interpretation.\n",
    "\n",
    "This approach ensures that the centrality scores are based solely on my preferences and not influenced by artists I haven't liked or the New Music Friday playlist. Let's dive into the code! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612732d8-7cc2-4527-bcc5-307649e7d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Featured_Artist(s) column\n",
    "def extract_featured_artists(df):\n",
    "    # Create new column by splitting on comma and taking all but first artist\n",
    "    df['Featured_Artist(s)'] = df['Artist Name(s)'].apply(\n",
    "        lambda x: ', '.join(str(x).split(',')[1:]).strip() if ',' in str(x) else ''\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Prepare the data\n",
    "def prepare_featured_artists(df):\n",
    "    # Prepare the 'Featured_Artist(s)' column for analysis.\n",
    "    # Ensures the column exists, handles missing values, and splits into lists. üßπ\n",
    "    # First create the column if it doesn't exist\n",
    "    df = extract_featured_artists(df)\n",
    "    \n",
    "    # Ensure 'Featured_Artist(s)' is a string and handle missing values\n",
    "    df['Featured_Artist(s)'] = df['Featured_Artist(s)'].fillna('').astype(str)\n",
    "    \n",
    "    # Split and clean the 'Featured_Artist(s)' column into lists\n",
    "    df['Featured_Artist(s)'] = df['Featured_Artist(s)'].str.split(',').apply(\n",
    "        lambda x: [artist.strip() for artist in x] if isinstance(x, list) else []\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def build_graph(df, df_liked_similar, include_nmf=False):\n",
    "    # Build a graph of artists and their connections.\n",
    "    # Only includes liked artists and their similar artists by default. üéØ\n",
    "    # Optionally includes NMF, not-liked, and mid artists (without adding edges). üö´\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes for liked artists\n",
    "    liked_artists = set(\n",
    "        df[df['playlist_origin'].isin(['df_liked', 'df_fav_albums'])]['Artist Name(s)']\n",
    "        .str.split(',').explode().str.strip()\n",
    "    )\n",
    "    G.add_nodes_from(liked_artists, type='liked')\n",
    "    \n",
    "    # Add nodes for similar artists (from liked)\n",
    "    similar_artists_liked = set(\n",
    "        df_liked_similar['Similar Artists']\n",
    "        .dropna()\n",
    "        .str.split(',').explode().str.strip()\n",
    "    )\n",
    "    G.add_nodes_from(similar_artists_liked, type='similar_liked')\n",
    "    \n",
    "    # Add edges based on similarity (from liked)\n",
    "    for _, row in df_liked_similar.iterrows():\n",
    "        artist = row['Artist']\n",
    "        if isinstance(row['Similar Artists'], str):\n",
    "            similar = row['Similar Artists'].split(', ')\n",
    "            for s in similar:\n",
    "                G.add_edge(artist, s, weight=1.0)\n",
    "    \n",
    "    # Optionally include NMF, not-liked, and mid artists (without adding edges)\n",
    "    if include_nmf:\n",
    "        nmf_artists = set(\n",
    "            df[df['playlist_origin'] == 'df_nmf']['Artist Name(s)']\n",
    "            .str.split(',').explode().str.strip()\n",
    "        )\n",
    "        not_liked_artists = set(\n",
    "            df[df['playlist_origin'] == 'df_not_liked']['Artist Name(s)']\n",
    "            .str.split(',').explode().str.strip()\n",
    "        )\n",
    "        mid_artists = set(\n",
    "            df[df['playlist_origin'] == 'df_mid']['Artist Name(s)']\n",
    "            .str.split(',').explode().str.strip()\n",
    "        )\n",
    "        G.add_nodes_from(nmf_artists, type='nmf')\n",
    "        G.add_nodes_from(not_liked_artists, type='not_liked')\n",
    "        G.add_nodes_from(mid_artists, type='mid')  # Mid artists exist but don‚Äôt interfere with the web\n",
    "    \n",
    "    return G\n",
    "\n",
    "def calculate_centrality_scores(G, df):\n",
    "    # Calculate PageRank centrality for all artists in the graph.\n",
    "    # Maps centrality scores back to the DataFrame for main artists. üìä\n",
    "    centrality_scores = nx.pagerank(G)\n",
    "    \n",
    "    # Map centrality scores back to DataFrame for main artists\n",
    "    df['Artist Centrality'] = (\n",
    "        df['Artist Name(s)']\n",
    "        .str.split(',').str[0].str.strip()\n",
    "        .map(centrality_scores).fillna(0)\n",
    "    )\n",
    "    \n",
    "    return df, centrality_scores\n",
    "\n",
    "# Normalize centrality scores to a 0-100 range\n",
    "def normalize_centrality_scores(df):\n",
    "    if df['Artist Centrality'].max() != 0:\n",
    "        df['Artist Centrality'] = (df['Artist Centrality'] / df['Artist Centrality'].max()) * 100\n",
    "    return df\n",
    "\n",
    "# Run the complete pipeline\n",
    "def run_centrality_analysis(df, df_liked_similar, include_nmf=False):\n",
    "    # Run the complete centrality analysis pipeline:\n",
    "    # 1. Prepare featured artists data.\n",
    "    # 2. Build the graph.\n",
    "    # 3. Calculate centrality scores.\n",
    "    # 4. Normalize scores to 0-100 range. üöÄ\n",
    "    # Prepare the featured artists data (still needed to ensure the column exists)\n",
    "    df = prepare_featured_artists(df)\n",
    "    \n",
    "    # Build the graph and calculate centrality\n",
    "    G = build_graph(df, df_liked_similar, include_nmf=include_nmf)\n",
    "    df, centrality_scores = calculate_centrality_scores(G, df)\n",
    "    df = normalize_centrality_scores(df)\n",
    "    \n",
    "    return df, G, centrality_scores\n",
    "\n",
    "# Execute the analysis for all artists (including NMF, not-liked, and mid)\n",
    "df, G, centrality_scores = run_centrality_analysis(df, df_liked_similar, include_nmf=True)\n",
    "\n",
    "# Extract the primary artist (first artist in 'Artist Name(s)' field)\n",
    "df['Primary Artist'] = df['Artist Name(s)'].str.split(',').str[0].str.strip()\n",
    "\n",
    "# Now group by Primary Artist to ensure unique artists are considered\n",
    "df_unique_artists = df.groupby('Primary Artist', as_index=False)['Artist Centrality'].max()\n",
    "\n",
    "# Get the top 30 unique primary artists by their centrality score\n",
    "top_artist_df = df_unique_artists.nlargest(30, 'Artist Centrality')[['Primary Artist', 'Artist Centrality']]\n",
    "\n",
    "# Print the top 30 unique primary artists by centrality\n",
    "print(\"\\nTop 30 Unique Primary Artists by Centrality:\")\n",
    "top_artist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da884820-f2ec-4ffb-a44c-2a97953d95f9",
   "metadata": {},
   "source": [
    "### Distribution of Artist Centrality Scores by Playlist Origin üìäüéµ\n",
    "\n",
    "This histogram shows how **Artist Centrality** scores are distributed across different playlists. The centrality score reflects how central an artist is to my music taste, based on their connections to my liked artists and their similar artists.\n",
    "\n",
    "- **Liked Artists (df_liked)**: Artists I‚Äôve explicitly liked on Spotify.\n",
    "- **Favorite Albums (df_fav_albums)**: Albums I‚Äôve enjoyed in recent years.\n",
    "- **Mid Albums (df_mid)**: Albums I felt were just okay. Not gbad, not great either.\n",
    "- **Not Liked Albums (df_not_liked)**: Albums I didn‚Äôt enjoy in recent years.\n",
    "- **New Music Friday (df_nmf)**: The most recent New Music Friday playlist.\n",
    "\n",
    "#### What We‚Äôre Looking For:\n",
    "- **High Centrality Scores**: Artists who are closely connected to my liked artists.\n",
    "- **Low Centrality Scores**: Artists who are less connected to my preferences.\n",
    "- **Patterns by Playlist**: Do certain playlists (e.g., liked vs. not liked) have distinct centrality distributions?\n",
    "\n",
    "This visualization helps us understand how my preferences are reflected in the network of artists and how new or not-liked artists compare to my favorites. üïµÔ∏è‚Äç‚ôÇÔ∏è‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511eaa0-d4bd-4297-a7cc-755148f260c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot the distribution of 'Artist Centrality', colored by playlist_origin\n",
    "# This shows how centrality scores vary across different playlists üé®\n",
    "sns.histplot(df, x='Artist Centrality', bins=30, hue='playlist_origin', kde=True, alpha=0.6, palette='viridis')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Artist Centrality Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Artist Centrality Scores by Playlist Origin')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d01b8b-b7f0-4693-90af-fd9ddac50d91",
   "metadata": {},
   "source": [
    "### Lucy Dacus is one of my most played artists of all time, let's look at what her Pagerank network looked like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9c243-a5d0-415d-baca-fc681fa5aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Lucy Dacus's centrality score\n",
    "lucy_dacus_centrality = df[df['Artist Name(s)'].str.contains('Lucy Dacus')]['Artist Centrality'].values[0]\n",
    "\n",
    "# Extract Lucy Dacus's connections from the graph\n",
    "lucy_dacus_connections = list(G.edges('Lucy Dacus'))\n",
    "\n",
    "# Print Lucy Dacus's centrality score and connections\n",
    "print(f\"Lucy Dacus's Centrality Score: {lucy_dacus_centrality:.2f}\")\n",
    "print(f\"Lucy Dacus's Connections: {lucy_dacus_connections}\")\n",
    "\n",
    "# Create a subgraph centered around Lucy Dacus\n",
    "lucy_dacus_subgraph = G.subgraph(['Lucy Dacus'] + [artist for edge in lucy_dacus_connections for artist in edge])\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Draw the network\n",
    "pos = nx.spring_layout(lucy_dacus_subgraph, seed=42)  # Layout for consistent positioning\n",
    "nx.draw_networkx_nodes(lucy_dacus_subgraph, pos, node_size=500, node_color='lightblue', alpha=0.9)\n",
    "nx.draw_networkx_edges(lucy_dacus_subgraph, pos, edge_color='gray', alpha=0.6)\n",
    "\n",
    "# Highlight Lucy Dacus\n",
    "nx.draw_networkx_nodes(lucy_dacus_subgraph, pos, nodelist=['Lucy Dacus'], node_size=1000, node_color='purple', alpha=0.9)\n",
    "nx.draw_networkx_labels(lucy_dacus_subgraph, pos, labels={'Lucy Dacus': 'Lucy Dacus'}, font_size=12, font_weight='bold')\n",
    "\n",
    "# Add labels for connected artists\n",
    "nx.draw_networkx_labels(lucy_dacus_subgraph, pos, font_size=10)\n",
    "\n",
    "# Add title and annotations\n",
    "plt.title(\"Lucy Dacus's Position in My Music Taste Network\", fontsize=16)\n",
    "plt.figtext(0.5, 0.05, f\"Centrality Score: {lucy_dacus_centrality:.2f}\", ha='center', fontsize=12, style='italic')\n",
    "\n",
    "# Remove axes\n",
    "plt.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c20db-4987-4708-b980-ed86d33d8c98",
   "metadata": {},
   "source": [
    "## Two More Features Before Primetime! üé≠\n",
    "\n",
    "**Mood Score**: Combines Valence, Danceability, and Liveness to capture the vibe.\n",
    "\n",
    "**Energy Profile**: Mashes Energy, Loudness, and Tempo to gauge the track‚Äôs intensity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c419738-f1f8-4269-8607-60b5139ddcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate features only on non-NMF data\n",
    "non_nmf_df = df[df['playlist_origin'] != 'df_nmf'].copy()\n",
    "\n",
    "# Create mood_score and energy_profile on non-NMF data\n",
    "non_nmf_df['mood_score'] = non_nmf_df[['Valence', 'Danceability', 'Liveness']].mean(axis=1)\n",
    "non_nmf_df['energy_profile'] = non_nmf_df[['Energy', 'Loudness', 'Tempo']].mean(axis=1)\n",
    "\n",
    "# Merge these features back into the main dataframe\n",
    "df = df.merge(non_nmf_df[['mood_score', 'energy_profile']], how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9de14-6eb1-4928-9101-5e416946e977",
   "metadata": {},
   "source": [
    "## A Clean Album ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abb64b-de0f-4e31-85bb-1d012d1a844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Album ID for better data tracking for New Music Friday albums\n",
    "df[\"Primary Artist\"] = df[\"Artist Name(s)\"].str.split(\",\").str[0].str.strip()\n",
    "\n",
    "# ADD THIS DIRECTLY AFTER (same indentation):\n",
    "df['album_id'] = df.apply(\n",
    "    lambda x: f\"{x['Primary Artist'].lower()}_{x['Album Name'].lower()}\".replace(' ', '_'),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b164e-c47e-4a71-8430-0a0c40c78800",
   "metadata": {},
   "source": [
    "## Previwing the features on the menu üìñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0a236-7e93-4a4f-ad96-aa77414f9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in the DataFrame:\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"\\nData Types of Each Column:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e10d2a-a4b1-4fc5-ba15-e0f339c9d738",
   "metadata": {},
   "source": [
    "## Standardize the numeric columns üìè\n",
    "When some numbers have a larger size than others, the model can be biased towards them, so we bring all the numeric columns on a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e2297-9c93-4b11-aa84-a465fcacbe93",
   "metadata": {},
   "source": [
    "### Seperate New Music Friday and Save it for Later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb709d88-3b66-4b3d-bd0f-31c17b361b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mood and energy scores first\n",
    "df['mood_score'] = df[['Valence', 'Danceability', 'Liveness']].mean(axis=1)\n",
    "df['energy_profile'] = df[['Energy', 'Loudness', 'Tempo']].mean(axis=1)\n",
    "\n",
    "# Split and save data\n",
    "df_nmf = df[df['playlist_origin'] == 'df_nmf'].copy()\n",
    "df = df[df['playlist_origin'] != 'df_nmf'].copy()\n",
    "\n",
    "# Save both versions pre-standardization\n",
    "df_nmf.to_csv('data/df_nmf_later.csv', index=False)\n",
    "df_cleaned_pre_standardized = pd.concat([df, df_nmf], ignore_index=True)\n",
    "df_cleaned_pre_standardized.to_csv('data/df_cleaned_pre_standardized.csv', index=False)\n",
    "\n",
    "# Store original values\n",
    "original_centrality = df_nmf['Artist Centrality'].copy()\n",
    "original_mood = df_nmf['mood_score'].copy()\n",
    "original_energy = df_nmf['energy_profile'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0ad30-6b7e-4bab-acdf-d566ec4615a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric columns to scale\n",
    "numeric_columns = [ \n",
    "    'Genres_encoded', \n",
    "    'Artist Centrality',\n",
    "    'Popularity',\n",
    "    'Record Label Frequency Encoded', \n",
    "    'mood_score', \n",
    "    'energy_profile'\n",
    "]\n",
    "\n",
    "# Initialize the scaler with a 1 to 100 range\n",
    "scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "\n",
    "# Fit the scaler on the training data (df)\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Transform the test data (df_nmf) using the fitted scaler\n",
    "df_nmf[numeric_columns] = scaler.transform(df_nmf[numeric_columns])\n",
    "\n",
    "# Save the scaled df_nmf for later use\n",
    "df_nmf.to_csv('data/df_nmf_later.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the scaled test data\n",
    "print(\"\\nScaled Test Data (df_nmf):\")\n",
    "df_nmf[numeric_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b7fcb4-3d9f-4119-985d-824a1e98eafc",
   "metadata": {},
   "source": [
    "## Tuning and Predicting with Random Forest & XGBoost üåü\n",
    "In this section, we fine-tune our Random Forest and XGBoost models using randomized search for optimal hyperparameters. The goal? To get the best possible performance in predicting song ratings. After tuning the models, we make predictions on the unseen data, combining both models' results to generate a more accurate score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d91d28-f7ea-46c1-9e10-345dfa959649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_models(df, features, test_size=0.2):\n",
    "    # Prepare data\n",
    "    X = df[features]\n",
    "    y = (df['liked'] - df['liked'].mean()) / df['liked'].std()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Model parameters\n",
    "    rf_params = {\n",
    "        'n_estimators': randint(50, 300),\n",
    "        'max_depth': randint(3, 15),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 5)\n",
    "    }\n",
    "    \n",
    "    xgb_params = {\n",
    "        'n_estimators': randint(50, 300),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'subsample': uniform(0.6, 0.4),\n",
    "        'colsample_bytree': uniform(0.6, 0.4)\n",
    "    }\n",
    "    \n",
    "    # Initialize models\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    xgb = XGBRegressor(random_state=42)\n",
    "    \n",
    "    # Perform randomized search\n",
    "    rf_search = RandomizedSearchCV(rf, rf_params, n_iter=20, cv=5, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "    xgb_search = RandomizedSearchCV(xgb, xgb_params, n_iter=20, cv=5, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Fit models\n",
    "    rf_search.fit(X_train, y_train)\n",
    "    xgb_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Train final models with best parameters\n",
    "    best_rf = RandomForestRegressor(**rf_search.best_params_, random_state=42)\n",
    "    best_xgb = XGBRegressor(**xgb_search.best_params_, random_state=42)\n",
    "    \n",
    "    best_rf.fit(X_train, y_train)\n",
    "    best_xgb.fit(X_train, y_train)\n",
    "    \n",
    "    return best_rf, best_xgb, rf_search.best_params_, xgb_search.best_params_, X_test, y_test\n",
    "\n",
    "def predict_with_tuned_models(best_rf, best_xgb, df_nmf, features, y_mean, y_std):\n",
    "    # Make predictions\n",
    "    rf_pred = best_rf.predict(df_nmf[features]) * y_std + y_mean\n",
    "    xgb_pred = best_xgb.predict(df_nmf[features]) * y_std + y_mean\n",
    "    \n",
    "    # Combine predictions\n",
    "    df_nmf['predicted_score'] = (rf_pred + xgb_pred) / 2\n",
    "    \n",
    "    return df_nmf\n",
    "\n",
    "# Usage\n",
    "features = [\n",
    "    'Genres_encoded', \n",
    "    'Artist Centrality',  \n",
    "    'Record Label Frequency Encoded', \n",
    "    'mood_score', \n",
    "    'Popularity',\n",
    "    'energy_profile'\n",
    "]\n",
    "best_rf, best_xgb, rf_params, xgb_params, X_test, y_test = tune_models(df, features)\n",
    "df_nmf = predict_with_tuned_models(best_rf, best_xgb, df_nmf, features, df['liked'].mean(), df['liked'].std())\n",
    "\n",
    "# Output best parameters and feature importances\n",
    "print(f\"Random Forest best parameters: {rf_params}\")\n",
    "print(f\"XGBoost best parameters: {xgb_params}\")\n",
    "print(f\"Random Forest feature importances: {best_rf.feature_importances_}\")\n",
    "print(f\"XGBoost feature importances: {best_xgb.feature_importances_}\")\n",
    "\n",
    "# Feature importance DataFrame\n",
    "feature_importance = pd.DataFrame({'feature': features, 'importance': best_rf.feature_importances_}).sort_values('importance', ascending=False)\n",
    "print(\"Feature importance (Random Forest):\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b494d-4137-409a-96c8-a4bbcff73a76",
   "metadata": {},
   "source": [
    "# 80/20 Train/Test of The Non NMF Data using RandomForrest and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6329f0f9-85af-4737-bc6e-5b81559867b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(df, features, rf_params, xgb_params):\n",
    "    # Only use non-NMF data for training\n",
    "    train_df = df[df['playlist_origin'] != 'df_nmf'].copy()\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = train_df[features]\n",
    "    y = train_df['liked']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize models with the best parameters from tuning\n",
    "    rf_model = RandomForestRegressor(**rf_params, random_state=42)\n",
    "    xgb_model = XGBRegressor(**xgb_params, random_state=42)\n",
    "    \n",
    "    # Train models\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Combine predictions (80/20 weight)\n",
    "    final_pred = (0.8 * rf_pred) + (0.2 * xgb_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, final_pred)\n",
    "    r2 = r2_score(y_test, final_pred)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Track Name': train_df.loc[X_test.index, 'Track Name'],\n",
    "        'Artist Name(s)': train_df.loc[X_test.index, 'Artist Name(s)'],\n",
    "        'Actual Score': y_test,\n",
    "        'Predicted Score': final_pred\n",
    "    })\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'RF Importance': rf_model.feature_importances_,\n",
    "        'XGB Importance': xgb_model.feature_importances_\n",
    "    })\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"R¬≤ Score: {r2:.2f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 Predictions vs Actual:\")\n",
    "    print(results_df.sort_values('Predicted Score', ascending=False).head(10))\n",
    "    \n",
    "    print(\"\\nFeature Importances:\")\n",
    "    print(importances.sort_values('RF Importance', ascending=False))\n",
    "    \n",
    "    return rf_model, xgb_model, results_df, importances\n",
    "\n",
    "# Usage\n",
    "features = ['Genres_encoded', 'Artist Centrality', 'Record Label Frequency Encoded', 'mood_score', 'Popularity', 'energy_profile']\n",
    "rf_model, xgb_model, results, importances = train_test_model(df, features, rf_params, xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd7455-4090-48fb-92bd-1fe791ed3f65",
   "metadata": {},
   "source": [
    "## Run the New Music Friday Regression Model\n",
    "\n",
    "We're using non-NMF data to train Random Forest and XGBoost models to predict how much users will like different tracks. Our improved model includes two key enhancements:\n",
    "\n",
    "1. **Artist Similarity Boost**: We reward New Music Friday artists that are similar to your liked artists, helping you discover new music that aligns with your established preferences.\n",
    "\n",
    "2. **Adaptive Ensemble Weighting**: Instead of a fixed 50/50 blend between models, we dynamically adjust weights based on which model performs better for different types of music.\n",
    "\n",
    "After training, we make predictions for new tracks and calculate confidence intervals to gauge prediction reliability. We then aggregate results by album, factoring in consistency and track count, and sort by weighted score to create a personalized list of top album recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3948a-af2a-497b-9b0e-5f68579ced00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adaptive_weights(X_train, y_train, X_pred, rf_model, xgb_model, features, n_bins=5):\n",
    "    # Make predictions on training data\n",
    "    rf_train_pred = rf_model.predict(X_train)\n",
    "    xgb_train_pred = xgb_model.predict(X_train)\n",
    "    \n",
    "    # Calculate errors\n",
    "    rf_errors = np.abs(rf_train_pred - y_train)\n",
    "    xgb_errors = np.abs(xgb_train_pred - y_train)\n",
    "    \n",
    "    # Create a dataframe with features, predictions, and errors\n",
    "    error_df = X_train.copy()\n",
    "    error_df['rf_error'] = rf_errors\n",
    "    error_df['xgb_error'] = xgb_errors\n",
    "    \n",
    "    # Get top 2 features by importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    top_features = feature_importance['feature'].head(2).tolist()\n",
    "    \n",
    "    # Create bins for each top feature\n",
    "    for feature in top_features:\n",
    "        error_df[f'{feature}_bin'] = pd.qcut(error_df[feature], n_bins, labels=False, duplicates='drop')\n",
    "    \n",
    "    # Calculate performance by bin\n",
    "    performance_by_bin = {}\n",
    "    for feature in top_features:\n",
    "        performance_by_bin[feature] = {}\n",
    "        for bin_idx in range(n_bins):\n",
    "            bin_data = error_df[error_df[f'{feature}_bin'] == bin_idx]\n",
    "            if len(bin_data) > 0:\n",
    "                rf_mean_error = bin_data['rf_error'].mean()\n",
    "                xgb_mean_error = bin_data['xgb_error'].mean()\n",
    "                total_error = rf_mean_error + xgb_mean_error\n",
    "                \n",
    "                if total_error > 0:\n",
    "                    # Weight inversely proportional to error (higher weight = better model)\n",
    "                    rf_weight = xgb_mean_error / total_error\n",
    "                else:\n",
    "                    rf_weight = 0.5\n",
    "                \n",
    "                performance_by_bin[feature][bin_idx] = rf_weight\n",
    "    \n",
    "    # Calculate weights for prediction data\n",
    "    weights = np.ones(len(X_pred)) * 0.5  # Default 50/50 weight\n",
    "    \n",
    "    for feature in top_features:\n",
    "        # Create bins for prediction data\n",
    "        try:\n",
    "            pred_bins = pd.qcut(X_pred[feature], n_bins, labels=False, duplicates='drop')\n",
    "            \n",
    "            # Apply weights based on bin performance\n",
    "            for bin_idx in range(n_bins):\n",
    "                if bin_idx in performance_by_bin[feature]:\n",
    "                    bin_mask = (pred_bins == bin_idx)\n",
    "                    if any(bin_mask):\n",
    "                        rf_weight = performance_by_bin[feature][bin_idx]\n",
    "                        # Scale weight to 0.3-0.7 range to avoid extreme values\n",
    "                        scaled_weight = 0.3 + (rf_weight * 0.4)\n",
    "                        weights[bin_mask] = scaled_weight\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate bins for {feature}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def calculate_artist_similarity_boost(df_nmf, df_liked_similar, liked_artists_df, boost_factor=15):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_nmf_copy = df_nmf.copy()\n",
    "    \n",
    "    # Extract primary artists from NMF\n",
    "    df_nmf_copy['Primary Artist'] = df_nmf_copy['Artist Name(s)'].str.split(',').str[0].str.strip()\n",
    "    \n",
    "    # Extract primary artists from liked songs and albums\n",
    "    liked_artists = set(liked_artists_df['Artist Name(s)'].str.split(',').str[0].str.strip())\n",
    "    \n",
    "    # Create a dictionary of similar artists from df_liked_similar\n",
    "    similar_artists_dict = {}\n",
    "    for _, row in df_liked_similar.iterrows():\n",
    "        if pd.notna(row['Similar Artists']):\n",
    "            artist = row['Artist']\n",
    "            similar = [s.strip() for s in row['Similar Artists'].split(',')]\n",
    "            similar_artists_dict[artist] = similar\n",
    "    \n",
    "    # Flatten the similar artists dictionary to create a mapping from similar artist to liked artist\n",
    "    similar_to_liked_mapping = {}\n",
    "    for liked_artist, similar_list in similar_artists_dict.items():\n",
    "        for similar_artist in similar_list:\n",
    "            if similar_artist not in similar_to_liked_mapping:\n",
    "                similar_to_liked_mapping[similar_artist] = []\n",
    "            similar_to_liked_mapping[similar_artist].append(liked_artist)\n",
    "    \n",
    "    # Function to find the best match for an artist using fuzzy matching\n",
    "    def find_best_match(artist, artist_set, threshold=85):\n",
    "        if artist in artist_set:\n",
    "            return artist, 100\n",
    "        \n",
    "        matches = process.extractOne(artist, artist_set)\n",
    "        if matches and matches[1] >= threshold:\n",
    "            return matches[0], matches[1]\n",
    "        return None, 0\n",
    "    \n",
    "    # Add similarity boost columns directly to the dataframe\n",
    "    df_nmf_copy['similarity_boost'] = 0\n",
    "    df_nmf_copy['match_type'] = 'no_match'\n",
    "    df_nmf_copy['matched_to'] = 'None'\n",
    "    \n",
    "    # Calculate similarity boost for each NMF artist\n",
    "    for i, row in df_nmf_copy.iterrows():\n",
    "        artist = row['Primary Artist']\n",
    "        \n",
    "        # Check if the artist is directly in liked artists\n",
    "        if artist in liked_artists:\n",
    "            df_nmf_copy.at[i, 'similarity_boost'] = boost_factor\n",
    "            df_nmf_copy.at[i, 'match_type'] = 'direct_match'\n",
    "            df_nmf_copy.at[i, 'matched_to'] = artist\n",
    "            continue\n",
    "        \n",
    "        # Check if the artist is in similar_to_liked_mapping\n",
    "        if artist in similar_to_liked_mapping:\n",
    "            liked_connections = similar_to_liked_mapping[artist]\n",
    "            df_nmf_copy.at[i, 'similarity_boost'] = boost_factor * 0.8\n",
    "            df_nmf_copy.at[i, 'match_type'] = 'similar_to_liked'\n",
    "            df_nmf_copy.at[i, 'matched_to'] = ', '.join(liked_connections[:3])  # Show up to 3 connections\n",
    "            continue\n",
    "        \n",
    "        # Try fuzzy matching with similar artists\n",
    "        best_match, match_score = find_best_match(artist, set(similar_to_liked_mapping.keys()))\n",
    "        if best_match:\n",
    "            liked_connections = similar_to_liked_mapping[best_match]\n",
    "            df_nmf_copy.at[i, 'similarity_boost'] = boost_factor * 0.6 * (match_score / 100)\n",
    "            df_nmf_copy.at[i, 'match_type'] = 'fuzzy_similar_match'\n",
    "            df_nmf_copy.at[i, 'matched_to'] = f\"{best_match} ({match_score}%) -> {', '.join(liked_connections[:2])}\"\n",
    "            continue\n",
    "        \n",
    "        # Try fuzzy matching with liked artists\n",
    "        best_match, match_score = find_best_match(artist, liked_artists)\n",
    "        if best_match:\n",
    "            df_nmf_copy.at[i, 'similarity_boost'] = boost_factor * 0.7 * (match_score / 100)\n",
    "            df_nmf_copy.at[i, 'match_type'] = 'fuzzy_direct_match'\n",
    "            df_nmf_copy.at[i, 'matched_to'] = f\"{best_match} ({match_score}%)\"\n",
    "            continue\n",
    "    \n",
    "    return df_nmf_copy\n",
    "\n",
    "def get_prediction_interval(X, model, y_std, y_mean, percentile=95):\n",
    "    \"\"\"Calculate prediction intervals from Random Forest\"\"\"\n",
    "    predictions = []\n",
    "    for estimator in model.estimators_:\n",
    "        predictions.append(estimator.predict(X) * y_std + y_mean)\n",
    "    predictions = np.array(predictions)\n",
    "    lower = np.percentile(predictions, (100-percentile)/2, axis=0)\n",
    "    upper = np.percentile(predictions, 100-(100-percentile)/2, axis=0)\n",
    "    return lower, upper\n",
    "\n",
    "def apply_confidence_scaling(predictions, model, X, y_std, y_mean, min_confidence=40):\n",
    "    \"\"\"Apply confidence-based scaling to predictions\"\"\"\n",
    "    # Get prediction intervals\n",
    "    lower, upper = get_prediction_interval(X, model, y_std, y_mean)\n",
    "    uncertainty = upper - lower\n",
    "    confidence = (1 - uncertainty/uncertainty.max()) * 100\n",
    "    \n",
    "    # Scale predictions based on confidence\n",
    "    scaled_predictions = predictions.copy()\n",
    "    \n",
    "    # High confidence: minimal adjustment (90-100% of original)\n",
    "    high_conf = confidence >= min_confidence\n",
    "    scaled_predictions[high_conf] = predictions[high_conf] * (0.9 + 0.1*(confidence[high_conf]/100))\n",
    "    \n",
    "    # Low confidence: aggressive scaling (60-90% of original)\n",
    "    low_conf = confidence < min_confidence\n",
    "    scaled_predictions[low_conf] = predictions[low_conf] * (0.6 + 0.3*(confidence[low_conf]/min_confidence))\n",
    "    \n",
    "    return np.clip(scaled_predictions, None, 100), confidence\n",
    "\n",
    "# Load the similar artists data\n",
    "df_liked_similar = pd.read_csv('data/liked_artists_only_similar.csv')\n",
    "\n",
    "# Filter liked and favorite albums for similarity boost\n",
    "liked_artists_df = df[df['playlist_origin'].isin(['df_liked', 'df_fav_albums'])]\n",
    "\n",
    "# Create known artists set\n",
    "known_artists = set(df_liked_similar['Artist'])\n",
    "\n",
    "# Features used for prediction\n",
    "features = [\n",
    "    'Genres_encoded', \n",
    "    'Artist Centrality',  \n",
    "    'Record Label Frequency Encoded', \n",
    "    'mood_score', \n",
    "    'Popularity',\n",
    "    'energy_profile'\n",
    "]\n",
    "\n",
    "# Normalize the target variable\n",
    "y_mean = df['liked'].mean()\n",
    "y_std = df['liked'].std()\n",
    "y_normalized = (df['liked'] - y_mean) / y_std\n",
    "\n",
    "# Prepare training data\n",
    "X = df[features]\n",
    "y = y_normalized  # Use normalized target\n",
    "\n",
    "# Initialize models with the best parameters from tuning\n",
    "rf_model = RandomForestRegressor(**rf_params, random_state=42)\n",
    "xgb_model = XGBRegressor(**xgb_params, random_state=42)\n",
    "\n",
    "# Train models\n",
    "rf_model.fit(X, y)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "# Get feature importance from both models\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance_rf': rf_model.feature_importances_\n",
    "}).sort_values('importance_rf', ascending=False)\n",
    "\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance_xgb': xgb_model.feature_importances_\n",
    "}).sort_values('importance_xgb', ascending=False)\n",
    "\n",
    "# Combine importance scores\n",
    "feature_importance = pd.merge(rf_importance, xgb_importance, on='feature')\n",
    "feature_importance['avg_importance'] = (feature_importance['importance_rf'] + feature_importance['importance_xgb']) / 2\n",
    "feature_importance = feature_importance.sort_values('avg_importance', ascending=False)\n",
    "\n",
    "# Prepare NMF data for prediction\n",
    "df_nmf_cleaned = df_nmf[features]\n",
    "\n",
    "# Calculate similarity boost for NMF artists\n",
    "df_nmf = calculate_artist_similarity_boost(df_nmf, df_liked_similar, liked_artists_df)\n",
    "\n",
    "# Make predictions and denormalize\n",
    "rf_predictions = rf_model.predict(df_nmf_cleaned) * y_std + y_mean\n",
    "xgb_predictions = xgb_model.predict(df_nmf_cleaned) * y_std + y_mean\n",
    "\n",
    "# Calculate adaptive weights for ensemble\n",
    "adaptive_weights = calculate_adaptive_weights(X, y_normalized, df_nmf_cleaned, rf_model, xgb_model, features)\n",
    "\n",
    "# Get raw ensemble predictions\n",
    "raw_predictions = (adaptive_weights * rf_predictions + \n",
    "                  (1 - adaptive_weights) * xgb_predictions)\n",
    "\n",
    "# Apply similarity boost\n",
    "boosted_predictions = raw_predictions + df_nmf['similarity_boost']\n",
    "\n",
    "# Apply confidence scaling\n",
    "final_predictions, confidence_scores = apply_confidence_scaling(\n",
    "    boosted_predictions, \n",
    "    rf_model, \n",
    "    df_nmf_cleaned, \n",
    "    y_std, \n",
    "    y_mean,\n",
    "    min_confidence=40\n",
    ")\n",
    "\n",
    "# Store results\n",
    "df_nmf['predicted_score'] = final_predictions\n",
    "df_nmf['prediction_confidence'] = confidence_scores\n",
    "df_nmf['is_unknown_artist'] = ~df_nmf['Artist Name(s)'].isin(known_artists)\n",
    "\n",
    "# Calculate prediction intervals\n",
    "lower_bound, upper_bound = get_prediction_interval(df_nmf_cleaned, rf_model, y_std, y_mean)\n",
    "df_nmf['prediction_lower'] = lower_bound\n",
    "df_nmf['prediction_upper'] = upper_bound\n",
    "df_nmf['prediction_uncertainty'] = upper_bound - lower_bound\n",
    "\n",
    "# Get the most common release date from NMF dataset\n",
    "nmf_release_date = df_nmf['Release Date'].mode().iloc[0]\n",
    "\n",
    "# Function to find common artists across all tracks in an album\n",
    "def get_common_artists(artist_series):\n",
    "    \"\"\"Find artists common to all tracks in an album\"\"\"\n",
    "    # Handle NaN/None values\n",
    "    artist_series = artist_series.dropna()\n",
    "    if len(artist_series) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Split each string into a set of artists\n",
    "    artist_lists = []\n",
    "    for artists in artist_series:\n",
    "        try:\n",
    "            artist_lists.append(set(a.strip() for a in artists.split(',')))\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    \n",
    "    if not artist_lists:\n",
    "        return \"\"\n",
    "    \n",
    "    # Find intersection of all artist sets\n",
    "    common_artists = set.intersection(*artist_lists)\n",
    "    \n",
    "    # If no common artists, use most frequent artist\n",
    "    if not common_artists:\n",
    "        all_artists = [a for artist_set in artist_lists for a in artist_set]\n",
    "        artist_counts = Counter(all_artists)\n",
    "        \n",
    "        # Get artists that appear in all tracks\n",
    "        most_common = [a for a, cnt in artist_counts.items() \n",
    "                      if cnt == len(artist_series)]\n",
    "        \n",
    "        if most_common:\n",
    "            common_artists = set(most_common)\n",
    "        else:\n",
    "            # Fall back to single most common artist\n",
    "            common_artists = {artist_counts.most_common(1)[0][0]}\n",
    "    \n",
    "    return ', '.join(sorted(common_artists))\n",
    "    return \"\"\n",
    "\n",
    "# Group by album and aggregate data\n",
    "album_predictions = df_nmf.groupby('Album Name').agg({\n",
    "    'Artist Name(s)': get_common_artists,\n",
    "    'predicted_score': ['mean', 'std', 'count'],\n",
    "    'prediction_uncertainty': 'mean',\n",
    "    'Genres': lambda x: ' | '.join(list(set(x))[:3]),\n",
    "    'Record Label': 'first',\n",
    "    'Artist Centrality': 'first', \n",
    "    'mood_score': 'first',         \n",
    "    'energy_profile': 'first',\n",
    "    'Genres_encoded': 'first',\n",
    "    'Record Label Frequency Encoded': 'first',\n",
    "    'prediction_confidence': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "album_predictions.columns = [\n",
    "    'Album Name', 'Artist', 'avg_score', 'score_std', 'track_count',\n",
    "    'avg_uncertainty', 'Genres', 'Label', 'Artist_Centrality', \n",
    "    'Mood_Score', 'Energy_Profile', 'Genres_encoded', \n",
    "    'Record_Label_Frequency_Encoded', 'confidence_score'\n",
    "]\n",
    "\n",
    "# Calculate final confidence score\n",
    "max_std = album_predictions['score_std'].max()\n",
    "max_uncertainty = album_predictions['avg_uncertainty'].max()\n",
    "\n",
    "album_predictions['confidence_score'] = (\n",
    "    (1 - album_predictions['score_std'] / max_std) * \n",
    "    (1 - album_predictions['avg_uncertainty'] / max_uncertainty) * \n",
    "    (1 - 1/(1 + album_predictions['track_count']))\n",
    ") * 100\n",
    "\n",
    "album_predictions['confidence_score'] = np.clip(album_predictions['confidence_score'], 1, 100)\n",
    "\n",
    "# Filter and sort albums\n",
    "album_recommendations = album_predictions[album_predictions['track_count'] >= 5].sort_values('avg_score', ascending=False)\n",
    "\n",
    "# Output results\n",
    "output_columns = [\n",
    "    'Artist', 'Album Name', 'avg_score', 'confidence_score',\n",
    "    'track_count', 'Genres', 'Label', 'Artist_Centrality', \n",
    "    'Mood_Score', 'Energy_Profile'\n",
    "]\n",
    "\n",
    "final_recommendations = album_recommendations[output_columns].copy()\n",
    "final_recommendations['Artist_Centrality'] = final_recommendations['Artist_Centrality'].clip(1, 100)\n",
    "final_recommendations['Mood_Score'] = final_recommendations['Mood_Score'].clip(1, 100)\n",
    "final_recommendations['Energy_Profile'] = final_recommendations['Energy_Profile'].clip(1, 100)\n",
    "\n",
    "# Save recommendations\n",
    "date_str = datetime.strptime(nmf_release_date, '%Y-%m-%d').strftime('%m-%d-%y')\n",
    "filename = f\"predictions/{date_str}_Album_Recommendations.csv\"\n",
    "final_recommendations.round(2).to_csv(filename, index=False)\n",
    "\n",
    "# Evaluation metrics\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    y_true_denormalized = y_true * y_std + y_mean\n",
    "    y_pred_denormalized = y_pred * y_std + y_mean\n",
    "    return -mean_squared_error(y_true_denormalized, y_pred_denormalized)\n",
    "\n",
    "custom_scorer_func = make_scorer(custom_scorer, greater_is_better=False)\n",
    "rf_cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring=custom_scorer_func)\n",
    "xgb_cv_scores = cross_val_score(xgb_model, X, y, cv=5, scoring=custom_scorer_func)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n=== New Music Friday Recommendations ({nmf_release_date}) ===\")\n",
    "print(f\"Albums with 5+ tracks: {len(album_recommendations)}\")\n",
    "print(\"\\nTop 20 Albums:\")\n",
    "print(album_recommendations[['Artist', 'Album Name', 'avg_score', 'track_count']].head(20).round(2).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Model Performance ===\")\n",
    "print(f\"Random Forest CV Score: {rf_cv_scores.mean():.3f} (+/- {rf_cv_scores.std() * 2:.3f})\")\n",
    "print(f\"XGBoost CV Score: {xgb_cv_scores.mean():.3f} (+/- {xgb_cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "print(\"\\n=== Feature Importance ===\")\n",
    "print(feature_importance[['feature', 'avg_importance']].round(3).to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'], feature_importance['avg_importance'])\n",
    "plt.xlabel('Average Importance')\n",
    "plt.title('Top Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ad19c-d758-4214-b737-8ff44a13791c",
   "metadata": {},
   "source": [
    "# Display the Top Recommended Albums!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b983fa-cc7c-40a3-a922-7e6434d2b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where the predictions are saved\n",
    "predictions_folder = \"predictions/\"\n",
    "\n",
    "# Get the latest predictions file\n",
    "files = [f for f in os.listdir(predictions_folder) if f.endswith(\"_Album_Recommendations.csv\")]\n",
    "\n",
    "if files:\n",
    "    thisweek = max(files, key=lambda f: os.path.getmtime(os.path.join(predictions_folder, f)))\n",
    "    print(f\"Loaded latest file: {thisweek}\")  # Optional, just for confirmation\n",
    "    model_output_df = pd.read_csv(os.path.join(predictions_folder, thisweek))\n",
    "    model_output_df = model_output_df.sort_values(by=\"avg_score\", ascending=False)\n",
    "\n",
    "    # Display all rows\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "    model_output_df\n",
    "else:\n",
    "    thisweek = None\n",
    "    None  # Ensures no unwanted output\n",
    "\n",
    "thisweek  # Stores the filename for reference\n",
    "model_output_df  #Show the Recommendations Dataframe for This Past Week!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574853d5-e2d1-4056-9078-06dd1c8cc947",
   "metadata": {},
   "source": [
    "## Grab Album Art for the NMF Albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a20ad-63e1-4b72-aa0c-da53b59ee761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Last.fm API key\n",
    "LASTFM_API_KEY = \"74a510ecc9fc62bf3e0edc6adc2e99f9\"\n",
    "\n",
    "# Function to get similar artists using Last.fm API\n",
    "def get_similar_artists(artist: str, api_key: str, limit: int = 5) -> dict:\n",
    "    base_url = \"http://ws.audioscrobbler.com/2.0/\"\n",
    "    params = {\n",
    "        'method': 'artist.getsimilar',\n",
    "        'artist': artist,\n",
    "        'api_key': api_key,\n",
    "        'limit': limit,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'similarartists' in data and 'artist' in data['similarartists']:\n",
    "            similar_artists = [artist['name'] for artist in data['similarartists']['artist']]\n",
    "            return {'Artist': artist, 'Similar Artists': \", \".join(similar_artists[:limit]), 'status': 'success'}\n",
    "    except Exception as e:\n",
    "        return {'Artist': artist, 'Similar Artists': None, 'status': f'error: {str(e)}'}\n",
    "    \n",
    "    return {'Artist': artist, 'Similar Artists': None, 'status': 'no_results'}\n",
    "\n",
    "# Function to get album art using Apple Music API\n",
    "def get_album_art(artist: str, album: str) -> dict:\n",
    "    try:\n",
    "        url = f\"https://itunes.apple.com/search?term={artist} {album}&entity=album\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'results' in data and len(data['results']) > 0:\n",
    "            album_art = data['results'][0].get('artworkUrl100', '').replace(\"100x100\", \"600x600\")\n",
    "            return {'Artist': artist, 'Album Name': album, 'Album Art': album_art, 'status': 'success'}\n",
    "    except Exception as e:\n",
    "        return {'Artist': artist, 'Album Name': album, 'Album Art': None, 'status': f'error: {str(e)}'}\n",
    "    \n",
    "    return {'Artist': artist, 'Album Name': album, 'Album Art': None, 'status': 'no_results'}\n",
    "\n",
    "# Main function to update album data\n",
    "def update_album_data(input_file: str, album_art_file: str, similar_artists_file: str, api_key: str) -> None:\n",
    "    print(f\"\\nStarting data fetch at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    df_input = pd.read_csv(input_file)\n",
    "    album_pairs = df_input[['Primary Artist', 'Album Name']].drop_duplicates()\n",
    "    recommended_artists = df_input[df_input['playlist_origin'] == 'df_nmf']['Primary Artist'].unique()\n",
    "    \n",
    "    try:\n",
    "        existing_album_art = pd.read_csv(album_art_file)\n",
    "    except FileNotFoundError:\n",
    "        existing_album_art = pd.DataFrame(columns=['Artist', 'Album Name', 'Album Art'])\n",
    "    \n",
    "    album_pairs = album_pairs.merge(\n",
    "        existing_album_art,\n",
    "        left_on=['Primary Artist', 'Album Name'],\n",
    "        right_on=['Artist', 'Album Name'],\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "    album_pairs = album_pairs[album_pairs['_merge'] == 'left_only'].drop(columns=['_merge', 'Album Art'])\n",
    "    \n",
    "    df_album_art = pd.DataFrame(columns=['Artist', 'Album Name', 'Album Art'])\n",
    "    df_similar = pd.DataFrame(columns=['Artist', 'Similar Artists'])\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_album = {\n",
    "            executor.submit(get_album_art, row['Primary Artist'], row['Album Name']): (row['Primary Artist'], row['Album Name'])\n",
    "            for _, row in album_pairs.iterrows()\n",
    "        }\n",
    "        for future in as_completed(future_to_album):\n",
    "            result = future.result()\n",
    "            if result['status'] == 'success' and result['Album Art']:\n",
    "                df_album_art = pd.concat([df_album_art, pd.DataFrame([result])], ignore_index=True)\n",
    "            sleep(0.25)\n",
    "    \n",
    "    updated_album_art = pd.concat([existing_album_art, df_album_art], ignore_index=True)\n",
    "    updated_album_art.to_csv(album_art_file, index=False)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_artist = {\n",
    "            executor.submit(get_similar_artists, artist, api_key): artist\n",
    "            for artist in recommended_artists\n",
    "        }\n",
    "        for future in as_completed(future_to_artist):\n",
    "            result = future.result()\n",
    "            if result['status'] == 'success' and result['Similar Artists']:\n",
    "                df_similar = pd.concat([df_similar, pd.DataFrame([result])], ignore_index=True)\n",
    "            sleep(0.25)\n",
    "    \n",
    "    df_similar.to_csv(similar_artists_file, index=False)\n",
    "    print(f\"\\nFinished at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"data/df_nmf_later.csv\"  \n",
    "    album_art_file = \"data/nmf_album_covers.csv\"  \n",
    "    similar_artists_file = \"data/nmf_similar_artists.csv\"  \n",
    "    update_album_data(input_file, album_art_file, similar_artists_file, LASTFM_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025f562-96cb-4e59-a32d-6338f3a7da96",
   "metadata": {},
   "source": [
    "## Grab the Spotify Link for each New Music Friday Album üîó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126d6b2-d2bb-4c08-bc71-a12a69115c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spotify_links(batch_size=None, overwrite=False):\n",
    "    \n",
    "    output_file = 'data/nmf_album_links.csv'\n",
    "    \n",
    "    # Check if the output file already exists\n",
    "    if os.path.exists(output_file) and not overwrite:\n",
    "        print(f\"Output file {output_file} already exists.\")\n",
    "        print(\"Loading existing links file instead of regenerating...\")\n",
    "        try:\n",
    "            existing_links = pd.read_csv(output_file)\n",
    "            print(f\"Loaded {len(existing_links)} existing album links.\")\n",
    "            return existing_links\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing file: {e}\")\n",
    "            print(\"Continuing with link generation...\")\n",
    "    \n",
    "    print(\"Starting Spotify link generation...\")\n",
    "    \n",
    "    # Initialize Spotify client with your credentials\n",
    "    client_id = \"71faef9605da4db495b691d96a0daa4b\"\n",
    "    client_secret = \"832e40da22e049bba93f29d9dbeb2e62\"\n",
    "    \n",
    "    try:\n",
    "        print(\"Authenticating with Spotify...\")\n",
    "        sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret\n",
    "        ))\n",
    "        print(\"Spotify authentication successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to authenticate with Spotify: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Read the NMF data\n",
    "    try:\n",
    "        print(\"Reading NMF data...\")\n",
    "        df = pd.read_csv('data/nmf.csv')\n",
    "        print(f\"Loaded {len(df)} tracks\")\n",
    "        \n",
    "        # Print the column names to verify\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading NMF data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Get unique albums\n",
    "        print(\"Extracting unique albums...\")\n",
    "        albums = df.drop_duplicates(subset=['Album Name'], keep='first')\n",
    "        print(f\"Found {len(albums)} unique albums\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting albums: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Function to get album ID from track URI with rate limiting\n",
    "    def get_album_id(track_uri):\n",
    "        if pd.isna(track_uri):\n",
    "            print(\"Warning: Found NaN URI\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"Processing track: {track_uri}\")\n",
    "        try:\n",
    "            # Extract just the ID part from the URI (spotify:track:ID_HERE)\n",
    "            if isinstance(track_uri, str) and 'spotify:track:' in track_uri:\n",
    "                track_id = track_uri.split(':')[-1]\n",
    "            else:\n",
    "                track_id = track_uri  # If it's already an ID\n",
    "            \n",
    "            print(f\"Extracted track ID: {track_id}\")\n",
    "                \n",
    "            # Add delay to respect rate limits\n",
    "            time.sleep(0.1)  # 100ms delay between requests\n",
    "            track_info = sp.track(track_id)\n",
    "            album_id = track_info['album']['id']\n",
    "            print(f\"Found album ID: {album_id}\")\n",
    "            return album_id\n",
    "        except spotipy.exceptions.SpotifyException as e:\n",
    "            print(f\"Spotify API error: {e}\")\n",
    "            if hasattr(e, 'http_status') and e.http_status == 429:  # Too Many Requests\n",
    "                print(\"Rate limit hit, waiting longer...\")\n",
    "                time.sleep(5)  # Wait 5 seconds before retrying\n",
    "                try:\n",
    "                    track_info = sp.track(track_id)\n",
    "                    return track_info['album']['id']\n",
    "                except:\n",
    "                    print(f\"Still failed after retry for track {track_id}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"Error getting album ID for track {track_id}: {e}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error for track URI {track_uri}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Check if we actually have the Track URI column\n",
    "    track_id_column = None\n",
    "    for possible_name in ['Track URI', 'track_uri', 'Track ID', 'track_id']:\n",
    "        if possible_name in albums.columns:\n",
    "            track_id_column = possible_name\n",
    "            print(f\"Found track identifier column: {track_id_column}\")\n",
    "            break\n",
    "    \n",
    "    if track_id_column is None:\n",
    "        print(\"ERROR: Could not find a track ID or URI column. Available columns are:\")\n",
    "        print(albums.columns.tolist())\n",
    "        return None\n",
    "    \n",
    "    # Determine which albums to process\n",
    "    if batch_size is not None:\n",
    "        albums_to_process = albums.head(batch_size)\n",
    "        print(f\"Processing a batch of {len(albums_to_process)} out of {len(albums)} total albums...\")\n",
    "    else:\n",
    "        albums_to_process = albums\n",
    "        print(f\"Processing all {len(albums)} albums...\")\n",
    "    \n",
    "    # Instead of using apply, let's process one by one for better error tracking\n",
    "    albums['Album ID'] = None\n",
    "    \n",
    "    for i, (idx, row) in enumerate(albums_to_process.iterrows()):\n",
    "        print(f\"\\nProcessing album {i+1}/{len(albums_to_process)}: {row['Album Name']}\")\n",
    "        album_id = get_album_id(row[track_id_column])\n",
    "        albums.at[idx, 'Album ID'] = album_id\n",
    "    \n",
    "    # Filter out any failed lookups\n",
    "    processed_albums = albums.dropna(subset=['Album ID'])\n",
    "    \n",
    "    processed_albums['Spotify URL'] = 'open.spotify.com/album/' + processed_albums['Album ID'].astype(str)\n",
    "    \n",
    "    # Select and reorder columns for output\n",
    "    output_columns = ['Album Name', 'Artist Name(s)', 'Album ID', 'Spotify URL']\n",
    "    # Make sure all required columns exist\n",
    "    output_columns = [col for col in output_columns if col in processed_albums.columns]\n",
    "    \n",
    "    output_df = processed_albums[output_columns]\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Successfully generated album links for {len(output_df)} albums\")\n",
    "    return output_df\n",
    "\n",
    "# Run the function\n",
    "# Pass overwrite=True to regenerate links even if the file exists\n",
    "result = generate_spotify_links(overwrite=True)  # Force regenerate all links\n",
    "print(\"Function completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec73467-5b49-4809-8203-805fbf599238",
   "metadata": {},
   "source": [
    "## Save a HTML copy of this notebook at its newest! üîΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78dd78cd-cf1f-41fc-9d5e-94bc1e6a70e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML version saved to graphics/Music_Taste_Machine_Learning_Data_Prep.html\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'graphics/' directory exists\n",
    "os.makedirs('graphics', exist_ok=True)\n",
    "\n",
    "# Load the current notebook with 'utf-8' encoding\n",
    "notebook_filename = 'Music Taste Machine Learning Data Prep.ipynb'\n",
    "with open(notebook_filename, 'r', encoding='utf-8') as f:\n",
    "    notebook_content = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Export the notebook as HTML\n",
    "html_exporter = HTMLExporter()\n",
    "html_data, resources = html_exporter.from_notebook_node(notebook_content)\n",
    "\n",
    "# Save the HTML to the 'graphics/' folder\n",
    "output_filename = 'graphics/Music_Taste_Machine_Learning_Data_Prep.html'\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(html_data)\n",
    "\n",
    "print(f\"HTML version saved to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
