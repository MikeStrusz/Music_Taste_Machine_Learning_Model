# Quick fix - remove the extra './data_clean_2025_validation/' prefix
import os

# Check what's actually in the data folder
print("Files in data folder:")
data_files = os.listdir('./data_clean_2025_validation')
for file in data_files:
    print(f"  {file}")

# The issue is the notebook has paths like: './data_clean_2025_validation/./data_clean_2025_validation/liked.csv'
# Let's create a quick fix script
fix_script = """
import pandas as pd
import os

# Manual file loading with correct paths
liked_albums = pd.read_csv('./data_clean_2025_validation/liked_albums.csv')
liked_songs = pd.read_csv('./data_clean_2025_validation/liked.csv')
filtered_data = pd.read_csv('./data_clean_2025_validation/filtered_data.csv')

print("âœ… Files loaded successfully!")
print(f"liked_albums: {len(liked_albums)} rows")
print(f"liked_songs: {len(liked_songs)} rows") 
print(f"filtered_data: {len(filtered_data)} rows")
"""

with open('quick_fix.py', 'w') as f:
    f.write(fix_script)

print("âœ… Created quick_fix.py - run this to test file loading")


import shutil

# Check if did_not_like.csv exists in original data folder
original_did_not_like = r'C:\Users\mrstr\Downloads\9_Module_Tableau\Capstone\Music_Taste_Machine_Learning_Model\data\did_not_like.csv'
clean_folder = './data_clean_2025_validation/'

if os.path.exists(original_did_not_like):
    # Copy it to clean folder
    shutil.copy2(original_did_not_like, clean_folder)
    print(f"âœ… Copied did_not_like.csv to clean folder")
else:
    print(f"âŒ did_not_like.csv not found in original data folder")

# List all files in clean folder to see what we have
print(f"\nðŸ“ Files in clean folder:")
clean_files = os.listdir(clean_folder)
for file in clean_files:
    print(f"  {file}")


import pandas as pd

# Clean the additional files that have 2025 data
additional_files_to_clean = ['did_not_like.csv', 'mid.csv', 'mid_-_albums_not_good__not_bad.csv']

print("ðŸ§¹ CLEANING ADDITIONAL FILES WITH 2025 DATA")
print("="*50)

for file in additional_files_to_clean:
    original_path = os.path.join(r'C:\Users\mrstr\Downloads\9_Module_Tableau\Capstone\Music_Taste_Machine_Learning_Model\data', file)
    
    if os.path.exists(original_path):
        df = pd.read_csv(original_path)
        original_count = len(df)
        
        # Remove 2025 data
        if 'Release Date' in df.columns:
            df_clean = df[~df['Release Date'].astype(str).str.contains('2025', na=False)]
        elif 'Added At' in df.columns:
            df_clean = df[~df['Added At'].astype(str).str.contains('2025', na=False)]
        else:
            df_clean = df
        
        cleaned_count = len(df_clean)
        removed_count = original_count - cleaned_count
        
        # Save to clean folder
        clean_path = os.path.join('./data_clean_2025_validation', file)
        df_clean.to_csv(clean_path, index=False)
        
        print(f"âœ… {file}: {original_count} â†’ {cleaned_count} (removed {removed_count} 2025 entries)")
    else:
        print(f"âŒ {file} not found")

print(f"\nðŸŽ¯ ALL FILES NOW CLEANED OF 2025 DATA LEAKAGE!")


import os
import glob

# Find all .ipynb files
notebook_files = glob.glob('**/*.ipynb', recursive=True)

for notebook in notebook_files:
    print(f"ðŸ” Checking: {notebook}")
    
    # Read notebook
    with open(notebook, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Find bad paths
    bad_paths = []
    if './data_clean_2025_validation/./data_clean_2025_validation/' in content:
        bad_paths.append('Double path issue found!')
    
    # Count occurrences
    double_path_count = content.count('./data_clean_2025_validation/./data_clean_2025_validation/')
    
    if double_path_count > 0:
        print(f"   âŒ Found {double_path_count} double paths")
        
        # Fix them
        fixed_content = content.replace(
            './data_clean_2025_validation/./data_clean_2025_validation/', 
            './data_clean_2025_validation/'
        )
        
        # Save fixed version
        with open(notebook, 'w', encoding='utf-8') as f:
            f.write(fixed_content)
        print(f"   âœ… Fixed {double_path_count} paths")
    else:
        print("   âœ… No double paths found")

print("\nðŸŽ‰ All double paths should be fixed!")


import pandas as pd

# Load the new predictions
new_predictions = pd.read_csv('./predictions/02-07-25_Album_Recommendations.csv')
print(f"ðŸ“Š NEW PREDICTIONS SHAPE: {new_predictions.shape}")

# Load your actual 2025 listening
actual_2025 = pd.read_csv(r'C:\Users\mrstr\Downloads\2025.csv')
actual_albums = actual_2025[['Album Name', 'Artist Name(s)']].drop_duplicates()
print(f"ðŸŽµ ACTUAL 2025 ALBUMS LISTENED TO: {len(actual_albums)}")

# Calculate true accuracy
new_predictions['match_key'] = new_predictions['Artist'] + " | " + new_predictions['Album Name']
actual_albums['match_key'] = actual_albums['Artist Name(s)'] + " | " + actual_albums['Album Name']

matches = new_predictions[new_predictions['match_key'].isin(actual_albums['match_key'])]
true_accuracy = len(matches) / len(actual_albums) * 100

print(f"\nðŸŽ¯ TRUE ACCURACY (NO DATA LEAKAGE): {true_accuracy:.1f}%")
print(f"   Correct predictions: {len(matches)}/{len(actual_albums)}")

# Compare with old inflated accuracy
print(f"ðŸ“ˆ COMPARISON:")
print(f"   Old accuracy (with leakage): 35.7%")
print(f"   New accuracy (clean): {true_accuracy:.1f}%")

# Show top predictions you actually listened to
if len(matches) > 0:
    print(f"\nðŸŽ‰ SUCCESSFUL PREDICTIONS:")
    successful = matches.nlargest(10, 'avg_score')
    for i, (_, album) in enumerate(successful.iterrows(), 1):
        print(f"   {i}. {album['Artist']} - {album['Album Name']} (Score: {album['avg_score']:.1f})")


# Let's get the REAL count of unique 2025 albums you actually listened to
actual_2025 = pd.read_csv(r'C:\Users\mrstr\Downloads\2025.csv')
unique_actual_albums = actual_2025[['Album Name', 'Artist Name(s)']].drop_duplicates()
true_album_count = len(unique_actual_albums)

print(f"ðŸŽµ ACTUAL UNIQUE 2025 ALBUMS LISTENED TO: {true_album_count}")
print(f"ðŸ“Š Your model predicted {len(matches)}/{true_album_count} correctly")
print(f"ðŸŽ¯ TRUE ACCURACY: {len(matches)/true_album_count*100:.1f}%")


# Proper unique album counting
actual_2025 = pd.read_csv(r'C:\Users\mrstr\Downloads\2025.csv')

# Clean artist names - take only primary artist (before first ; or , for features)
actual_2025['Primary Artist'] = actual_2025['Artist Name(s)'].str.split(';').str[0].str.split(',').str[0].str.strip()

# Create proper unique album key (Primary Artist + Album Name)
actual_2025['album_key'] = actual_2025['Primary Artist'] + " | " + actual_2025['Album Name']

# Get true unique albums
true_unique_albums = actual_2025[['album_key', 'Primary Artist', 'Album Name']].drop_duplicates()
true_album_count = len(true_unique_albums)

print(f"ðŸŽµ TRUE UNIQUE 2025 ALBUMS (proper counting): {true_album_count}")
print(f"   (Accounting for features, soundtracks, etc.)")

# Now recalculate accuracy with proper matching
new_predictions['primary_artist'] = new_predictions['Artist'].str.split(';').str[0].str.split(',').str[0].str.strip()
new_predictions['album_key'] = new_predictions['primary_artist'] + " | " + new_predictions['Album Name']

# Match with proper album keys
matches_clean = new_predictions[new_predictions['album_key'].isin(true_unique_albums['album_key'])]
true_accuracy_clean = len(matches_clean) / true_album_count * 100

print(f"\nðŸŽ¯ TRUE ACCURACY (proper counting): {true_accuracy_clean:.1f}%")
print(f"   Correct predictions: {len(matches_clean)}/{true_album_count}")

# Show the actual unique album count
print(f"\nðŸ“Š SAMPLE OF UNIQUE ALBUMS:")
print(true_unique_albums[['Primary Artist', 'Album Name']].head(10).to_string(index=False))


# Analyze the prediction file
pred_file = './predictions/02-07-25_Album_Recommendations.csv'
df_preds = pd.read_csv(pred_file)

print("ðŸ“Š PREDICTION FILE ANALYSIS")
print("="*50)
print(f"File: {pred_file}")
print(f"Shape: {df_preds.shape}")
print(f"Albums predicted: {len(df_preds)}")

print(f"\nðŸ“‹ DATA DICTIONARY:")
print("="*50)
for col in df_preds.columns:
    dtype = df_preds[col].dtype
    sample = df_preds[col].iloc[0] if len(df_preds) > 0 else "N/A"
    print(f"â€¢ {col} ({dtype}): {sample}")

print(f"\nðŸ“ˆ SCORE DISTRIBUTION:")
print("="*50)
print(f"avg_score range: {df_preds['avg_score'].min():.1f} - {df_preds['avg_score'].max():.1f}")
print(f"confidence_score range: {df_preds['confidence_score'].min():.1f} - {df_preds['confidence_score'].max():.1f}")
print(f"Artist_Centrality range: {df_preds['Artist_Centrality'].min():.1f} - {df_preds['Artist_Centrality'].max():.1f}")
print(f"Mood_Score range: {df_preds['Mood_Score'].min():.1f} - {df_preds['Mood_Score'].max():.1f}")
print(f"Energy_Profile range: {df_preds['Energy_Profile'].min():.1f} - {df_preds['Energy_Profile'].max():.1f}")

print(f"\nðŸŽµ TOP 10 PREDICTED ALBUMS:")
print("="*50)
top_10 = df_preds.nlargest(10, 'avg_score')[['Artist', 'Album Name', 'avg_score', 'Genres']]
for i, (_, album) in enumerate(top_10.iterrows(), 1):
    print(f"{i:2d}. {album['Artist']} - {album['Album Name']} ({album['avg_score']:.1f})")

# How to use as feature in end-of-year model
print(f"\nðŸš€ INTEGRATION WITH END-OF-YEAR MODEL:")
print("="*50)
print("""
USE CASES AS FEATURES:

1. EARLY_SIGNAL_STRENGTH:
   - Use avg_score as early prediction confidence
   - Albums with high scores in Feb are strong candidates

2. PREDICTION_TRAJECTORY:  
   - Track how predictions change over weekly runs
   - Albums that maintain high scores = reliable predictions

3. GENRE_ALIGNMENT:
   - Compare predicted genres with actual end-of-year preferences
   - See which genre predictions were most accurate

4. ARTIST_DISCOVERY_POWER:
   - Use Artist_Centrality to find new artists you ended up loving
   - High centrality + actual listening = successful discovery

CONCRETE INTEGRATION:

# Add to your training data as features:
df['feb_prediction_score'] = # Map from this file
df['feb_confidence'] = # Map confidence_score  
df['predicted_mood'] = # Map Mood_Score
df['predicted_energy'] = # Map Energy_Profile
df['early_artist_centrality'] = # Map Artist_Centrality

# Then train model to see if early predictions improve end-of-year accuracy!
""")
