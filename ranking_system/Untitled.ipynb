{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f2906c-b1ae-4dd3-9428-f30ed415ad83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in data folder:\n",
      "  album_metadata_cache.csv\n",
      "  apple_music_similar_artists.csv\n",
      "  artist_ids.csv\n",
      "  df_cleaned_pre_standardized.csv\n",
      "  filtered_data.csv\n",
      "  liked.csv\n",
      "  liked_albums.csv\n",
      "  nmf_similar_artists.csv\n",
      "  obscure_artists_mike_likes.csv\n",
      "  ten_genres.csv\n",
      "  unique_artists.csv\n",
      "  validation_2025_albums.csv\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u2705' in position 312: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m\n\u001b[0;32m     12\u001b[0m fix_script \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124mimport pandas as pd\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124mimport os\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124mprint(f\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiltered_data: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mlen(filtered_data)} rows\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquick_fix.py\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 28\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(fix_script)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Created quick_fix.py - run this to test file loading\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_encode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,encoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u2705' in position 312: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# Quick fix - remove the extra './data_clean_2025_validation/' prefix\n",
    "import os\n",
    "\n",
    "# Check what's actually in the data folder\n",
    "print(\"Files in data folder:\")\n",
    "data_files = os.listdir('./data_clean_2025_validation')\n",
    "for file in data_files:\n",
    "    print(f\"  {file}\")\n",
    "\n",
    "# The issue is the notebook has paths like: './data_clean_2025_validation/./data_clean_2025_validation/liked.csv'\n",
    "# Let's create a quick fix script\n",
    "fix_script = \"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Manual file loading with correct paths\n",
    "liked_albums = pd.read_csv('./data_clean_2025_validation/liked_albums.csv')\n",
    "liked_songs = pd.read_csv('./data_clean_2025_validation/liked.csv')\n",
    "filtered_data = pd.read_csv('./data_clean_2025_validation/filtered_data.csv')\n",
    "\n",
    "print(\"âœ… Files loaded successfully!\")\n",
    "print(f\"liked_albums: {len(liked_albums)} rows\")\n",
    "print(f\"liked_songs: {len(liked_songs)} rows\") \n",
    "print(f\"filtered_data: {len(filtered_data)} rows\")\n",
    "\"\"\"\n",
    "\n",
    "with open('quick_fix.py', 'w') as f:\n",
    "    f.write(fix_script)\n",
    "\n",
    "print(\"âœ… Created quick_fix.py - run this to test file loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b29494a-264a-4af3-b56a-4b251371accc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Copied did_not_like.csv to clean folder\n",
      "\n",
      "ðŸ“ Files in clean folder:\n",
      "  album_metadata_cache.csv\n",
      "  apple_music_similar_artists.csv\n",
      "  artist_ids.csv\n",
      "  df_cleaned_pre_standardized.csv\n",
      "  did_not_like.csv\n",
      "  filtered_data.csv\n",
      "  liked.csv\n",
      "  liked_albums.csv\n",
      "  nmf_similar_artists.csv\n",
      "  obscure_artists_mike_likes.csv\n",
      "  ten_genres.csv\n",
      "  unique_artists.csv\n",
      "  validation_2025_albums.csv\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Check if did_not_like.csv exists in original data folder\n",
    "original_did_not_like = r'C:\\Users\\mrstr\\Downloads\\9_Module_Tableau\\Capstone\\Music_Taste_Machine_Learning_Model\\data\\did_not_like.csv'\n",
    "clean_folder = './data_clean_2025_validation/'\n",
    "\n",
    "if os.path.exists(original_did_not_like):\n",
    "    # Copy it to clean folder\n",
    "    shutil.copy2(original_did_not_like, clean_folder)\n",
    "    print(f\"âœ… Copied did_not_like.csv to clean folder\")\n",
    "else:\n",
    "    print(f\"âŒ did_not_like.csv not found in original data folder\")\n",
    "\n",
    "# List all files in clean folder to see what we have\n",
    "print(f\"\\nðŸ“ Files in clean folder:\")\n",
    "clean_files = os.listdir(clean_folder)\n",
    "for file in clean_files:\n",
    "    print(f\"  {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "258860c1-9a38-406b-b5ea-53a6c59f0269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ CLEANING ADDITIONAL FILES WITH 2025 DATA\n",
      "==================================================\n",
      "âœ… did_not_like.csv: 1780 â†’ 1060 (removed 720 2025 entries)\n",
      "âœ… mid.csv: 1149 â†’ 9 (removed 1140 2025 entries)\n",
      "âœ… mid_-_albums_not_good__not_bad.csv: 954 â†’ 9 (removed 945 2025 entries)\n",
      "\n",
      "ðŸŽ¯ ALL FILES NOW CLEANED OF 2025 DATA LEAKAGE!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Clean the additional files that have 2025 data\n",
    "additional_files_to_clean = ['did_not_like.csv', 'mid.csv', 'mid_-_albums_not_good__not_bad.csv']\n",
    "\n",
    "print(\"ðŸ§¹ CLEANING ADDITIONAL FILES WITH 2025 DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for file in additional_files_to_clean:\n",
    "    original_path = os.path.join(r'C:\\Users\\mrstr\\Downloads\\9_Module_Tableau\\Capstone\\Music_Taste_Machine_Learning_Model\\data', file)\n",
    "    \n",
    "    if os.path.exists(original_path):\n",
    "        df = pd.read_csv(original_path)\n",
    "        original_count = len(df)\n",
    "        \n",
    "        # Remove 2025 data\n",
    "        if 'Release Date' in df.columns:\n",
    "            df_clean = df[~df['Release Date'].astype(str).str.contains('2025', na=False)]\n",
    "        elif 'Added At' in df.columns:\n",
    "            df_clean = df[~df['Added At'].astype(str).str.contains('2025', na=False)]\n",
    "        else:\n",
    "            df_clean = df\n",
    "        \n",
    "        cleaned_count = len(df_clean)\n",
    "        removed_count = original_count - cleaned_count\n",
    "        \n",
    "        # Save to clean folder\n",
    "        clean_path = os.path.join('./data_clean_2025_validation', file)\n",
    "        df_clean.to_csv(clean_path, index=False)\n",
    "        \n",
    "        print(f\"âœ… {file}: {original_count} â†’ {cleaned_count} (removed {removed_count} 2025 entries)\")\n",
    "    else:\n",
    "        print(f\"âŒ {file} not found\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ ALL FILES NOW CLEANED OF 2025 DATA LEAKAGE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b700cd2-df0d-4985-be97-b1f66e049d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking: Music_Taste_Model_Clean_Validation.ipynb\n",
      "   âŒ Found 14 double paths\n",
      "   âœ… Fixed 14 paths\n",
      "ðŸ” Checking: Untitled.ipynb\n",
      "   âŒ Found 1 double paths\n",
      "   âœ… Fixed 1 paths\n",
      "\n",
      "ðŸŽ‰ All double paths should be fixed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Find all .ipynb files\n",
    "notebook_files = glob.glob('**/*.ipynb', recursive=True)\n",
    "\n",
    "for notebook in notebook_files:\n",
    "    print(f\"ðŸ” Checking: {notebook}\")\n",
    "    \n",
    "    # Read notebook\n",
    "    with open(notebook, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Find bad paths\n",
    "    bad_paths = []\n",
    "    if './data_clean_2025_validation/./data_clean_2025_validation/' in content:\n",
    "        bad_paths.append('Double path issue found!')\n",
    "    \n",
    "    # Count occurrences\n",
    "    double_path_count = content.count('./data_clean_2025_validation/./data_clean_2025_validation/')\n",
    "    \n",
    "    if double_path_count > 0:\n",
    "        print(f\"   âŒ Found {double_path_count} double paths\")\n",
    "        \n",
    "        # Fix them\n",
    "        fixed_content = content.replace(\n",
    "            './data_clean_2025_validation/./data_clean_2025_validation/', \n",
    "            './data_clean_2025_validation/'\n",
    "        )\n",
    "        \n",
    "        # Save fixed version\n",
    "        with open(notebook, 'w', encoding='utf-8') as f:\n",
    "            f.write(fixed_content)\n",
    "        print(f\"   âœ… Fixed {double_path_count} paths\")\n",
    "    else:\n",
    "        print(\"   âœ… No double paths found\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All double paths should be fixed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fd02a37-ee93-491d-9703-cdc3616bf203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š NEW PREDICTIONS SHAPE: (253, 10)\n",
      "ðŸŽµ ACTUAL 2025 ALBUMS LISTENED TO: 610\n",
      "\n",
      "ðŸŽ¯ TRUE ACCURACY (NO DATA LEAKAGE): 41.3%\n",
      "   Correct predictions: 252/610\n",
      "ðŸ“ˆ COMPARISON:\n",
      "   Old accuracy (with leakage): 35.7%\n",
      "   New accuracy (clean): 41.3%\n",
      "\n",
      "ðŸŽ‰ SUCCESSFUL PREDICTIONS:\n",
      "   1. Vincent Lima - To Love A Thing That Fades (Score: 100.0)\n",
      "   2. Hand Habits - Blue Reminder (Score: 100.0)\n",
      "   3. Runaway Brother - Want You Need (Score: 100.0)\n",
      "   4. The Loft - Everything Changes Everything Stays The Same (Score: 100.0)\n",
      "   5. SAINt JHN - FESTIVAL SEASON (Score: 100.0)\n",
      "   6. Ellie Holcomb - Far Country (Score: 100.0)\n",
      "   7. Lucy Dacus - Forever Is A Feeling (Score: 100.0)\n",
      "   8. Matt Maltese - Hers (Score: 100.0)\n",
      "   9. Sydney Rose - I Know What I Want (Score: 100.0)\n",
      "   10. Moontype - I Let the Wind Push Down On Me (Score: 100.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new predictions\n",
    "new_predictions = pd.read_csv('./predictions/02-07-25_Album_Recommendations.csv')\n",
    "print(f\"ðŸ“Š NEW PREDICTIONS SHAPE: {new_predictions.shape}\")\n",
    "\n",
    "# Load your actual 2025 listening\n",
    "actual_2025 = pd.read_csv(r'C:\\Users\\mrstr\\Downloads\\2025.csv')\n",
    "actual_albums = actual_2025[['Album Name', 'Artist Name(s)']].drop_duplicates()\n",
    "print(f\"ðŸŽµ ACTUAL 2025 ALBUMS LISTENED TO: {len(actual_albums)}\")\n",
    "\n",
    "# Calculate true accuracy\n",
    "new_predictions['match_key'] = new_predictions['Artist'] + \" | \" + new_predictions['Album Name']\n",
    "actual_albums['match_key'] = actual_albums['Artist Name(s)'] + \" | \" + actual_albums['Album Name']\n",
    "\n",
    "matches = new_predictions[new_predictions['match_key'].isin(actual_albums['match_key'])]\n",
    "true_accuracy = len(matches) / len(actual_albums) * 100\n",
    "\n",
    "print(f\"\\nðŸŽ¯ TRUE ACCURACY (NO DATA LEAKAGE): {true_accuracy:.1f}%\")\n",
    "print(f\"   Correct predictions: {len(matches)}/{len(actual_albums)}\")\n",
    "\n",
    "# Compare with old inflated accuracy\n",
    "print(f\"ðŸ“ˆ COMPARISON:\")\n",
    "print(f\"   Old accuracy (with leakage): 35.7%\")\n",
    "print(f\"   New accuracy (clean): {true_accuracy:.1f}%\")\n",
    "\n",
    "# Show top predictions you actually listened to\n",
    "if len(matches) > 0:\n",
    "    print(f\"\\nðŸŽ‰ SUCCESSFUL PREDICTIONS:\")\n",
    "    successful = matches.nlargest(10, 'avg_score')\n",
    "    for i, (_, album) in enumerate(successful.iterrows(), 1):\n",
    "        print(f\"   {i}. {album['Artist']} - {album['Album Name']} (Score: {album['avg_score']:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49a7ed06-e37f-4686-a971-7d7339222036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽµ ACTUAL UNIQUE 2025 ALBUMS LISTENED TO: 610\n",
      "ðŸ“Š Your model predicted 252/610 correctly\n",
      "ðŸŽ¯ TRUE ACCURACY: 41.3%\n"
     ]
    }
   ],
   "source": [
    "# Let's get the REAL count of unique 2025 albums you actually listened to\n",
    "actual_2025 = pd.read_csv(r'C:\\Users\\mrstr\\Downloads\\2025.csv')\n",
    "unique_actual_albums = actual_2025[['Album Name', 'Artist Name(s)']].drop_duplicates()\n",
    "true_album_count = len(unique_actual_albums)\n",
    "\n",
    "print(f\"ðŸŽµ ACTUAL UNIQUE 2025 ALBUMS LISTENED TO: {true_album_count}\")\n",
    "print(f\"ðŸ“Š Your model predicted {len(matches)}/{true_album_count} correctly\")\n",
    "print(f\"ðŸŽ¯ TRUE ACCURACY: {len(matches)/true_album_count*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "461597f2-a154-46cc-9aed-54f61a9c674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽµ TRUE UNIQUE 2025 ALBUMS (proper counting): 283\n",
      "   (Accounting for features, soundtracks, etc.)\n",
      "\n",
      "ðŸŽ¯ TRUE ACCURACY (proper counting): 89.4%\n",
      "   Correct predictions: 253/283\n",
      "\n",
      "ðŸ“Š SAMPLE OF UNIQUE ALBUMS:\n",
      "   Primary Artist                    Album Name\n",
      "        The Hunts             Hibernating Heart\n",
      "      Jim Nothing          Grey Eyes, Grey Lynn\n",
      "  Franz Ferdinand                The Human Fear\n",
      "Moonchild Sanelly                     Full Moon\n",
      "       Mac Miller                  Balloonerism\n",
      "            MEGGO eavesdropper ;; death stories\n",
      "  Charlie Houston               Big After I Die\n",
      "      Circa Waves           Death & Love, Pt. 1\n",
      "  Heather Maloney                Exploding Star\n",
      "Penny and Sparrow                         Lefty\n"
     ]
    }
   ],
   "source": [
    "# Proper unique album counting\n",
    "actual_2025 = pd.read_csv(r'C:\\Users\\mrstr\\Downloads\\2025.csv')\n",
    "\n",
    "# Clean artist names - take only primary artist (before first ; or , for features)\n",
    "actual_2025['Primary Artist'] = actual_2025['Artist Name(s)'].str.split(';').str[0].str.split(',').str[0].str.strip()\n",
    "\n",
    "# Create proper unique album key (Primary Artist + Album Name)\n",
    "actual_2025['album_key'] = actual_2025['Primary Artist'] + \" | \" + actual_2025['Album Name']\n",
    "\n",
    "# Get true unique albums\n",
    "true_unique_albums = actual_2025[['album_key', 'Primary Artist', 'Album Name']].drop_duplicates()\n",
    "true_album_count = len(true_unique_albums)\n",
    "\n",
    "print(f\"ðŸŽµ TRUE UNIQUE 2025 ALBUMS (proper counting): {true_album_count}\")\n",
    "print(f\"   (Accounting for features, soundtracks, etc.)\")\n",
    "\n",
    "# Now recalculate accuracy with proper matching\n",
    "new_predictions['primary_artist'] = new_predictions['Artist'].str.split(';').str[0].str.split(',').str[0].str.strip()\n",
    "new_predictions['album_key'] = new_predictions['primary_artist'] + \" | \" + new_predictions['Album Name']\n",
    "\n",
    "# Match with proper album keys\n",
    "matches_clean = new_predictions[new_predictions['album_key'].isin(true_unique_albums['album_key'])]\n",
    "true_accuracy_clean = len(matches_clean) / true_album_count * 100\n",
    "\n",
    "print(f\"\\nðŸŽ¯ TRUE ACCURACY (proper counting): {true_accuracy_clean:.1f}%\")\n",
    "print(f\"   Correct predictions: {len(matches_clean)}/{true_album_count}\")\n",
    "\n",
    "# Show the actual unique album count\n",
    "print(f\"\\nðŸ“Š SAMPLE OF UNIQUE ALBUMS:\")\n",
    "print(true_unique_albums[['Primary Artist', 'Album Name']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5921d0bb-e6ad-4d5a-a4e2-814f182c3491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š PREDICTION FILE ANALYSIS\n",
      "==================================================\n",
      "File: ./predictions/02-07-25_Album_Recommendations.csv\n",
      "Shape: (253, 10)\n",
      "Albums predicted: 253\n",
      "\n",
      "ðŸ“‹ DATA DICTIONARY:\n",
      "==================================================\n",
      "â€¢ Artist (object): Vincent Lima\n",
      "â€¢ Album Name (object): To Love A Thing That Fades\n",
      "â€¢ avg_score (float64): 100.0\n",
      "â€¢ confidence_score (float64): 93.75\n",
      "â€¢ track_count (int64): 15\n",
      "â€¢ Genres (object): folk, pop\n",
      "â€¢ Label (object): Blue Shutters Music LLC / Island Records\n",
      "â€¢ Artist_Centrality (float64): 20.17\n",
      "â€¢ Mood_Score (float64): 21.3\n",
      "â€¢ Energy_Profile (float64): 68.41\n",
      "\n",
      "ðŸ“ˆ SCORE DISTRIBUTION:\n",
      "==================================================\n",
      "avg_score range: 31.8 - 100.0\n",
      "confidence_score range: 1.0 - 97.3\n",
      "Artist_Centrality range: 1.0 - 66.9\n",
      "Mood_Score range: 6.3 - 86.6\n",
      "Energy_Profile range: 18.9 - 84.5\n",
      "\n",
      "ðŸŽµ TOP 10 PREDICTED ALBUMS:\n",
      "==================================================\n",
      " 1. Vincent Lima - To Love A Thing That Fades (100.0)\n",
      " 2. Hand Habits - Blue Reminder (100.0)\n",
      " 3. Runaway Brother - Want You Need (100.0)\n",
      " 4. The Loft - Everything Changes Everything Stays The Same (100.0)\n",
      " 5. SAINt JHN - FESTIVAL SEASON (100.0)\n",
      " 6. Ellie Holcomb - Far Country (100.0)\n",
      " 7. Lucy Dacus - Forever Is A Feeling (100.0)\n",
      " 8. Matt Maltese - Hers (100.0)\n",
      " 9. Sydney Rose - I Know What I Want (100.0)\n",
      "10. Moontype - I Let the Wind Push Down On Me (100.0)\n",
      "\n",
      "ðŸš€ INTEGRATION WITH END-OF-YEAR MODEL:\n",
      "==================================================\n",
      "\n",
      "USE CASES AS FEATURES:\n",
      "\n",
      "1. EARLY_SIGNAL_STRENGTH:\n",
      "   - Use avg_score as early prediction confidence\n",
      "   - Albums with high scores in Feb are strong candidates\n",
      "\n",
      "2. PREDICTION_TRAJECTORY:  \n",
      "   - Track how predictions change over weekly runs\n",
      "   - Albums that maintain high scores = reliable predictions\n",
      "\n",
      "3. GENRE_ALIGNMENT:\n",
      "   - Compare predicted genres with actual end-of-year preferences\n",
      "   - See which genre predictions were most accurate\n",
      "\n",
      "4. ARTIST_DISCOVERY_POWER:\n",
      "   - Use Artist_Centrality to find new artists you ended up loving\n",
      "   - High centrality + actual listening = successful discovery\n",
      "\n",
      "CONCRETE INTEGRATION:\n",
      "\n",
      "# Add to your training data as features:\n",
      "df['feb_prediction_score'] = # Map from this file\n",
      "df['feb_confidence'] = # Map confidence_score  \n",
      "df['predicted_mood'] = # Map Mood_Score\n",
      "df['predicted_energy'] = # Map Energy_Profile\n",
      "df['early_artist_centrality'] = # Map Artist_Centrality\n",
      "\n",
      "# Then train model to see if early predictions improve end-of-year accuracy!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze the prediction file\n",
    "pred_file = './predictions/02-07-25_Album_Recommendations.csv'\n",
    "df_preds = pd.read_csv(pred_file)\n",
    "\n",
    "print(\"ðŸ“Š PREDICTION FILE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"File: {pred_file}\")\n",
    "print(f\"Shape: {df_preds.shape}\")\n",
    "print(f\"Albums predicted: {len(df_preds)}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ DATA DICTIONARY:\")\n",
    "print(\"=\"*50)\n",
    "for col in df_preds.columns:\n",
    "    dtype = df_preds[col].dtype\n",
    "    sample = df_preds[col].iloc[0] if len(df_preds) > 0 else \"N/A\"\n",
    "    print(f\"â€¢ {col} ({dtype}): {sample}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ SCORE DISTRIBUTION:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"avg_score range: {df_preds['avg_score'].min():.1f} - {df_preds['avg_score'].max():.1f}\")\n",
    "print(f\"confidence_score range: {df_preds['confidence_score'].min():.1f} - {df_preds['confidence_score'].max():.1f}\")\n",
    "print(f\"Artist_Centrality range: {df_preds['Artist_Centrality'].min():.1f} - {df_preds['Artist_Centrality'].max():.1f}\")\n",
    "print(f\"Mood_Score range: {df_preds['Mood_Score'].min():.1f} - {df_preds['Mood_Score'].max():.1f}\")\n",
    "print(f\"Energy_Profile range: {df_preds['Energy_Profile'].min():.1f} - {df_preds['Energy_Profile'].max():.1f}\")\n",
    "\n",
    "print(f\"\\nðŸŽµ TOP 10 PREDICTED ALBUMS:\")\n",
    "print(\"=\"*50)\n",
    "top_10 = df_preds.nlargest(10, 'avg_score')[['Artist', 'Album Name', 'avg_score', 'Genres']]\n",
    "for i, (_, album) in enumerate(top_10.iterrows(), 1):\n",
    "    print(f\"{i:2d}. {album['Artist']} - {album['Album Name']} ({album['avg_score']:.1f})\")\n",
    "\n",
    "# How to use as feature in end-of-year model\n",
    "print(f\"\\nðŸš€ INTEGRATION WITH END-OF-YEAR MODEL:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "USE CASES AS FEATURES:\n",
    "\n",
    "1. EARLY_SIGNAL_STRENGTH:\n",
    "   - Use avg_score as early prediction confidence\n",
    "   - Albums with high scores in Feb are strong candidates\n",
    "\n",
    "2. PREDICTION_TRAJECTORY:  \n",
    "   - Track how predictions change over weekly runs\n",
    "   - Albums that maintain high scores = reliable predictions\n",
    "\n",
    "3. GENRE_ALIGNMENT:\n",
    "   - Compare predicted genres with actual end-of-year preferences\n",
    "   - See which genre predictions were most accurate\n",
    "\n",
    "4. ARTIST_DISCOVERY_POWER:\n",
    "   - Use Artist_Centrality to find new artists you ended up loving\n",
    "   - High centrality + actual listening = successful discovery\n",
    "\n",
    "CONCRETE INTEGRATION:\n",
    "\n",
    "# Add to your training data as features:\n",
    "df['feb_prediction_score'] = # Map from this file\n",
    "df['feb_confidence'] = # Map confidence_score  \n",
    "df['predicted_mood'] = # Map Mood_Score\n",
    "df['predicted_energy'] = # Map Energy_Profile\n",
    "df['early_artist_centrality'] = # Map Artist_Centrality\n",
    "\n",
    "# Then train model to see if early predictions improve end-of-year accuracy!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
