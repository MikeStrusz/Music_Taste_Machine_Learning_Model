





# Standard Library Imports
import os
import csv
import openpyxl
from datetime import datetime
from time import sleep
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple, Optional

# Third-Party Imports
import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
from textblob import TextBlob
from transformers import pipeline
from collections import Counter

# Fuzzy Matching
from fuzzywuzzy import fuzz, process

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, make_scorer
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from scipy.stats import randint, uniform

# Network Analysis
import networkx as nx

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from tqdm import tqdm
from nbconvert import HTMLExporter
import nbformat

# Streamlit 
import streamlit as st

# Suppress all warnings
import warnings
warnings.filterwarnings('ignore')






df_liked = pd.read_csv("data/liked.csv")  # For PageRank only
df_training = pd.read_csv("data/2026_training_complete_with_features.csv")  # NEW!
df_nmf = pd.read_csv("data/nmf.csv")  # Test set
df_liked_similar = pd.read_csv('data/liked_artists_only_similar.csv')





# Initialize LastFM API client with rate limiting and request limits
class LastFMAPI:
    def __init__(self, api_key: str, rate_limit_delay: float = 0.25, limit: int = 8):
        self.api_key = api_key
        self.base_url = "http://ws.audioscrobbler.com/2.0/"
        self.rate_limit_delay = rate_limit_delay
        self.limit = limit

    def get_similar_artists(self, artist_name: str) -> List[str]:
        params = {
            'method': 'artist.getSimilar',
            'artist': artist_name,
            'api_key': self.api_key,
            'limit': self.limit,
            'format': 'json'
        }
        
        try:
            response = requests.get(self.base_url, params=params)
            response.raise_for_status()
            
            if 'X-RateLimit-Remaining' in response.headers:
                remaining = int(response.headers['X-RateLimit-Remaining'])
                if remaining == 0:
                    sleep(self.rate_limit_delay)
            
            data = response.json()
            if 'similarartists' in data and 'artist' in data['similarartists']:
                return [artist['name'] for artist in data['similarartists']['artist'][:self.limit]]
            return []
            
        except Exception as e:
            print(f"Error fetching similar artists for {artist_name}: {e}")
            return []

def extract_primary_artist(artist_string: str) -> str:
    if pd.isna(artist_string):
        return ""
    return artist_string.split(",")[0].strip()

def update_similar_artists_all_sources(api_key: str, output_path: str = 'data/liked_artists_only_similar.csv') -> pd.DataFrame:
    print("Loading ALL artists from all data sources...")
    
    # Load existing similar artists data
    existing_data: Dict[str, List[str]] = {}
    if os.path.exists(output_path):
        existing_df = pd.read_csv(output_path)
        existing_data = dict(zip(existing_df['Artist'], existing_df['Similar Artists']))
        print(f"Loaded {len(existing_data)} existing artists from database")
    
    # Load artists from ALL data sources
    all_artists = set()
    
    # 1. Liked songs
    try:
        df_liked = pd.read_csv("data/liked.csv")
        liked_artists = df_liked['Artist Name(s)'].apply(extract_primary_artist).unique()
        all_artists.update(liked_artists)
        print(f"  - Liked songs: {len(liked_artists)} artists")
    except Exception as e:
        print(f"  - Error loading liked.csv: {e}")
    
    # 2. New training data (Top 100 + Honorable + Mid + Not Liked)
    try:
        df_training = pd.read_csv("data/2026_training_complete_with_features.csv")
        training_artists = df_training['Artist Name(s)'].apply(extract_primary_artist).unique()
        all_artists.update(training_artists)
        print(f"  - Training data: {len(training_artists)} artists")
    except Exception as e:
        print(f"  - Error loading training data: {e}")
        # Fallback to individual files
        try:
            df_top_100 = pd.read_csv("data/top_100_all_years_clean.csv")
            top_artists = df_top_100['Artist'].apply(extract_primary_artist).unique()
            all_artists.update(top_artists)
            print(f"  - Top 100 data: {len(top_artists)} artists")
        except:
            pass
    
    # 3. NMF data (current New Music Friday)
    try:
        df_nmf = pd.read_csv("data/nmf.csv")
        nmf_artists = df_nmf['Artist Name(s)'].apply(extract_primary_artist).unique()
        all_artists.update(nmf_artists)
        print(f"  - NMF data: {len(nmf_artists)} artists")
    except Exception as e:
        print(f"  - Error loading nmf.csv: {e}")
    
    # Remove empty strings
    all_artists.discard("")
    
    print(f"\nTotal unique artists from all sources: {len(all_artists)}")
    
    # Find new artists not in existing data
    new_artists = all_artists - set(existing_data.keys())
    print(f"Found {len(new_artists)} new artists to process")
    
    if not new_artists:
        print("No new artists to process. Database is up to date!")
        return pd.DataFrame({
            'Artist': list(existing_data.keys()),
            'Similar Artists': list(existing_data.values())
        })
    
    # Process new artists
    api = LastFMAPI(api_key)
    results = {}
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_artist = {
            executor.submit(api.get_similar_artists, artist): artist 
            for artist in new_artists
        }
        
        for future in tqdm(as_completed(future_to_artist), 
                         total=len(future_to_artist),
                         desc="Fetching similar artists"):
            artist = future_to_artist[future]
            similar_artists = future.result()
            results[artist] = ', '.join(similar_artists)
    
    # Combine existing and new data
    combined_data = {**existing_data, **results}
    
    # Create and save DataFrame
    output_df = pd.DataFrame({
        'Artist': list(combined_data.keys()),
        'Similar Artists': list(combined_data.values())
    })
    
    output_df.to_csv(output_path, index=False)
    print(f"\nâœ… Successfully updated database with {len(new_artists)} new artists")
    print(f"Total artists in database: {len(combined_data)}")
    
    return output_df

# Main execution - UPDATED FOR 2026
if __name__ == "__main__":
    API_KEY = "74a510ecc9fc62bf3e0edc6adc2e99f9"
    OUTPUT_PATH = "data/liked_artists_only_similar.csv"
    
    # Run with ALL data sources
    df_liked_similar = update_similar_artists_all_sources(API_KEY, OUTPUT_PATH)
    
# Now df_liked_similar is ready to use with ALL your artists
df_liked_similar.head()





# === 2026 RETOOLED DATA LOADING ===

print("ðŸŽµ LOADING 2026 RETOOLED DATASETS")
print("=" * 60)

# 1. Liked songs (PageRank network ONLY - NOT for training)
df_liked = pd.read_csv("data/liked.csv")
df_liked['liked'] = 100  # For PageRank network
df_liked['playlist_origin'] = 'df_liked_network_only'
print(f"âœ“ Liked songs (network): {len(df_liked)} tracks")

# 2. Complete training data (Top 100 + Honorable + Mid + Not Liked)
df_training = pd.read_csv("data/2026_training_complete_with_features.csv")
print(f"âœ“ Training data (retooled): {len(df_training)} tracks")
print(f"  Score range: {df_training['liked'].min():.1f} - {df_training['liked'].max():.1f}")
print(f"  Unique scores: {df_training['liked'].nunique()}")
print(f"  Sources: {df_training['source_type'].unique().tolist()}")

# 3. NMF data (what we're predicting)
df_nmf = pd.read_csv("data/nmf.csv")
df_nmf['liked'] = np.nan  # These are our predictions
df_nmf['playlist_origin'] = 'df_nmf_test'
print(f"âœ“ NMF test data: {len(df_nmf)} tracks")

# 4. Similar artists (unified network - ALL artists)
df_liked_similar = pd.read_csv('data/liked_artists_only_similar.csv')
print(f"âœ“ Similar artists: {len(df_liked_similar)} artists in unified network")

print("\n" + "=" * 60)
print("ðŸŽ¯ 2026 RETOOLING SUMMARY")
print("=" * 60)
print("TRAINING DATA NOW INCLUDES:")
print(f"  â€¢ Top 100 rankings: 550 albums (100.0-60.4 sliding scale)")
print(f"  â€¢ Honorable mentions: 323 albums (60.0 flat)")
print(f"  â€¢ Mid albums: 152 albums (45.0 flat)")
print(f"  â€¢ Not liked: 321 albums (30.0 flat)")
print(f"\nTOTAL: {len(df_training)} tracks with {df_training['liked'].nunique()} unique scores")
print("\nKEY CHANGE: Liked songs (100) used for PageRank network ONLY")
print("            NOT included in training data (prevents bias)")

print("\n" + "=" * 60)
print("ðŸŽ§ NMF DATA PREVIEW (What We're Predicting)")
print("=" * 60)
print(f"Total tracks: {len(df_nmf)}")
print(f"Unique albums: {df_nmf['Album Name'].nunique()}")
print(f"Unique artists: {df_nmf['Artist Name(s)'].nunique()}")

# Get most recent NMF date
if 'Release Date' in df_nmf.columns:
    df_nmf['Release Date'] = pd.to_datetime(df_nmf['Release Date'], errors='coerce')
    nmf_date = df_nmf['Release Date'].max()
    print(f"Most recent release: {nmf_date.date() if not pd.isna(nmf_date) else 'Unknown'}")

print("\nðŸ“‹ Sample NMF albums (5 random):")
sample_albums = df_nmf[['Artist Name(s)', 'Album Name']].drop_duplicates().sample(5, random_state=42)
for idx, row in sample_albums.iterrows():
    print(f"  â€¢ {row['Artist Name(s)']} - {row['Album Name']}")

print("\nðŸ“Š DATA CHECK:")
print(f"df_training columns: {df_training.columns.tolist()[:10]}...")
print(f"df_nmf columns: {df_nmf.columns.tolist()[:10]}...")
print(f"df_liked_similar columns: {df_liked_similar.columns.tolist()}")


# === CHECK 2026 RETOOLED DATA ===

print("ðŸ” CHECKING 2026 RETOOLED DATA DISTRIBUTION")
print("=" * 60)

# Check training data scores
print("\nðŸŽ¯ TRAINING DATA SCORE DISTRIBUTION:")
score_counts = df_training['liked'].value_counts().sort_index(ascending=False)

# Show top 10 and bottom 10 scores
print("\nTop 10 scores (highest):")
for score, count in list(score_counts.items())[:10]:
    print(f"  {score:.1f}: {count} tracks")

print("\nBottom 10 scores (lowest):")
for score, count in list(score_counts.items())[-10:]:
    print(f"  {score:.1f}: {count} tracks")

# Show distribution by source type
print("\nðŸ“Š DISTRIBUTION BY SOURCE TYPE:")
source_counts = df_training['source_type'].value_counts()
for source, count in source_counts.items():
    avg_score = df_training[df_training['source_type'] == source]['liked'].mean()
    print(f"  {source}: {count} tracks (avg score: {avg_score:.1f})")

# Quick check of NMF data
print(f"\nðŸŽ§ NMF DATA CHECK:")
print(f"  Tracks: {len(df_nmf)}")
print(f"  Unique albums: {df_nmf['Album Name'].nunique()}")
print(f"  Target 'liked' column: {df_nmf['liked'].isna().sum()} NaN (correct - we predict these)")

# Show a quick visualization of score distribution
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
# Histogram of all scores
plt.hist(df_training['liked'], bins=30, alpha=0.7, color='blue', edgecolor='black')
plt.xlabel('Score')
plt.ylabel('Count')
plt.title('Training Data Score Distribution')

plt.subplot(1, 2, 2)
# Boxplot by source type
source_order = ['top_100_ranked', 'honorable_mention', 'mid', 'not_liked']
plot_data = [df_training[df_training['source_type'] == source]['liked'] for source in source_order]
plt.boxplot(plot_data, labels=source_order)
plt.xticks(rotation=45)
plt.ylabel('Score')
plt.title('Score Distribution by Source Type')

plt.tight_layout()
plt.show()

print("\nâœ… 2026 Retooled data verified!")
print(f"   Training: {len(df_training)} tracks, {df_training['liked'].nunique()} unique scores")
print(f"   Network: {len(df_liked)} liked tracks for PageRank")
print(f"   Test: {len(df_nmf)} NMF tracks to predict")


# === FEATURE ENGINEERING SETUP ===

print("\n" + "=" * 60)
print("âš™ï¸  PREPARING FOR FEATURE ENGINEERING")
print("=" * 60)

# Check what features we have
print("Checking available features in training data...")
print(f"Training data columns: {len(df_training.columns)} total")

# Identify feature columns vs metadata
feature_cols = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 
                'Speechiness', 'Acousticness', 'Instrumentalness', 
                'Liveness', 'Valence', 'Tempo', 'Popularity']

print(f"\nSpotify audio features available: {[col for col in feature_cols if col in df_training.columns]}")

# Check for missing values in key columns
print("\nðŸ” Checking for missing values in training data:")
for col in ['Artist Name(s)', 'Album Name', 'Genres', 'Record Label', 'liked']:
    if col in df_training.columns:
        missing = df_training[col].isna().sum()
        print(f"  {col}: {missing} missing ({missing/len(df_training)*100:.1f}%)")

print("\nâœ… Ready for feature engineering!")





# === 2026 DATA CLEANUP ===

print("ðŸ§¹ CLEANING UNNECESSARY COLUMNS")
print("=" * 60)

# Columns to drop from ALL dataframes
columns_to_drop = ['Added By', 'Added At', 'Time Signature']

# Apply to each dataframe
print("Dropping unnecessary columns...")
for df_name, dataframe in [('df_training', df_training), ('df_liked', df_liked), ('df_nmf', df_nmf)]:
    before = len(dataframe.columns)
    
    # Only drop columns that exist
    cols_exist = [col for col in columns_to_drop if col in dataframe.columns]
    if cols_exist:
        dataframe.drop(columns=cols_exist, inplace=True, errors='ignore')
        after = len(dataframe.columns)
        print(f"  {df_name}: Dropped {before - after} columns ({cols_exist})")
    else:
        print(f"  {df_name}: No columns to drop")

print("\nðŸ“Š CLEANED DATA SHAPES:")
print(f"  df_training: {df_training.shape[0]} rows Ã— {df_training.shape[1]} columns")
print(f"  df_liked: {df_liked.shape[0]} rows Ã— {df_liked.shape[1]} columns")
print(f"  df_nmf: {df_nmf.shape[0]} rows Ã— {df_nmf.shape[1]} columns")

# Quick check for other unnecessary columns
print("\nðŸ” CHECKING FOR OTHER UNNECESSARY COLUMNS:")
all_columns = set(df_training.columns) | set(df_liked.columns) | set(df_nmf.columns)

# Identify potentially useless columns
useless_patterns = ['Unnamed:', 'Unnamed_', 'Unnamed ', 'index', 'id_', '_id']
useless_cols = [col for col in all_columns if any(pattern in str(col) for pattern in useless_patterns)]

if useless_cols:
    print(f"Found {len(useless_cols)} potentially useless columns:")
    for col in useless_cols[:10]:  # Show first 10
        print(f"  â€¢ {col}")
    print("Consider removing these if they're not needed for features.")
else:
    print("No obvious useless columns found.")

print("\nâœ… Data cleanup complete!")
print("   Removed timestamp/added columns that don't help predictions.")





# ============================================
# ðŸ“Š DATA PREP & CACHE ANALYSIS
# ============================================

print("ðŸ“Š DATA PREP & CACHE ANALYSIS")
print("=" * 60)

# -----------------------------------------------------------------
# 1. ESSENTIAL FUNCTIONS
# -----------------------------------------------------------------
import pandas as pd
import numpy as np
import os
import re
from datetime import datetime

def extract_primary_artist_fixed(artist_string):
    """Extract primary artist from string with multiple artists - CAREFUL WITH '&' and 'and'"""
    if pd.isna(artist_string): 
        return None
    
    artist_str = str(artist_string).strip()
    
    # Special case: preserve "&" and "and" in band names like "Robinson & Rohe"
    # First check if it's a single band name with "&" or "and" (not a collaboration list)
    if ';' not in artist_str and ' feat. ' not in artist_str and ' featuring ' not in artist_str:
        # Likely a single band name, keep intact
        return artist_str
    
    # It's a collaboration list, extract first artist
    separators = [';', ',', ' feat. ', ' featuring ']
    for sep in separators:
        if sep in artist_str.lower():
            if sep == ' and ':
                parts = re.split(r'\band\b', artist_str, flags=re.IGNORECASE)
            else:
                parts = artist_str.split(sep)
            return parts[0].strip()
    
    return artist_str

def normalize_string(text):
    """Normalize string for consistent comparison"""
    if pd.isna(text): 
        return None
    return str(text).lower().strip()

def clean_genres(genre_string):
    """Clean genre strings by removing duplicates, numbers, etc."""
    if pd.isna(genre_string) or genre_string == '': 
        return ''
    
    all_parts = [part.strip() for part in str(genre_string).split('|')]
    all_genres = []
    for part in all_parts:
        genres = [g.strip() for g in part.split(',')]
        all_genres.extend(genres)
    
    seen = set()
    unique_genres = []
    for genre in all_genres:
        if not genre or genre == '': 
            continue
        if any(char.isdigit() for char in genre): 
            continue
        if len(genre.split()) > 3: 
            continue
        genre_lower = genre.lower()
        if genre_lower not in seen:
            unique_genres.append(genre)
            seen.add(genre_lower)
    
    return ', '.join(unique_genres)

print("âœ… Essential functions defined")

# -----------------------------------------------------------------
# 2. LOAD ALL DATASETS
# -----------------------------------------------------------------
print("\nðŸ“ LOADING DATASETS...")

# Load main datasets
df_liked = pd.read_csv("data/liked.csv")
df_liked['liked'] = 100
df_liked['playlist_origin'] = 'df_liked_network_only'

df_training = pd.read_csv("data/2026_training_complete_with_features.csv")

df_nmf = pd.read_csv("data/nmf.csv")
df_nmf['liked'] = np.nan
df_nmf['playlist_origin'] = 'df_nmf_test'

df_liked_similar = pd.read_csv('data/liked_artists_only_similar.csv')

print(f"âœ“ Liked songs: {len(df_liked)} tracks")
print(f"âœ“ Training data: {len(df_training)} tracks")
print(f"âœ“ NMF test data: {len(df_nmf)} tracks")
print(f"âœ“ Similar artists: {len(df_liked_similar)} artists")

# Clean unnecessary columns
columns_to_drop = ['Added By', 'Added At', 'Time Signature']
for df_name, dataframe in [('Training', df_training), ('Liked', df_liked), ('NMF', df_nmf)]:
    cols_exist = [col for col in columns_to_drop if col in dataframe.columns]
    if cols_exist:
        dataframe.drop(columns=cols_exist, inplace=True, errors='ignore')

# -----------------------------------------------------------------
# 3. LOAD MANUAL OVERRIDES
# -----------------------------------------------------------------
print("\nðŸ” LOADING MANUAL OVERRIDES...")

manual_overrides_path = "data/obscure_artists_mike_likes.csv"
manual_entries = []

if os.path.exists(manual_overrides_path):
    try:
        manual_df = pd.read_csv(manual_overrides_path)
        print(f"âœ“ Found manual overrides: {len(manual_df):,} entries")
        
        for _, row in manual_df.iterrows():
            if 'Artist' in manual_df.columns and 'Genres' in manual_df.columns:
                artist = row['Artist']
                genres = row['Genres']
                
                if pd.notna(artist) and pd.notna(genres):
                    artist_norm = normalize_string(extract_primary_artist_fixed(artist))
                    genres_clean = clean_genres(genres)
                    
                    if artist_norm and genres_clean:
                        manual_entries.append({
                            'artist_norm': artist_norm,
                            'genres': genres_clean,
                            'original_artist': artist,
                            'source': 'manual_override'
                        })
    except Exception as e:
        print(f"âš ï¸ Could not load manual overrides: {e}")
        manual_entries = []
else:
    print(f"ðŸ“ No manual overrides file found")

# -----------------------------------------------------------------
# 4. BUILD ARTIST-ALBUM MAP & CHECK CACHE
# -----------------------------------------------------------------
print("\nðŸ—ºï¸ BUILDING ARTIST-ALBUM MAP & CHECKING CACHE...")

artist_albums_map = {}
artist_lookup = {}
album_lookup = {}

for df_name, dataframe in [("Training", df_training), ("NMF", df_nmf), ("Liked", df_liked)]:
    print(f"  Processing {df_name}...")
    
    dataframe['artist_norm'] = dataframe['Artist Name(s)'].apply(extract_primary_artist_fixed).apply(normalize_string)
    dataframe['album_norm'] = dataframe['Album Name'].apply(normalize_string)
    
    for _, row in dataframe.iterrows():
        artist = row['artist_norm']
        album = row['album_norm']
        
        if artist and album:
            if artist not in artist_albums_map:
                artist_albums_map[artist] = set()
            artist_albums_map[artist].add(album)
            
            if artist not in artist_lookup:
                artist_lookup[artist] = row['Artist Name(s)']
            if (artist, album) not in album_lookup:
                album_lookup[(artist, album)] = row['Album Name']
    
    dataframe.drop(columns=['artist_norm', 'album_norm'], inplace=True, errors='ignore')

print(f"âœ“ Map created: {len(artist_albums_map):,} artists")
total_pairs = sum(len(albums) for albums in artist_albums_map.values())
print(f"âœ“ Total artist-album pairs: {total_pairs:,}")

# -----------------------------------------------------------------
# 5. CHECK CACHE COVERAGE
# -----------------------------------------------------------------
print("\nðŸ” CHECKING CACHE COVERAGE...")

cache_file = "data/ten_genres_fixed_2026_complete.csv"
cache_lookup = {}
manual_lookup = {}

# Load main cache
if os.path.exists(cache_file):
    cache_df = pd.read_csv(cache_file)
    print(f"âœ“ Loaded cache: {len(cache_df):,} entries")
    
    for _, row in cache_df.iterrows():
        artist_norm = normalize_string(extract_primary_artist_fixed(row['Artist']))
        album_norm = normalize_string(row['Album']) if pd.notna(row['Album']) else ''
        
        if artist_norm:
            if artist_norm not in cache_lookup:
                cache_lookup[artist_norm] = set()
            cache_lookup[artist_norm].add(album_norm)
else:
    print("âš ï¸ No cache file found, starting fresh")

# Add manual entries to lookup
for entry in manual_entries:
    artist_norm = entry['artist_norm']
    if artist_norm not in manual_lookup:
        manual_lookup[artist_norm] = set()
    manual_lookup[artist_norm].add('any_album')

# Combine lookups
combined_lookup = {**cache_lookup}
for artist, albums in manual_lookup.items():
    if artist not in combined_lookup:
        combined_lookup[artist] = set()
    combined_lookup[artist].update(albums)

print(f"âœ“ Combined: {len(cache_lookup):,} cached + {len(manual_lookup):,} manual artists")

# -----------------------------------------------------------------
# 6. IDENTIFY WHAT NEEDS FETCHING
# -----------------------------------------------------------------
print("\nðŸŽ¯ IDENTIFYING WHAT NEEDS FETCHING...")

pairs_to_fetch = []
missing_artists = set()

for artist_norm, albums in artist_albums_map.items():
    artist_covered = False
    
    if artist_norm in combined_lookup:
        for album_norm in albums:
            if (album_norm in combined_lookup[artist_norm] or 
                'any_album' in combined_lookup[artist_norm]):
                artist_covered = True
                break
    
    if not artist_covered:
        missing_artists.add(artist_norm)
        for album_norm in albums:
            original_artist = artist_lookup.get(artist_norm, artist_norm)
            original_album = album_lookup.get((artist_norm, album_norm), album_norm)
            pairs_to_fetch.append((original_artist, original_album))

# -----------------------------------------------------------------
# 7. FINAL SUMMARY
# -----------------------------------------------------------------
print("\n" + "=" * 60)
print("ðŸ“Š PREP COMPLETE - SUMMARY")
print("=" * 60)
print(f"\nðŸ“ˆ DATASET SIZES:")
print(f"  â€¢ Training: {len(df_training):,} tracks")
print(f"  â€¢ NMF test: {len(df_nmf):,} tracks")
print(f"  â€¢ Liked: {len(df_liked):,} tracks")
print(f"  â€¢ Similar artists: {len(df_liked_similar):,} artists")

print(f"\nðŸŽ¯ CACHE COVERAGE:")
print(f"  â€¢ Total artist-album pairs: {total_pairs:,}")
print(f"  â€¢ Already cached: {total_pairs - len(pairs_to_fetch):,} ({(total_pairs - len(pairs_to_fetch))/total_pairs*100:.1f}%)")
print(f"  â€¢ Need to fetch: {len(pairs_to_fetch):,} pairs from {len(missing_artists):,} artists")

if pairs_to_fetch:
    print(f"\nâ° Estimated fetch time: ~{len(pairs_to_fetch)/8/60:.1f} minutes")
    print(f"ðŸ’¾ Pairs saved to: 'data/pairs_to_fetch.csv'")
    
    # Save for reference
    pairs_df = pd.DataFrame(pairs_to_fetch, columns=['Artist', 'Album'])
    pairs_df.to_csv('data/pairs_to_fetch.csv', index=False)
else:
    print("\nðŸŽ‰ ALL DATA ALREADY IN CACHE/MANUAL!")

print(f"\nâœ… READY FOR GENRE FETCHING" if pairs_to_fetch else "âœ… READY TO APPLY GENRES")





# ============================================
# ðŸ“¡ SMART GENRE FETCHER WITH ALL LEARNED PATTERNS
# ============================================

print("ðŸ“¡ SMART GENRE FETCHER WITH ALL LEARNED PATTERNS")
print("=" * 60)

import requests
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import random

# -----------------------------------------------------------------
# 1. ULTIMATE SMART ARTIST CLEANER (WITH ALL LEARNED PATTERNS)
# -----------------------------------------------------------------
def ultimate_artist_cleaner(artist_string):
    """
    Ultimate artist cleaner with ALL patterns we discovered:
    1. Killed By Deaf pattern
    2. Ampersand/band name preservation
    3. Collaboration strings
    4. Serbian "I" pattern
    5. Location markers
    6. Punctuation/formatting fixes
    """
    if pd.isna(artist_string):
        return []
    
    strategies = []
    original = str(artist_string).strip()
    
    # Basic clean (remove trailing ... but preserve for now)
    clean = original.replace('...', '').strip() if original.endswith('...') else original
    
    # Strategy 0: Original cleaned
    strategies.append((clean, "cleaned_original"))
    
    # Pattern 1: Killed By Deaf (common in your data)
    if ';Killed By Deaf' in clean:
        band_only = clean.split(';Killed By Deaf')[0].strip()
        strategies.append((band_only, "remove_killed_by_deaf"))
        strategies.append(("Killed By Deaf", "kbd_as_artist"))
    
    # Pattern 2: Ampersand handling - PRESERVE BAND NAMES
    # Only split if it's a list (has semicolon), otherwise keep intact
    if '&' in clean and ';' not in clean:
        # Single band name with &, keep intact but also try variations
        strategies.append((clean, "full_band_name"))
        
        # For "X & The Y" patterns
        if '& The ' in clean:
            before_the = clean.split('& The ')[0].strip()
            strategies.append((before_the, "band_leader_only"))
    
    # Pattern 3: Collaboration strings (semicolon means multiple artists)
    if ';' in clean:
        # Try first collaborator
        first_collab = clean.split(';')[0].strip()
        strategies.append((first_collab, "first_collaborator"))
        
        # Try all individual collaborators (max 3)
        all_collabs = [c.strip() for c in clean.split(';')]
        for i, collab in enumerate(all_collabs[:3]):
            # Preserve "&" within individual collaborator names
            strategies.append((collab, f"collaborator_{i+1}"))
    
    # Pattern 4: "and" in band names - BE CAREFUL!
    if ' and ' in clean.lower() and ';' not in clean:
        # Likely a band name, keep intact
        strategies.append((clean, "full_band_name_with_and"))
        
        # But also try common variations
        parts = re.split(r'\s+and\s+', clean, flags=re.IGNORECASE)
        if len(parts) > 1:
            strategies.append((parts[0].strip(), "first_part_of_band"))
    
    # Pattern 5: Location markers (e.g., " / South Korean")
    if '/' in clean:
        no_location = clean.split('/')[0].strip()
        strategies.append((no_location, "remove_location"))
    
    # Pattern 6: Remove parentheses (e.g., "(VA)")
    if '(' in clean and ')' in clean:
        no_parens = re.sub(r'\([^)]*\)', '', clean).strip()
        if no_parens:
            strategies.append((no_parens, "remove_parentheses"))
    
    # Pattern 7: Serbian/Croatian "I" pattern
    if ' I ' in clean:
        parts = clean.split(' I ')
        strategies.append((parts[0] + " Markovic", "serbian_pattern"))
    
    # Pattern 8: Common fixes from your debugging
    common_fixes = {
        'KAYTRAMINE': 'Kaytranada',
        'ER': 'Emergency Room',
        'SWSL': '',
        'Sliim': 'Slim',
        'Bambino': '',
        'VA': ''
    }
    
    for wrong, right in common_fixes.items():
        if wrong in clean.upper():
            fixed = re.sub(wrong, right, clean, flags=re.IGNORECASE)
            if fixed.strip():
                strategies.append((fixed.strip(), f"fix_{wrong}"))
    
    # Pattern 9: Remove punctuation but preserve &
    no_punct = re.sub(r'[^\w\s&]', ' ', clean).strip()
    no_punct = ' '.join(no_punct.split())  # Normalize spaces
    if no_punct != clean:
        strategies.append((no_punct, "no_punctuation"))
    
    # Remove duplicates and empty strings
    seen = set()
    unique = []
    for artist, strategy in strategies:
        if artist and artist not in seen and len(artist) > 1:
            seen.add(artist)
            unique.append((artist, strategy))
    
    return unique

# -----------------------------------------------------------------
# 2. ULTIMATE SMART GENRE FETCHER CLASS
# -----------------------------------------------------------------
class SmartGenreFetcher:
    """Ultimate fetcher with all learned patterns - use this going forward!"""
    
    def __init__(self, api_key: str, max_workers: int = 10):
        self.api_key = api_key
        self.max_workers = max_workers
        self.base_url = "http://ws.audioscrobbler.com/2.0/"
        self.ignored_tags = {
            'seen live', 'albums i own', 'favorite', 'favourites', 'favourite',
            'my playlist', 'spotify', 'pandora', 'wish i had seen live',
            'awesome', 'love at first listen', 'love', 'amazing', 'listened',
            'personal', 'my music', 'all', 'best', 'top', 'great', 'good',
            '2022', '2023', '2024', '2025', '2021', '2020', '2019', '2018', '2017'
        }
        self.successful_artists = set()
        self.failed_artists = set()
    
    def _try_single_fetch(self, artist: str, album: str = None):
        """Single fetch attempt with rate limiting"""
        time.sleep(random.uniform(0.05, 0.15))
        
        result = {'artist': artist, 'album': album, 'genres': [], 'success': False}
        
        # Try album first (if provided and valid)
        if album and album != 'nan' and str(album).lower() not in ['na', 'none', '']:
            try:
                params = {
                    'method': 'album.getTopTags',
                    'artist': artist, 
                    'album': album,
                    'api_key': self.api_key, 
                    'format': 'json'
                }
                response = requests.get(self.base_url, params=params, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    if 'toptags' in data and 'tag' in data['toptags']:
                        tags = []
                        for tag in data['toptags']['tag'][:12]:
                            tag_name = tag['name'].lower().strip()
                            if (tag_name and 
                                tag_name not in self.ignored_tags and
                                not any(char.isdigit() for char in tag_name)):
                                tags.append(tag_name)
                        if tags:
                            result['genres'] = tags[:10]
                            result['success'] = True
                            return result
            except:
                pass
        
        # Try artist
        try:
            params = {
                'method': 'artist.getTopTags',
                'artist': artist,
                'api_key': self.api_key,
                'format': 'json'
            }
            response = requests.get(self.base_url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'toptags' in data and 'tag' in data['toptags']:
                    tags = []
                    for tag in data['toptags']['tag'][:12]:
                        tag_name = tag['name'].lower().strip()
                        if (tag_name and 
                            tag_name not in self.ignored_tags and
                            not any(char.isdigit() for char in tag_name)):
                            tags.append(tag_name)
                    if tags:
                        result['genres'] = tags[:10]
                        result['success'] = True
        except:
            pass
        
        return result
    
    def smart_fetch_single(self, artist: str, album: str = None):
        """Smart fetch with multiple strategies"""
        strategies = ultimate_artist_cleaner(artist)
        
        for artist_to_try, strategy in strategies:
            result = self._try_single_fetch(artist_to_try, album)
            if result['success']:
                result['strategy'] = strategy
                result['queried_as'] = artist_to_try
                return result
        
        return {
            'artist': artist, 
            'album': album, 
            'genres': [], 
            'success': False, 
            'strategy': 'all_failed'
        }
    
    def fetch_all(self, pairs: list):
        """Bulk fetch with threading AND smart strategies"""
        print(f"ðŸš€ Starting SMART fetch of {len(pairs):,} pairs with {self.max_workers} workers...")
        print(f"â° Estimated time: ~{len(pairs)/(self.max_workers*2)/60:.1f} minutes")
        
        results = []
        start_time = time.time()
        completed = 0
        total = len(pairs)
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_idx = {}
            for idx, (artist, album) in enumerate(pairs):
                future = executor.submit(self.smart_fetch_single, artist, album)
                future_to_idx[future] = idx
            
            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                result = future.result()
                results.append((idx, result))
                
                completed += 1
                if completed % 50 == 0 or completed == total:
                    elapsed = time.time() - start_time
                    rate = completed / elapsed if elapsed > 0 else 0
                    remaining = total - completed
                    eta = remaining / rate if rate > 0 else 0
                    print(f"  Progress: {completed:,}/{total:,} ({completed/total*100:.1f}%) | "
                          f"Rate: {rate:.1f}/sec | ETA: {eta/60:.1f} min")
        
        results.sort(key=lambda x: x[0])
        return [r[1] for r in results]
    
    def auto_update_manual_overrides(self, results, artist_lookup, manual_path):
        """Save truly failed artists to manual overrides file"""
        # Track successes
        for result in results:
            if result['success']:
                artist_norm = normalize_string(extract_primary_artist_fixed(result['artist']))
                self.successful_artists.add(artist_norm)
        
        # Identify truly failed artists
        failed_to_add = []
        for result in results:
            if not result['success']:
                artist_norm = normalize_string(extract_primary_artist_fixed(result['artist']))
                if artist_norm not in self.successful_artists:
                    self.failed_artists.add(artist_norm)
                    original_artist = artist_lookup.get(artist_norm, result['artist'])
                    
                    # Generate educated guess based on name patterns
                    educated_guess = self._suggest_genre_from_name(original_artist)
                    failed_to_add.append((original_artist, educated_guess))
        
        if failed_to_add:
            print(f"\nðŸ’¾ SAVING {len(failed_to_add)} FAILED ARTISTS TO MANUAL OVERRIDES...")
            
            # Load existing or create new
            if os.path.exists(manual_path):
                existing_df = pd.read_csv(manual_path)
            else:
                existing_df = pd.DataFrame(columns=['Artist', 'Genres'])
            
            # Add new entries
            new_entries = []
            for artist, genres in failed_to_add:
                # Check if already exists
                artist_norm = normalize_string(extract_primary_artist_fixed(artist))
                exists = False
                for _, row in existing_df.iterrows():
                    if 'Artist' in existing_df.columns:
                        existing_norm = normalize_string(extract_primary_artist_fixed(row['Artist']))
                        if existing_norm == artist_norm:
                            exists = True
                            break
                
                if not exists:
                    new_entries.append({'Artist': artist, 'Genres': genres})
                    print(f"  â€¢ {artist[:40]}... â†’ {genres}")
            
            if new_entries:
                new_df = pd.DataFrame(new_entries)
                updated_df = pd.concat([existing_df, new_df], ignore_index=True)
                updated_df.to_csv(manual_path, index=False)
                print(f"âœ… Added {len(new_entries)} new artists to {manual_path}")
        
        return list(self.failed_artists)
    
    def _suggest_genre_from_name(self, artist_name):
        """Make educated genre guesses based on artist name patterns"""
        name_lower = artist_name.lower()
        
        # Pattern-based suggestions
        if any(word in name_lower for word in ['crows', 'westwater', 'hooper', 'lovett', 'stogel']):
            return "folk, singer-songwriter, acoustic"
        elif any(word in name_lower for word in ['stroik', 'zdan', 'swsl', 'kaytramine']):
            return "electronic, experimental"
        elif '&' in artist_name or ' and ' in name_lower:
            return "see similar artists, check bandcamp"
        elif any(word in name_lower for word in ['boban', 'markovic', 'orkesta']):
            return "balkan, brass, gypsy"
        elif any(word in name_lower for word in ['lily', 'jamaar']):
            return "R&B, soul"
        elif '7th' in name_lower or '7amurai' in name_lower:
            return "hip-hop, rap, electronic"
        
        return "indie, alternative, rock"  # Default fallback

# -----------------------------------------------------------------
# 3. EXECUTE THE SMART FETCH (IF NEEDED)
# -----------------------------------------------------------------
# Check if we have pairs to fetch (from Cell 1)
if 'pairs_to_fetch' in locals() and len(pairs_to_fetch) > 0:
    print(f"\nðŸŽ¯ FETCHING {len(pairs_to_fetch):,} MISSING PAIRS...")
    print("=" * 60)
    
    API_KEY = '74a510ecc9fc62bf3e0edc6adc2e99f9'  # Consider moving to env var
    fetcher = SmartGenreFetcher(API_KEY, max_workers=10)
    results = fetcher.fetch_all(pairs_to_fetch)
    
    # Analyze results
    successful = sum(1 for r in results if r['success'])
    failed = len(results) - successful
    
    print(f"\n" + "=" * 60)
    print(f"ðŸ“Š FETCH RESULTS:")
    print(f"  Total pairs: {len(results):,}")
    print(f"  Successful: {successful:,}")
    print(f"  Failed: {failed:,}")
    print(f"  Success rate: {successful/len(results)*100:.1f}%")
    
    # Show some successful strategies
    print(f"\nðŸ” SUCCESSFUL STRATEGIES (sample):")
    successful_results = [r for r in results if r['success']]
    for result in successful_results[:5]:
        if 'strategy' in result:
            print(f"  â€¢ {result['artist'][:30]}... â†’ {result['strategy']}")
    
    # Auto-update manual overrides with failures
    if failed > 0:
        truly_failed = fetcher.auto_update_manual_overrides(
            results, 
            artist_lookup, 
            manual_overrides_path
        )
        print(f"\nðŸ“ {len(truly_failed)} artists added to manual overrides for review")
    
    # Update cache with successful fetches
    if successful > 0:
        print(f"\nðŸ’¾ UPDATING MAIN CACHE...")
        
        new_entries = []
        for result in results:
            if result['success']:
                entry = {
                    'Artist': result['artist'],
                    'Album': result['album'],
                    'Genres': ', '.join(result['genres']),
                    'Source': f'smart_fetch_{result.get("strategy", "direct")}',
                    'Fetch_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'Fixed': 'Yes_2026_smart'
                }
                new_entries.append(entry)
        
        # Load and update cache
        cache_file = "data/ten_genres_fixed_2026_complete.csv"
        new_cache_df = pd.DataFrame(new_entries)
        new_cache_df['Genres'] = new_cache_df['Genres'].apply(clean_genres)
        
        if os.path.exists(cache_file):
            existing_cache = pd.read_csv(cache_file)
            updated_cache = pd.concat([existing_cache, new_cache_df], ignore_index=True)
        else:
            updated_cache = new_cache_df
        
        # Deduplicate
        updated_cache = updated_cache.drop_duplicates(subset=['Artist', 'Album'], keep='last')
        updated_cache.to_csv(cache_file, index=False)
        
        print(f"âœ… Cache updated: {len(updated_cache):,} total entries")
    
    print("\n" + "=" * 60)
    print("âœ… SMART FETCH COMPLETE!")
    print("=" * 60)
    
else:
    print("\nðŸŽ‰ NO FETCH NEEDED - ALL DATA ALREADY IN CACHE/MANUAL!")

# -----------------------------------------------------------------
# 4. APPLY GENRES TO ALL DATAFRAMES (ALWAYS RUN)
# -----------------------------------------------------------------
print("\nðŸ”„ APPLYING GENRES TO ALL DATAFRAMES...")
print("=" * 60)

# Load the latest cache
cache_file = "data/ten_genres_fixed_2026_complete.csv"
if os.path.exists(cache_file):
    cache_df = pd.read_csv(cache_file)
    print(f"âœ“ Loaded cache: {len(cache_df):,} entries")
else:
    print("âš ï¸ No cache file found, creating empty")
    cache_df = pd.DataFrame(columns=['Artist', 'Album', 'Genres'])

# Load latest manual overrides
if os.path.exists(manual_overrides_path):
    manual_df = pd.read_csv(manual_overrides_path)
    print(f"âœ“ Loaded manual overrides: {len(manual_df):,} entries")
else:
    manual_df = pd.DataFrame(columns=['Artist', 'Genres'])

# Prepare combined genre lookup
cache_df['artist_norm'] = cache_df['Artist'].apply(extract_primary_artist_fixed).apply(normalize_string)
manual_df['artist_norm'] = manual_df['Artist'].apply(extract_primary_artist_fixed).apply(normalize_string)

# Combine cache and manual, preferring manual for conflicts
combined_genres = pd.concat([
    cache_df[['artist_norm', 'Genres']],
    manual_df[['artist_norm', 'Genres']]
], ignore_index=True)

# Deduplicate, keeping manual entries first (they're added later in concat)
combined_genres = combined_genres.drop_duplicates(subset=['artist_norm'], keep='last')

# Apply to all dataframes
for df_name, dataframe in [("Training", df_training), ("NMF", df_nmf), ("Liked", df_liked)]:
    print(f"\nðŸ“ Updating {df_name}...")
    
    # Add normalized artist column
    dataframe['artist_norm'] = dataframe['Artist Name(s)'].apply(extract_primary_artist_fixed).apply(normalize_string)
    
    # Merge with combined genres
    merged = dataframe.merge(
        combined_genres,
        on='artist_norm',
        how='left',
        suffixes=('', '_new')
    )
    
    # Update genres column
    if 'Genres' in dataframe.columns and 'Genres_new' in merged.columns:
        # Keep existing if we have it, otherwise use new
        dataframe['Genres'] = merged['Genres_new'].combine_first(merged['Genres'])
    elif 'Genres_new' in merged.columns:
        dataframe['Genres'] = merged['Genres_new']
    
    # Clean the genres
    dataframe['Genres'] = dataframe['Genres'].apply(clean_genres)
    
    # Clean up temp column
    dataframe.drop(columns=['artist_norm'], inplace=True, errors='ignore')
    
    # Report coverage
    total = len(dataframe)
    with_genres = dataframe['Genres'].notna().sum()
    print(f"  {with_genres:,}/{total:,} tracks with genres ({with_genres/total*100:.1f}%)")

# -----------------------------------------------------------------
# 5. FINAL SUMMARY
# -----------------------------------------------------------------
print("\n" + "=" * 60)
print("ðŸŽ‰ GENRE SYSTEM COMPLETE!")
print("=" * 60)

print(f"\nðŸ“Š FINAL STATS:")
print(f"  â€¢ Training data: {len(df_training):,} tracks")
print(f"  â€¢ NMF test data: {len(df_nmf):,} tracks")
print(f"  â€¢ Liked songs: {len(df_liked):,} tracks")

# Calculate overall coverage
all_tracks = len(df_training) + len(df_nmf) + len(df_liked)
all_with_genres = (
    df_training['Genres'].notna().sum() + 
    df_nmf['Genres'].notna().sum() + 
    df_liked['Genres'].notna().sum()
)

print(f"  â€¢ Total tracks: {all_tracks:,}")
print(f"  â€¢ With genres: {all_with_genres:,} ({all_with_genres/all_tracks*100:.1f}% coverage)")
print(f"  â€¢ Manual overrides: {len(manual_df):,} entries")
print(f"  â€¢ Cache entries: {len(cache_df):,} entries")

print(f"\nâœ… READY FOR FEATURE ENGINEERING & NMF!")





# ============================================
# ðŸŽ¯ FINDING TOP 30 GENRES (SIMPLE VERSION)
# ============================================

print("ðŸŽ¯ FINDING TOP 30 GENRES")
print("=" * 60)

# Use the training dataframe which has liked scores
df = df_training.copy()

print(f"Analyzing {len(df):,} tracks from training data")

# Step 1: Explode the Genres column
df_exploded = df.assign(Genres=df['Genres'].str.split(', ')).explode('Genres')

# Step 2: Filter out genres that appear fewer than 40 times
genre_counts = df_exploded['Genres'].value_counts()
frequent_genres = genre_counts[genre_counts >= 40].index

# Filter the exploded DataFrame to only include frequent genres
df_frequent_genres = df_exploded[df_exploded['Genres'].isin(frequent_genres)]

# Step 3: Group by Genre and calculate the mean of the 'liked' column
genre_liked_avg = df_frequent_genres.groupby('Genres')['liked'].mean().reset_index()

# Step 4: Sort by the average 'liked' score in descending order and get the top 30
top_30_genres = genre_liked_avg.sort_values(by='liked', ascending=False).head(30)

# Display the top 30 genres
print("\nTop 30 Genres by Average Liked Score (Filtered for Frequent Genres):")
print("=" * 80)
print(f"{'Rank':<4} {'Genre':<30} {'Avg Liked':<12} {'Count':<8}")
print("-" * 80)

for i, (_, row) in enumerate(top_30_genres.iterrows(), 1):
    print(f"{i:<4} {row['Genres'][:28]:<30} {row['liked']:<12.2f} {genre_counts[row['Genres']]:<8}")

print("=" * 80)








# === RECORD LABEL FREQUENCY ENCODING (FIXED - PROPER SORTING) ===

print("ðŸ·ï¸ RECORD LABEL FREQUENCY ENCODING - PROPER SORTING")
print("=" * 60)

# Count labels in LIKED songs
liked_label_counts = df_liked['Record Label'].value_counts()
print(f"Labels in Liked songs: {len(liked_label_counts):,} unique labels")

# Count labels in TOP 100
top100_label_counts = df_training[df_training['source_type'] == 'top_100_ranked']['Record Label'].value_counts()
print(f"Labels in Top 100: {len(top100_label_counts):,} unique labels")

# Combine counts (SUM where overlap)
all_label_counts = pd.concat([liked_label_counts, top100_label_counts])
freq_encoding = all_label_counts.groupby(all_label_counts.index).sum()

# Sort by frequency (highest first) - THIS WAS MISSING!
freq_encoding = freq_encoding.sort_values(ascending=False)

print(f"\nðŸ“Š COMBINED STATS:")
print(f"Total unique labels: {len(freq_encoding):,}")
print(f"Labels in BOTH Liked & Top 100: {len(set(liked_label_counts.index) & set(top100_label_counts.index)):,}")
print(f"Labels only in Liked: {len(set(liked_label_counts.index) - set(top100_label_counts.index)):,}")
print(f"Labels only in Top 100: {len(set(top100_label_counts.index) - set(liked_label_counts.index)):,}")

# Show ACTUAL top labels by frequency
print(f"\nðŸ† ACTUAL TOP 10 LABELS (by total frequency):")
for i, (label, total_count) in enumerate(freq_encoding.head(10).items(), 1):
    liked_count = liked_label_counts.get(label, 0)
    top100_count = top100_label_counts.get(label, 0)
    print(f"{i:2d}. {label[:40]:<40} | Total: {total_count:4d} (Liked: {liked_count:4d}, Top100: {top100_count:4d})")

# Show some REAL major labels you actually listen to
print(f"\nðŸ” REAL MAJOR LABELS IN YOUR DATA:")
major_labels_to_check = ['Columbia', 'Dead Oceans', 'Anti/Epitaph', 'Interscope', 'Carpark', 'ATO']
for label in major_labels_to_check:
    if label in freq_encoding.index:
        total = freq_encoding[label]
        liked = liked_label_counts.get(label, 0)
        top100 = top100_label_counts.get(label, 0)
        print(f"  {label:<20} â†’ Total: {total:4d} (Liked: {liked:4d}, Top100: {top100:4d})")

# Median for fallback
median_freq = freq_encoding.median()
print(f"\nðŸ“ˆ Fallback (median): {median_freq:.1f}")

# Apply encoding (scaled to 1-100 range later)
def apply_label_encoding(df_to_encode, freq_map, fallback):
    return df_to_encode['Record Label'].map(freq_map).fillna(fallback)

df_training['Record Label Frequency Encoded'] = apply_label_encoding(df_training, freq_encoding, median_freq)
df_liked['Record Label Frequency Encoded'] = apply_label_encoding(df_liked, freq_encoding, median_freq)
df_nmf['Record Label Frequency Encoded'] = apply_label_encoding(df_nmf, freq_encoding, median_freq)

print(f"\nâœ… Applied raw frequencies to all dataframes")
print(f"   Will be scaled to 1-100 range with other features")

# Quick validation
print(f"\nðŸŽ¯ SAMPLE CHECK - Your actual favorite labels:")
sample_artists = df_training[df_training['source_type'] == 'top_100_ranked'].head(3)
for _, row in sample_artists.iterrows():
    label = row['Record Label']
    freq = freq_encoding.get(label, median_freq)
    print(f"  {row['Artist Name(s)'][:25]:<25} | Label: {label[:30]:<30} | Freq: {freq:4.0f}")





# === GENRE TARGET ENCODING (FIXED - NO INFINITE LOOP) ===

print("ðŸŽšï¸ GENRE TARGET ENCODING")
print("=" * 60)

# Check what we're working with
print(f"Training data shape: {df_training.shape}")
print(f"Liked data shape: {df_liked.shape}")
print(f"NMF data shape: {df_nmf.shape}")

# Define genre preferences (keep your existing)
disliked_genres = {
    'drone': 9, 'psychedelic': 7, 'improv': 8, 'ambient': 6, 'experimental': 7,
    'instrumental': 5, 'classical': 5, 'hardcore': 8, 'downtempo': 6, 'slowcore': 6,
    'noise': 9, 'satanic': 10, 'pagan': 8, 'metalcore': 9, 'deathcore': 10, 
    'death metal': 10, 'metal': 8, 'metallic hardcore': 9, 'beatdown deathcore': 10, 
    'nydm': 9, 'soundscape': 7, 'alternative metal': 8, 'horror punk': 8, 
    'sludge metal': 9, 'thrash metal': 9, 'death thrash metal': 10, 
    'heavy metal': 8, 'black metal': 10, 'doom metal': 9, 'death doom metal': 10,
    'techno': 8, 'hard techno': 9, 'tech house': 7, 'minimal techno': 8,
    'acid techno': 8, 'industrial techno': 9,  
    'psychedelic rock': 7, 'psychedelic pop': 7, 'neo-psychedelic': 7, 
    'psychedelic folk': 7, 'psychedelia': 7, 'psych': 7, 'psych rock': 7, 
    'psych pop': 7, 'psychedelic soul': 7, 'acid rock': 7
}

preferred_genres = {
    'chamber pop': 9, 'bedroom pop': 8.5,
    'indie folk': 8, 'post punk': 7, 'indie': 6.5, 'jangle pop': 6.3,
    'retro soul': 5, 'folk pop': 3.5, 'indie rock': 3.5, 'indie pop': 3.5,
    'baroque pop': 3.5, 'americana': 3
}

def create_genre_encoding_map(df_train, genre_col='Genres', target_col='liked', smoothing=35):
    """Create encoding map from training data only"""
    train_df = df_train.copy()
    global_mean = train_df[target_col].mean()
    
    # Prepare genres
    train_df['genre_list'] = train_df[genre_col].fillna('Unknown').apply(
        lambda x: [g.strip() for g in str(x).split(', ') if g.strip() and g not in ['seen live', 'Unknown']]
    )
    
    # Explode and calculate
    exploded = train_df.explode('genre_list')
    genre_stats = exploded.groupby('genre_list').agg(
        mean_score=(target_col, 'mean'),
        count=(target_col, 'size')
    ).reset_index()
    
    # Apply smoothing
    genre_stats['smoothed'] = (
        (genre_stats['mean_score'] * genre_stats['count']) + 
        (global_mean * smoothing)
    ) / (genre_stats['count'] + smoothing)
    
    return dict(zip(genre_stats['genre_list'], genre_stats['smoothed'])), global_mean

def encode_single_genre(genre_str, encoding_map, global_mean):
    """Encode a single genre string using the encoding map"""
    if pd.isna(genre_str) or genre_str == 'Unknown':
        return global_mean
    
    genres = [g.strip() for g in str(genre_str).split(', ') 
             if g.strip() and g not in ['seen live', 'Unknown']]
    
    if not genres:
        return global_mean
    
    # Calculate base encoded value
    base_values = []
    for genre in genres:
        if genre in encoding_map:
            base_values.append(encoding_map[genre])
        else:
            base_values.append(global_mean)
    
    if not base_values:
        return global_mean
    
    base_score = np.mean(base_values)
    
    # Apply preference adjustments
    for genre in genres:
        genre_lower = genre.lower()
        
        # Check preferred genres
        for pref_key, pref_weight in preferred_genres.items():
            if pref_key in genre_lower:
                boost = 1 + (pref_weight / 20)
                base_score = min(100, base_score * boost)
        
        # Check disliked genres
        for dislike_key, severity in disliked_genres.items():
            if dislike_key in genre_lower:
                penalty = 1 - (severity / 20)
                base_score = base_score * penalty
    
    return np.clip(base_score, 30, 100)

# Create encoding map from training data
print("\nðŸ“Š Creating genre encoding map from training data...")
encoding_map, global_mean = create_genre_encoding_map(df_training, smoothing=35)
print(f"Global mean 'liked' score: {global_mean:.2f}")
print(f"Encoded {len(encoding_map)} unique genres")

# Apply encoding to all dataframes
print("\nðŸ”§ Applying encoding to dataframes...")

# Training data
df_training['Genres_encoded'] = df_training['Genres'].apply(
    lambda x: encode_single_genre(x, encoding_map, global_mean)
)

# NMF data
df_nmf['Genres_encoded'] = df_nmf['Genres'].apply(
    lambda x: encode_single_genre(x, encoding_map, global_mean)
)

# Liked data (same encoding)
df_liked['Genres_encoded'] = df_liked['Genres'].apply(
    lambda x: encode_single_genre(x, encoding_map, global_mean)
)

print(f"\nâœ… Encoding applied to:")
print(f"  â€¢ Training: {len(df_training):,} tracks")
print(f"  â€¢ Liked: {len(df_liked):,} tracks")
print(f"  â€¢ NMF: {len(df_nmf):,} tracks")

# Show statistics
print(f"\nðŸ“Š ENCODING STATISTICS:")
print(f"Training range: {df_training['Genres_encoded'].min():.1f} - {df_training['Genres_encoded'].max():.1f}")
print(f"NMF range: {df_nmf['Genres_encoded'].min():.1f} - {df_nmf['Genres_encoded'].max():.1f}")
print(f"Liked range: {df_liked['Genres_encoded'].min():.1f} - {df_liked['Genres_encoded'].max():.1f}")

# Show sample of NMF encodings
print(f"\nðŸŽ§ NMF SAMPLE ENCODINGS (first 10):")
for idx, row in df_nmf[['Artist Name(s)', 'Album Name', 'Genres', 'Genres_encoded']].head(10).iterrows():
    artist = str(row['Artist Name(s)'])[:25] if pd.notna(row['Artist Name(s)']) else "Unknown"
    album = str(row['Album Name'])[:25] if pd.notna(row['Album Name']) else "Unknown"
    genres = str(row['Genres'])[:30] if pd.notna(row['Genres']) else 'Unknown'
    print(f"{artist:<25} | {album:<25} | {genres:<30} | {row['Genres_encoded']:6.1f}")

print("\nâœ… Genre encoding complete!")





# === ARTIST CENTRALITY ANALYSIS (2026 RETOOLED - COMPLETE NETWORK) ===

print("ðŸŽ¯ ARTIST CENTRALITY USING COMPLETE ARTIST NETWORK")
print("=" * 60)

print(f"Total artists in similarity database: {len(df_liked_similar):,}")
print(f"Training artists: {df_training['Artist Name(s)'].nunique():,}")
print(f"Liked artists: {df_liked['Artist Name(s)'].nunique():,}")
print(f"NMF artists: {df_nmf['Artist Name(s)'].nunique():,}")

def extract_primary_artist(artist_string):
    """Extract primary artist from a comma-separated list"""
    if pd.isna(artist_string):
        return ""
    return artist_string.split(',')[0].strip()

def build_complete_artist_network(df_training, df_liked, df_nmf, df_liked_similar):
    """
    Build a complete network using ALL artists and ALL similarity data
    Returns: Graph, and primary artist mapping for each track
    """
    print("\nðŸ”— BUILDING COMPLETE ARTIST NETWORK...")
    
    G = nx.Graph()
    
    # 1. Add ALL artists from ALL sources as nodes
    all_artists = set()
    
    # From training data (Top 100, Honorable, Mid, Not Liked)
    training_artists = df_training['Artist Name(s)'].apply(extract_primary_artist).unique()
    all_artists.update(training_artists)
    print(f"  â€¢ Training artists: {len(training_artists):,}")
    
    # From liked songs (network only)
    liked_artists = df_liked['Artist Name(s)'].apply(extract_primary_artist).unique()
    all_artists.update(liked_artists)
    print(f"  â€¢ Liked artists: {len(liked_artists):,}")
    
    # From NMF (test set)
    nmf_artists = df_nmf['Artist Name(s)'].apply(extract_primary_artist).unique()
    all_artists.update(nmf_artists)
    print(f"  â€¢ NMF artists: {len(nmf_artists):,}")
    
    # Remove empty strings
    all_artists.discard("")
    print(f"  â€¢ Total unique artists: {len(all_artists):,}")
    
    # Add all artists to graph with type labels
    for artist in all_artists:
        # Determine artist type
        if artist in liked_artists:
            artist_type = 'liked'
        elif artist in training_artists:
            # Check training source type
            if artist in df_training[df_training['source_type'] == 'top_100_ranked']['Artist Name(s)'].apply(extract_primary_artist).values:
                artist_type = 'top_100'
            elif artist in df_training[df_training['source_type'] == 'honorable_mention']['Artist Name(s)'].apply(extract_primary_artist).values:
                artist_type = 'honorable'
            elif artist in df_training[df_training['source_type'] == 'mid']['Artist Name(s)'].apply(extract_primary_artist).values:
                artist_type = 'mid'
            elif artist in df_training[df_training['source_type'] == 'not_liked']['Artist Name(s)'].apply(extract_primary_artist).values:
                artist_type = 'not_liked'
            else:
                artist_type = 'training'
        else:
            artist_type = 'nmf'
        
        G.add_node(artist, type=artist_type)
    
    # 2. Add ALL similarity edges from your complete database
    print(f"\nðŸ”— ADDING SIMILARITY EDGES...")
    edges_added = 0
    
    for _, row in df_liked_similar.iterrows():
        main_artist = row['Artist']
        similar_artists = str(row['Similar Artists']).split(', ')
        
        # Only add edges if main artist is in our graph
        if main_artist in G:
            for similar in similar_artists:
                if similar and similar in G:
                    G.add_edge(main_artist, similar, weight=1.0)
                    edges_added += 1
    
    print(f"  â€¢ Edges added: {edges_added:,}")
    print(f"  â€¢ Graph density: {nx.density(G):.6f}")
    
    return G

def calculate_centrality_complete(G, df_training, df_liked, df_nmf):
    """
    Calculate PageRank centrality using the complete network
    Returns: Updated dataframes with centrality scores
    """
    print("\nðŸ“Š CALCULATING CENTRALITY SCORES...")
    
    # Calculate PageRank on the complete network
    centrality = nx.pagerank(G, alpha=0.85)  # Standard damping factor
    
    # Create a mapping function
    def map_centrality(artist_string):
        primary = extract_primary_artist(artist_string)
        return centrality.get(primary, 0.0)
    
    # Apply to all dataframes
    df_training['Artist Centrality'] = df_training['Artist Name(s)'].apply(map_centrality)
    df_liked['Artist Centrality'] = df_liked['Artist Name(s)'].apply(map_centrality)
    df_nmf['Artist Centrality'] = df_nmf['Artist Name(s)'].apply(map_centrality)
    
    print(f"  â€¢ Artists with non-zero centrality: {sum(1 for v in centrality.values() if v > 0):,}")
    print(f"  â€¢ Max centrality: {max(centrality.values()) if centrality else 0:.6f}")
    print(f"  â€¢ Min centrality: {min(centrality.values()) if centrality else 0:.6f}")
    
    return df_training, df_liked, df_nmf, centrality

def normalize_centrality_scores(df_training, df_liked, df_nmf):
    """
    Normalize centrality scores to 0-100 range across ALL data
    """
    print("\nðŸ“ NORMALIZING SCORES TO 0-100 RANGE...")
    
    # Find max across ALL dataframes
    all_scores = pd.concat([
        df_training['Artist Centrality'],
        df_liked['Artist Centrality'],
        df_nmf['Artist Centrality']
    ])
    
    max_score = all_scores.max()
    
    if max_score > 0:
        # Normalize each dataframe
        df_training['Artist Centrality'] = (df_training['Artist Centrality'] / max_score) * 100
        df_liked['Artist Centrality'] = (df_liked['Artist Centrality'] / max_score) * 100
        df_nmf['Artist Centrality'] = (df_nmf['Artist Centrality'] / max_score) * 100
        
        print(f"  â€¢ Normalization factor: 1/{max_score:.6f} Ã— 100")
    else:
        print("  âš ï¸ All centrality scores are zero!")
    
    return df_training, df_liked, df_nmf

# ===== EXECUTE THE COMPLETE ANALYSIS =====
print("\n" + "=" * 60)
print("ðŸš€ EXECUTING CENTRALITY ANALYSIS")
print("=" * 60)

# 1. Build the complete network
G = build_complete_artist_network(df_training, df_liked, df_nmf, df_liked_similar)

# 2. Calculate centrality
df_training, df_liked, df_nmf, centrality_scores = calculate_centrality_complete(
    G, df_training, df_liked, df_nmf
)

# 3. Normalize scores
df_training, df_liked, df_nmf = normalize_centrality_scores(df_training, df_liked, df_nmf)

# ===== ANALYSIS & INSIGHTS =====
print("\n" + "=" * 60)
print("ðŸ“Š CENTRALITY ANALYSIS RESULTS")
print("=" * 60)

# Get top artists by centrality
top_artists = sorted(centrality_scores.items(), key=lambda x: x[1], reverse=True)[:30]

print("\nðŸ† TOP 30 MOST CENTRAL ARTISTS IN YOUR TASTE NETWORK:")
print("-" * 80)
for i, (artist, score) in enumerate(top_artists[:30], 1):
    # Find where this artist appears
    sources = []
    if artist in df_liked['Artist Name(s)'].apply(extract_primary_artist).values:
        sources.append('Liked')
    if artist in df_training[df_training['source_type'] == 'top_100_ranked']['Artist Name(s)'].apply(extract_primary_artist).values:
        sources.append('Top100')
    if artist in df_training[df_training['source_type'] == 'honorable_mention']['Artist Name(s)'].apply(extract_primary_artist).values:
        sources.append('Honorable')
    if artist in df_training[df_training['source_type'] == 'mid']['Artist Name(s)'].apply(extract_primary_artist).values:
        sources.append('Mid')
    if artist in df_training[df_training['source_type'] == 'not_liked']['Artist Name(s)'].apply(extract_primary_artist).values:
        sources.append('NotLiked')
    if artist in df_nmf['Artist Name(s)'].apply(extract_primary_artist).values:
        sources.append('NMF')
    
    normalized_score = (score / max(centrality_scores.values())) * 100 if centrality_scores else 0
    print(f"{i:2d}. {artist[:35]:<35} | {normalized_score:6.1f} | Sources: {', '.join(sources[:3])}")

# Show distribution by source type
print(f"\nðŸ“ˆ CENTRALITY DISTRIBUTION BY SOURCE TYPE:")
for source_name, source_df in [
    ('Liked', df_liked),
    ('Top 100', df_training[df_training['source_type'] == 'top_100_ranked']),
    ('Honorable', df_training[df_training['source_type'] == 'honorable_mention']),
    ('Mid', df_training[df_training['source_type'] == 'mid']),
    ('Not Liked', df_training[df_training['source_type'] == 'not_liked']),
    ('NMF', df_nmf)
]:
    if len(source_df) > 0:
        mean_centrality = source_df['Artist Centrality'].mean()
        median_centrality = source_df['Artist Centrality'].median()
        print(f"  â€¢ {source_name:<12}: Mean={mean_centrality:5.1f}, Median={median_centrality:5.1f}")

# Network statistics
print(f"\nðŸ”— NETWORK STATISTICS:")
print(f"  â€¢ Total nodes (artists): {G.number_of_nodes():,}")
print(f"  â€¢ Total edges (similarities): {G.number_of_edges():,}")
print(f"  â€¢ Connected components: {nx.number_connected_components(G):,}")
print(f"  â€¢ Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.1f}")

# Check how many NMF artists are connected to your taste network
print(f"\nðŸŽ§ NMF ARTIST CONNECTIVITY:")
nmf_artists = df_nmf['Artist Name(s)'].apply(extract_primary_artist).unique()
connected_nmf = [artist for artist in nmf_artists if artist in G and G.degree(artist) > 0]
isolated_nmf = [artist for artist in nmf_artists if artist in G and G.degree(artist) == 0]

print(f"  â€¢ Total NMF artists: {len(nmf_artists):,}")
print(f"  â€¢ Connected to your taste network: {len(connected_nmf):,} ({len(connected_nmf)/len(nmf_artists)*100:.1f}%)")
print(f"  â€¢ Isolated (no connections): {len(isolated_nmf):,} ({len(isolated_nmf)/len(nmf_artists)*100:.1f}%)")

if connected_nmf:
    print(f"\nðŸ† MOST CONNECTED NMF ARTISTS:")
    for artist in connected_nmf[:10]:
        degree = G.degree(artist)
        centrality = df_nmf[df_nmf['Artist Name(s)'].apply(extract_primary_artist) == artist]['Artist Centrality'].iloc[0]
        print(f"  â€¢ {artist[:30]:<30} | Connections: {degree:3d} | Centrality: {centrality:5.1f}")

print("\n" + "=" * 60)
print("âœ… CENTRALITY ANALYSIS COMPLETE!")
print("=" * 60)





# === ULTRA-SIMPLE CENTRALITY SUMMARY ===
print("ðŸŽ¯ NMF CENTRALITY PRIORITIZATION")
print("=" * 50)

# Get top 10 albums by centrality
top_nmf = df_nmf.groupby('Album Name').agg({
    'Artist Name(s)': 'first',
    'Artist Centrality': 'mean',
    'Track Name': 'count'
}).query('`Track Name` >= 5').nlargest(10, 'Artist Centrality')

for i, (album, row) in enumerate(top_nmf.iterrows(), 1):
    print(f"{i:2d}. {row['Artist Name(s)'][:25]:<25} - {album[:25]:<25} ({row['Artist Centrality']:.1f})")

print(f"\nðŸ“Š Stats: {len(top_nmf)} albums >70 centrality, {len(df_nmf['Album Name'].unique())} total NMF albums")
print("âœ… Use this to decide listening order!")





# === LUCY DACUS NETWORK WITH TOP 100 HIERARCHY ===

print("ðŸŒ LUCY DACUS NETWORK WITH TOP 100 HIERARCHY")
print("=" * 60)

# First, we need to rebuild the graph with correct hierarchy
def rebuild_graph_with_hierarchy(df_training, df_liked, df_nmf, df_liked_similar):
    """Rebuild graph with Top 100 trumping Liked hierarchy"""
    G = nx.Graph()
    
    def extract_primary(artist_string):
        if pd.isna(artist_string):
            return ""
        return artist_string.split(',')[0].strip()
    
    # Get artists from each source
    top_100_artists = set(df_training[df_training['source_type'] == 'top_100_ranked']
                         ['Artist Name(s)'].apply(extract_primary))
    honorable_artists = set(df_training[df_training['source_type'] == 'honorable_mention']
                          ['Artist Name(s)'].apply(extract_primary))
    mid_artists = set(df_training[df_training['source_type'] == 'mid']
                     ['Artist Name(s)'].apply(extract_primary))
    not_liked_artists = set(df_training[df_training['source_type'] == 'not_liked']
                          ['Artist Name(s)'].apply(extract_primary))
    liked_artists = set(df_liked['Artist Name(s)'].apply(extract_primary))
    nmf_artists = set(df_nmf['Artist Name(s)'].apply(extract_primary))
    
    # Add ALL artists with correct hierarchy
    all_artists = (top_100_artists | liked_artists | honorable_artists | 
                   mid_artists | not_liked_artists | nmf_artists)
    all_artists.discard("")
    
    # Determine final type for each artist (Top 100 trumps everything)
    for artist in all_artists:
        if artist in top_100_artists:
            G.add_node(artist, type='top_100')
        elif artist in liked_artists:
            G.add_node(artist, type='liked')
        elif artist in honorable_artists:
            G.add_node(artist, type='honorable')
        elif artist in mid_artists:
            G.add_node(artist, type='mid')
        elif artist in not_liked_artists:
            G.add_node(artist, type='not_liked')
        elif artist in nmf_artists:
            G.add_node(artist, type='nmf')
        else:
            G.add_node(artist, type='unknown')
    
    # Add similarity edges
    for _, row in df_liked_similar.iterrows():
        main_artist = row['Artist']
        similar_artists = str(row['Similar Artists']).split(', ')
        
        if main_artist in G:
            for similar in similar_artists:
                if similar and similar in G:
                    G.add_edge(main_artist, similar, weight=1.0)
    
    return G

# Rebuild the graph with correct hierarchy
print("ðŸ”§ Rebuilding graph with Top 100 hierarchy...")
G_fixed = rebuild_graph_with_hierarchy(df_training, df_liked, df_nmf, df_liked_similar)
print(f"âœ“ Graph rebuilt with {G_fixed.number_of_nodes():,} nodes")

if 'Lucy Dacus' in G_fixed:
    # Get Lucy's info with hierarchy
    lucy_type = G_fixed.nodes['Lucy Dacus'].get('type', 'unknown')
    
    # Check where Lucy appears
    lucy_sources = []
    if 'Lucy Dacus' in df_training[df_training['source_type'] == 'top_100_ranked']['Artist Name(s)'].apply(
        lambda x: x.split(',')[0].strip() if pd.notna(x) else "").values:
        lucy_sources.append('Top 100')
    if 'Lucy Dacus' in df_liked['Artist Name(s)'].apply(
        lambda x: x.split(',')[0].strip() if pd.notna(x) else "").values:
        lucy_sources.append('Liked Songs')
    
    lucy_centrality = df_training[
        df_training['Artist Name(s)'].apply(
            lambda x: 'Lucy Dacus' in str(x) if pd.notna(x) else False
        )
    ]['Artist Centrality'].iloc[0] if len(df_training[
        df_training['Artist Name(s)'].apply(
            lambda x: 'Lucy Dacus' in str(x) if pd.notna(x) else False
        )
    ]) > 0 else 0
    
    print(f"\nðŸŽ¯ LUCY DACUS HIERARCHY INFO:")
    print(f"  â€¢ Network Type: {lucy_type}")
    print(f"  â€¢ Appears in: {', '.join(lucy_sources) if lucy_sources else 'Unknown'}")
    print(f"  â€¢ Centrality: {lucy_centrality:.1f}")
    
    # Get connections
    lucy_direct = list(G_fixed.neighbors('Lucy Dacus'))
    
    # Categorize connections by hierarchy
    direct_by_type = {
        'top_100': [],
        'liked': [],
        'honorable': [],
        'mid': [],
        'not_liked': [],
        'nmf': [],
        'unknown': []
    }
    
    for artist in lucy_direct:
        artist_type = G_fixed.nodes[artist].get('type', 'unknown')
        # Get centrality
        centrality = None
        for df in [df_training, df_liked, df_nmf]:
            artist_rows = df[df['Artist Name(s)'].apply(
                lambda x: str(x).split(',')[0].strip() == artist if pd.notna(x) else False
            )]
            if len(artist_rows) > 0:
                centrality = artist_rows['Artist Centrality'].iloc[0]
                break
        
        direct_by_type[artist_type].append((artist, centrality or 0))
    
    print(f"\nðŸ”— DIRECT CONNECTIONS BY HIERARCHY:")
    print("-" * 80)
    
    for type_name, artists in direct_by_type.items():
        if artists:
            # Sort by centrality
            artists.sort(key=lambda x: x[1], reverse=True)
            print(f"\n{type_name.upper().replace('_', ' ')} ({len(artists)} artists):")
            for artist, centrality in artists[:10]:  # Show top 10 per type
                # Check if also in Top 100
                is_also_top_100 = artist in df_training[
                    df_training['source_type'] == 'top_100_ranked'
                ]['Artist Name(s)'].apply(
                    lambda x: x.split(',')[0].strip() if pd.notna(x) else ""
                ).values
                
                top_100_marker = " â­" if is_also_top_100 else ""
                print(f"  â€¢ {artist[:25]:<25} {top_100_marker:2} | Centrality: {centrality:5.1f}")
    
    # Create focused visualization (just direct connections for clarity)
    print(f"\nðŸŽ¨ CREATING FOCUSED VISUALIZATION...")
    
    # Get top 15 direct connections for visualization
    all_direct_with_centrality = []
    for artist in lucy_direct:
        centrality = None
        for df in [df_training, df_liked, df_nmf]:
            artist_rows = df[df['Artist Name(s)'].apply(
                lambda x: str(x).split(',')[0].strip() == artist if pd.notna(x) else False
            )]
            if len(artist_rows) > 0:
                centrality = artist_rows['Artist Centrality'].iloc[0]
                break
        all_direct_with_centrality.append((artist, centrality or 0))
    
    # Sort by centrality and take top 15
    all_direct_with_centrality.sort(key=lambda x: x[1], reverse=True)
    top_direct = [artist for artist, _ in all_direct_with_centrality[:15]]
    
    # Create subgraph
    subgraph_nodes = ['Lucy Dacus'] + top_direct
    lucy_subgraph = G_fixed.subgraph(subgraph_nodes)
    
    plt.figure(figsize=(12, 8))
    
    # Create circular layout
    pos = nx.circular_layout(lucy_subgraph)
    
    # Adjust Lucy position
    pos['Lucy Dacus'] = np.array([0, 0])
    
    # Color mapping with hierarchy
    def get_node_style(node):
        if node == 'Lucy Dacus':
            return 'purple', 1000, 'Lucy Dacus', 12  # color, size, label, fontsize
        
        node_type = lucy_subgraph.nodes[node].get('type', 'unknown')
        # Check if also in Top 100
        is_also_top_100 = node in df_training[
            df_training['source_type'] == 'top_100_ranked'
        ]['Artist Name(s)'].apply(
            lambda x: x.split(',')[0].strip() if pd.notna(x) else ""
        ).values
        
        if is_also_top_100:
            # Top 100 artists get special styling
            return 'gold', 450, f"{node} â­", 9
        elif node_type == 'top_100':
            return 'blue', 400, node, 9
        elif node_type == 'liked':
            return 'green', 350, node, 9
        elif node_type == 'honorable':
            return 'cyan', 300, node, 8
        elif node_type == 'mid':
            return 'yellow', 300, node, 8
        elif node_type == 'not_liked':
            return 'red', 300, node, 8
        elif node_type == 'nmf':
            return 'orange', 300, node, 8
        else:
            return 'gray', 250, node, 8
    
    # Draw edges
    nx.draw_networkx_edges(lucy_subgraph, pos, 
                          edge_color='purple', 
                          width=2.0, 
                          alpha=0.6)
    
    # Draw nodes with styles
    for node in lucy_subgraph.nodes():
        color, size, label, fontsize = get_node_style(node)
        
        # Draw node
        nx.draw_networkx_nodes(lucy_subgraph, pos, 
                              nodelist=[node],
                              node_color=[color],
                              node_size=[size],
                              alpha=0.9,
                              edgecolors='black',
                              linewidths=1)
        
        # Add label
        plt.text(pos[node][0], pos[node][1], label,
                fontsize=fontsize,
                fontweight='bold' if 'â­' in label or node == 'Lucy Dacus' else 'normal',
                ha='center', va='center',
                bbox=dict(boxstyle="round,pad=0.3", 
                        facecolor='white', 
                        alpha=0.9,
                        edgecolor=color))
    
    # Add centrality scores
    for node in lucy_subgraph.nodes():
        if node != 'Lucy Dacus':
            centrality = None
            for df in [df_training, df_liked, df_nmf]:
                artist_rows = df[df['Artist Name(s)'].apply(
                    lambda x: str(x).split(',')[0].strip() == node if pd.notna(x) else False
                )]
                if len(artist_rows) > 0:
                    centrality = artist_rows['Artist Centrality'].iloc[0]
                    break
            
            if centrality:
                # Position score below artist name
                plt.text(pos[node][0], pos[node][1] - 0.12,
                        f"{centrality:.1f}",
                        fontsize=8,
                        ha='center', va='center',
                        bbox=dict(boxstyle="round,pad=0.1", 
                                facecolor='lightgray', 
                                alpha=0.7))
    
    # Add Lucy's centrality
    plt.text(0, 0.15, f"Centrality: {lucy_centrality:.1f}",
            fontsize=10, fontweight='bold',
            ha='center', va='center',
            bbox=dict(boxstyle="round,pad=0.3", 
                    facecolor='white', 
                    alpha=0.9,
                    edgecolor='purple'))
    
    # Add legend
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='purple', label='Lucy Dacus (Center)'),
        Patch(facecolor='gold', label='Top 100 Artists â­'),
        Patch(facecolor='blue', label='Top 100 (Non-Liked)'),
        Patch(facecolor='green', label='Liked Songs'),
        Patch(facecolor='cyan', label='Honorable Mention'),
        Patch(facecolor='yellow', label='Mid'),
        Patch(facecolor='red', label='Not Liked'),
        Patch(facecolor='orange', label='NMF')
    ]
    
    plt.legend(handles=legend_elements, loc='upper left', fontsize=9,
              title="ARTIST HIERARCHY (Top 100 â­ trumps all)", 
              title_fontsize=10,
              bbox_to_anchor=(1.05, 1), borderaxespad=0.)
    
    # Add title
    plt.title("Lucy Dacus Network with Top 100 Hierarchy", fontsize=14, pad=20)
    
    # Add stats
    stats_text = (
        f"Direct Connections: {len(lucy_direct)}\n"
        f"Shown: Top {len(top_direct)} by centrality\n"
        f"Lucy's Type: {lucy_type} ({', '.join(lucy_sources)})\n"
        f"Top 100 connections: {len(direct_by_type['top_100'])}"
    )
    
    plt.figtext(0.02, 0.02, stats_text, fontsize=9,
               bbox=dict(boxstyle="round,pad=0.5", facecolor='white', alpha=0.8))
    
    plt.axis('off')
    plt.tight_layout()
    plt.show()
    
    print(f"\nâœ… Network visualization with hierarchy complete!")
    
else:
    print("âš ï¸ Lucy Dacus not found in the fixed graph")

print("\n" + "=" * 60)
print("ðŸŒ NETWORK ANALYSIS WITH HIERARCHY COMPLETE")
print("=" * 60)





# === MOOD & ENERGY FEATURES (SIMPLIFIED) ===

print("ðŸŽ­ MOOD & ENERGY")
print("=" * 50)

# Calculate for all dataframes
for dataframe, name in [(df_training, 'Training'), (df_nmf, 'NMF'), (df_liked, 'Liked')]:
    dataframe['mood_score'] = dataframe[['Valence', 'Danceability', 'Liveness']].mean(axis=1)
    dataframe['energy_profile'] = dataframe[['Energy', 'Loudness', 'Tempo']].mean(axis=1)

print(f"âœ“ Added to: Training({len(df_training):,}), NMF({len(df_nmf):,}), Liked({len(df_liked):,})")

# Quick stats
print(f"\nðŸ“Š Mood Score Range:")
print(f"  Training: {df_training['mood_score'].min():.3f} - {df_training['mood_score'].max():.3f}")
print(f"  NMF: {df_nmf['mood_score'].min():.3f} - {df_nmf['mood_score'].max():.3f}")

print(f"\nðŸ“Š Energy Profile Range:")
print(f"  Training: {df_training['energy_profile'].min():.3f} - {df_training['energy_profile'].max():.3f}")
print(f"  NMF: {df_nmf['energy_profile'].min():.3f} - {df_nmf['energy_profile'].max():.3f}")

print("\nâœ… Done")





# === ALBUM ID FOR NMF ===

print("ðŸŽµ CREATING ALBUM IDS")
print("=" * 50)

# For NMF dataframe only (what we're predicting)
df_nmf['Primary Artist'] = df_nmf['Artist Name(s)'].str.split(',').str[0].str.strip()
df_nmf['album_id'] = df_nmf.apply(
    lambda x: f"{x['Primary Artist'].lower()}_{x['Album Name'].lower()}".replace(' ', '_').replace('/', '_'),
    axis=1
)

print(f"âœ“ Created album IDs for {len(df_nmf)} NMF tracks")
print(f"  Unique albums: {df_nmf['Album Name'].nunique()}")
print(f"  Unique album IDs: {df_nmf['album_id'].nunique()}")

# Show sample
print(f"\nðŸ“‹ Sample album IDs:")
for _, row in df_nmf[['Artist Name(s)', 'Album Name', 'album_id']].drop_duplicates().head(5).iterrows():
    print(f"  {row['Artist Name(s)'][:20]:<20} - {row['Album Name'][:20]:<20} â†’ {row['album_id'][:30]}...")

print("\nâœ… Done")





# === CHECK COLUMNS ===

print("ðŸ“‹ COLUMNS IN EACH DATAFRAME")
print("=" * 50)

for name, df in [("Training", df_training), ("NMF", df_nmf), ("Liked", df_liked)]:
    print(f"\n{name} ({len(df.columns)} columns):")
    print(f"{', '.join(df.columns.tolist()[:12])}...")








# === STANDARDIZE NUMERIC COLUMNS ===

print("ðŸ“ STANDARDIZING NUMERIC COLUMNS")
print("=" * 50)

# Define numeric columns to scale (same for all dataframes)
numeric_columns = [
    'Genres_encoded',
    'Artist Centrality',
    'Popularity',
    'Record Label Frequency Encoded',
    'mood_score',
    'energy_profile'
]

# Check which columns exist
available_cols = [col for col in numeric_columns if col in df_training.columns]
print(f"Scaling {len(available_cols)} columns: {available_cols}")

# Initialize scaler (scale to 0-1 range)
scaler = MinMaxScaler()

# Fit on training data only
scaler.fit(df_training[available_cols])

# Transform all dataframes
df_training[available_cols] = scaler.transform(df_training[available_cols])
df_nmf[available_cols] = scaler.transform(df_nmf[available_cols])
df_liked[available_cols] = scaler.transform(df_liked[available_cols])

print(f"\nâœ… Scaled: Training({len(df_training):,}), NMF({len(df_nmf):,}), Liked({len(df_liked):,})")
print(f"   All features now in range: 0.0 - 1.0")


# === SCALE TO 1-100 RANGE ===

print("ðŸ“ SCALING TO 1-100 RANGE")
print("=" * 50)

# Scale to 1-100 (like original plan)
scaler = MinMaxScaler(feature_range=(1, 100))

# Fit on training data only
scaler.fit(df_training[numeric_columns])

# Transform all dataframes
df_training[numeric_columns] = scaler.transform(df_training[numeric_columns])
df_nmf[numeric_columns] = scaler.transform(df_nmf[numeric_columns])
df_liked[numeric_columns] = scaler.transform(df_liked[numeric_columns])

print(f"âœ… Scaled to 1-100 range")
print(f"Training range: {df_training[numeric_columns].min().min():.1f}-{df_training[numeric_columns].max().max():.1f}")
print(f"NMF range: {df_nmf[numeric_columns].min().min():.1f}-{df_nmf[numeric_columns].max().max():.1f}")

# Save NMF for later
df_nmf.to_csv('data/df_nmf_later.csv', index=False)
print(f"ðŸ“ Saved NMF to: data/df_nmf_later.csv")





# === TUNE MODELS WITH 2026 RETOOLING (NO LIKED SONGS IN TRAINING) ===

print("ðŸŒŸ TUNING MODELS (2026 RETOOLING - NO LIKED SONGS)")
print("=" * 60)

# Verify we're using the correct training data
print(f"Training data sources: {df_training['source_type'].unique()}")
print(f"Training data size: {len(df_training):,} tracks")
print(f"Liked songs NOT included: {len(df_liked):,} tracks (for network only)")

# Features for training
features = [
    'Genres_encoded', 
    'Artist Centrality',  
    'Record Label Frequency Encoded', 
    'mood_score', 
    'Popularity',
    'energy_profile'
]

print(f"\nðŸ”§ Using {len(features)} features:")
for i, feat in enumerate(features, 1):
    print(f"  {i:2d}. {feat}")

def tune_models_2026(df_training, features, test_size=0.2):
    """
    Tune models using ONLY retooled training data (no liked songs)
    """
    print("\nðŸŽ¯ TUNING RANDOM FOREST & XGBOOST...")
    
    # Prepare data from training dataframe only
    X = df_training[features]
    y = df_training['liked']
    
    # Normalize target (optional, but helps some models)
    y_mean = y.mean()
    y_std = y.std()
    y_normalized = (y - y_mean) / y_std
    
    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_normalized, test_size=test_size, random_state=42
    )
    
    # Random Forest parameters
    rf_params = {
        'n_estimators': randint(50, 300),
        'max_depth': randint(3, 15),
        'min_samples_split': randint(2, 10),
        'min_samples_leaf': randint(1, 5)
    }
    
    # XGBoost parameters  
    xgb_params = {
        'n_estimators': randint(50, 300),
        'max_depth': randint(3, 10),
        'learning_rate': uniform(0.01, 0.3),
        'subsample': uniform(0.6, 0.4),
        'colsample_bytree': uniform(0.6, 0.4)
    }
    
    # Initialize models
    rf = RandomForestRegressor(random_state=42)
    xgb = XGBRegressor(random_state=42)
    
    # Randomized search
    print("  Running RandomizedSearchCV...")
    rf_search = RandomizedSearchCV(
        rf, rf_params, n_iter=20, cv=5, 
        scoring='neg_mean_squared_error', 
        random_state=42, n_jobs=-1
    )
    xgb_search = RandomizedSearchCV(
        xgb, xgb_params, n_iter=20, cv=5,
        scoring='neg_mean_squared_error',
        random_state=42, n_jobs=-1
    )
    
    # Fit models
    rf_search.fit(X_train, y_train)
    xgb_search.fit(X_train, y_train)
    
    # Train final models with best parameters
    best_rf = RandomForestRegressor(**rf_search.best_params_, random_state=42)
    best_xgb = XGBRegressor(**xgb_search.best_params_, random_state=42)
    
    best_rf.fit(X_train, y_train)
    best_xgb.fit(X_train, y_train)
    
    print(f"  âœ“ Tuning complete")
    
    return best_rf, best_xgb, rf_search.best_params_, xgb_search.best_params_, X_test, y_test, y_mean, y_std

def predict_nmf_2026(best_rf, best_xgb, df_nmf, features, y_mean, y_std):
    """
    Predict NMF scores using tuned models
    """
    print("\nðŸŽ¯ PREDICTING NMF SCORES...")
    
    # Make predictions (models were trained on normalized y)
    rf_pred = best_rf.predict(df_nmf[features]) * y_std + y_mean
    xgb_pred = best_xgb.predict(df_nmf[features]) * y_std + y_mean
    
    # Combine predictions (simple average)
    df_nmf['predicted_score'] = (rf_pred + xgb_pred) / 2
    
    print(f"  âœ“ Predictions made for {len(df_nmf):,} NMF tracks")
    print(f"  Score range: {df_nmf['predicted_score'].min():.1f} - {df_nmf['predicted_score'].max():.1f}")
    
    return df_nmf

# Execute tuning and prediction
best_rf, best_xgb, rf_params, xgb_params, X_test, y_test, y_mean, y_std = tune_models_2026(
    df_training, features
)

df_nmf = predict_nmf_2026(
    best_rf, best_xgb, df_nmf, features, y_mean, y_std
)

# Output results
print(f"\nðŸ“Š TUNING RESULTS:")
print(f"Random Forest best params: {rf_params}")
print(f"XGBoost best params: {xgb_params}")

print(f"\nðŸŽ¯ FEATURE IMPORTANCE:")
# Random Forest importance
rf_importance = pd.DataFrame({
    'feature': features,
    'importance': best_rf.feature_importances_
}).sort_values('importance', ascending=False)

print("Random Forest (most important first):")
for i, (_, row) in enumerate(rf_importance.iterrows(), 1):
    print(f"  {i:2d}. {row['feature']:<30} {row['importance']:.3f}")

# XGBoost importance
xgb_importance = pd.DataFrame({
    'feature': features,
    'importance': best_xgb.feature_importances_
}).sort_values('importance', ascending=False)

print("\nXGBoost (most important first):")
for i, (_, row) in enumerate(xgb_importance.iterrows(), 1):
    print(f"  {i:2d}. {row['feature']:<30} {row['importance']:.3f}")

print(f"\nâœ… 2026 RETOOLING COMPLIANT: Liked songs excluded from training")
print(f"   Used only: {len(df_training):,} training tracks")





# === FINAL 80/20 ALBUM-LEVEL VALIDATION ===

print("ðŸŽ¯ FINAL 80/20 ALBUM-LEVEL VALIDATION")
print("=" * 60)

# Get unique albums
unique_albums = df_training[['Album Name', 'Artist Name(s)']].drop_duplicates()
print(f"Unique albums: {len(unique_albums):,}")

# 80/20 split at album level
albums_train, albums_test = train_test_split(unique_albums, test_size=0.2, random_state=42)

# Get tracks
train_indices = df_training[
    df_training['Album Name'].isin(albums_train['Album Name']) &
    df_training['Artist Name(s)'].isin(albums_train['Artist Name(s)'])
].index

test_indices = df_training[
    df_training['Album Name'].isin(albums_test['Album Name']) &
    df_training['Artist Name(s)'].isin(albums_test['Artist Name(s)'])
].index

print(f"\nðŸ“Š SPLIT:")
print(f"Train: {len(albums_train):,} albums â†’ {len(train_indices):,} tracks")
print(f"Test:  {len(albums_test):,} albums â†’ {len(test_indices):,} tracks")

# Prepare data
X_train = df_training.loc[train_indices, features]
y_train = df_training.loc[train_indices, 'liked']
X_test = df_training.loc[test_indices, features]
y_test = df_training.loc[test_indices, 'liked']

print(f"\nðŸŽ¯ TARGET STATS:")
print(f"Train: {y_train.min():.1f}-{y_train.max():.1f} (mean: {y_train.mean():.1f})")
print(f"Test:  {y_test.min():.1f}-{y_test.max():.1f} (mean: {y_test.mean():.1f})")

# Train
rf_model = RandomForestRegressor(**rf_params, random_state=42)
xgb_model = XGBRegressor(**xgb_params, random_state=42)

rf_model.fit(X_train, y_train)
xgb_model.fit(X_train, y_train)

# Predict
rf_pred = rf_model.predict(X_test)
xgb_pred = xgb_model.predict(X_test)
ensemble_pred = (0.8 * rf_pred) + (0.2 * xgb_pred)

# Metrics
r2 = r2_score(y_test, ensemble_pred)
mse = mean_squared_error(y_test, ensemble_pred)
mae = mean_absolute_error(y_test, ensemble_pred)
rmse = np.sqrt(mse)

print(f"\nðŸ“ˆ RESULTS:")
print(f"RÂ²:   {r2:.3f}")
print(f"MSE:  {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAE:  {mae:.2f}")

print(f"\nðŸ” INTERPRETATION:")
if r2 > 0.6:
    print(f"âœ… GOOD: RÂ² = {r2:.3f} (album-level generalization is solid)")
elif r2 > 0.4:
    print(f"âš ï¸  OKAY: RÂ² = {r2:.3f} (some predictive power)")
elif r2 > 0.2:
    print(f"ðŸ“‰ WEAK: RÂ² = {r2:.3f} (limited generalization)")
else:
    print(f"âŒ POOR: RÂ² = {r2:.3f} (album memorization problem)")

print(f"\nðŸŽ¯ COMPARISON TO TRACK-LEVEL:")
print(f"Track-level RÂ² (inflated): 0.873")
print(f"Album-level RÂ² (real):     {r2:.3f}")
print(f"Inflation:                 {0.873 - r2:.3f}")

# Show some predictions vs actual
print(f"\nðŸ“‹ SAMPLE PREDICTIONS:")
sample_results = pd.DataFrame({
    'Artist': df_training.loc[test_indices, 'Artist Name(s)'].iloc[:10],
    'Album': df_training.loc[test_indices, 'Album Name'].iloc[:10],
    'Actual': y_test.iloc[:10],
    'Predicted': ensemble_pred[:10],
    'Error': np.abs(y_test.iloc[:10] - ensemble_pred[:10])
})

for i, (_, row) in enumerate(sample_results.iterrows(), 1):
    print(f"{i:2d}. {row['Artist'][:20]:<20} | Act: {row['Actual']:5.1f} | Pred: {row['Predicted']:5.1f} | Err: {row['Error']:5.1f}")

print(f"\n" + "=" * 60)
print(f"ðŸŽ¯ FINAL VERDICT: {'GOOD' if r2 > 0.5 else 'NEEDS WORK'}")
print("=" * 60)


# === RETRAIN WITH ALBUM-AWARE SPLIT FOR NMF ===

print("\nðŸ”§ RETRAINING FOR NMF PREDICTIONS (ALBUM-AWARE)")
print("=" * 60)

# Use 80% of albums for training, 20% held out
unique_albums = df_training[['Album Name', 'Artist Name(s)']].drop_duplicates()
train_albums, _ = train_test_split(unique_albums, test_size=0.2, random_state=42)

# Get training tracks
train_indices = df_training[
    df_training['Album Name'].isin(train_albums['Album Name']) &
    df_training['Artist Name(s)'].isin(train_albums['Artist Name(s)'])
].index

print(f"Training on: {len(train_albums):,} albums â†’ {len(train_indices):,} tracks")
print(f"(Holding out {len(unique_albums) - len(train_albums):,} albums for generalization)")

# Final training
X_final = df_training.loc[train_indices, features]
y_final = df_training.loc[train_indices, 'liked']

final_rf = RandomForestRegressor(**rf_params, random_state=42)
final_xgb = XGBRegressor(**xgb_params, random_state=42)

final_rf.fit(X_final, y_final)
final_xgb.fit(X_final, y_final)

# Predict NMF
rf_nmf_pred = final_rf.predict(df_nmf[features])
xgb_nmf_pred = final_xgb.predict(df_nmf[features])
df_nmf['predicted_score'] = (0.8 * rf_nmf_pred) + (0.2 * xgb_nmf_pred)

print(f"\nâœ… RETRAINED for album generalization")
print(f"   NMF predictions updated (range: {df_nmf['predicted_score'].min():.1f}-{df_nmf['predicted_score'].max():.1f})")


# === DISCOVERY MODEL: CLASSIFICATION FOR 85+ SCORES ===
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score

# 1. Define the 'Discovery' target (1 if score >= 85, else 0)
# This captures the 'variance' that matters: Is it a favorite or not?
df_training['is_discovery'] = (df_training['liked'] >= 85).astype(int)

# 2. Use the same album-level split for consistency
X_train_clf = df_training.loc[train_indices, features]
y_train_clf = df_training.loc[train_indices, 'is_discovery']

# 3. Train the Discovery Classifier
# We use class_weight='balanced' because 85+ albums are rare gems!
clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
clf.fit(X_train_clf, y_train_clf)

print("âœ… Discovery Model (clf) trained successfully!")
print(f"   Target: Finding the {df_training['is_discovery'].sum()} tracks you rated 85+")



# === FIXED: FEATURE COMPARISON ===

print("ðŸŽ¯ FIXED: FEATURE PERFORMANCE COMPARISON")
print("=" * 60)

# Train regression model for comparison
from sklearn.ensemble import RandomForestRegressor

# Use same album split for fair comparison
X_train_reg = df_training.loc[train_indices, features]
y_train_reg = df_training.loc[train_indices, 'liked']

rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train_reg, y_train_reg)

# Get importances
reg_importance = pd.DataFrame({
    'feature': features,
    'regression_importance': rf_reg.feature_importances_
}).sort_values('regression_importance', ascending=False)

clf_importance = pd.DataFrame({
    'feature': features,
    'discovery_importance': clf.feature_importances_
}).sort_values('discovery_importance', ascending=False)

# Merge and compare
importance_compare = pd.merge(reg_importance, clf_importance, on='feature')
importance_compare['reg_rank'] = importance_compare['regression_importance'].rank(ascending=False)
importance_compare['clf_rank'] = importance_compare['discovery_importance'].rank(ascending=False)
importance_compare['rank_diff'] = importance_compare['clf_rank'] - importance_compare['reg_rank']  # Positive = better for discovery

print(f"\nðŸ“Š YOUR METRICS FIRST:")
print(f"   Score Prediction RÂ²:     0.317 (weak)")
print(f"   Discovery ROC AUC:       0.730 (decent!)")
print(f"   Discovery Accuracy:      0.807 (good!)")
print(f"   Discovery Precision:     0.263 (low - many false positives)")
print(f"   Discovery Recall:        0.175 (low - misses many you'd like)")

print(f"\nðŸ” FEATURE RANKINGS (1 = most important):")
print("=" * 80)
for i, (_, row) in enumerate(importance_compare.sort_values('reg_rank').iterrows(), 1):
    reg_rank = int(row['reg_rank'])
    clf_rank = int(row['clf_rank'])
    rank_change = clf_rank - reg_rank
    
    # Arrow direction
    if rank_change < -2:
        arrow = "â†–ï¸  MUCH BETTER for discovery"
    elif rank_change < 0:
        arrow = "â† Better for discovery"
    elif rank_change == 0:
        arrow = "â†” Same"
    elif rank_change <= 2:
        arrow = "â†’ Better for scores"
    else:
        arrow = "â†˜ï¸  MUCH BETTER for scores"
    
    print(f"{reg_rank:2d}â†’{clf_rank:2d} {arrow:<25} | {row['feature']:<30}")
    print(f"      Score importance: {row['regression_importance']:.3f}")
    print(f"      Discovery importance: {row['discovery_importance']:.3f}")

print(f"\nðŸŽ¯ KEY FINDINGS:")
print("=" * 80)

# Top discovery features
top_discovery = importance_compare.nlargest(3, 'discovery_importance')
print(f"\nðŸ† BEST FOR DISCOVERY (finding albums you'll love):")
for i, (_, row) in enumerate(top_discovery.iterrows(), 1):
    print(f"  {i}. {row['feature']}")
    print(f"     Importance: {row['discovery_importance']:.3f} (Rank: #{int(row['clf_rank'])})")

# Most improved
improved = importance_compare[importance_compare['rank_diff'] > 0].nlargest(2, 'rank_diff')
if len(improved) > 0:
    print(f"\nðŸš€ MOST IMPROVED for discovery:")
    for _, row in improved.iterrows():
        print(f"  â€¢ {row['feature']}: Rank â†‘{int(row['rank_diff'])} places")
        print(f"    (Score: #{int(row['reg_rank'])} â†’ Discovery: #{int(row['clf_rank'])})")

# Most worse
worsened = importance_compare[importance_compare['rank_diff'] < 0].nsmallest(2, 'rank_diff')
if len(worsened) > 0:
    print(f"\nðŸ“‰ WORSE for discovery:")
    for _, row in worsened.iterrows():
        print(f"  â€¢ {row['feature']}: Rank â†“{abs(int(row['rank_diff']))} places")
        print(f"    (Score: #{int(row['reg_rank'])} â†’ Discovery: #{int(row['clf_rank'])})")

print(f"\nðŸŽ¯ WHAT THIS MEANS FOR YOUR 2026 SYSTEM:")
print("=" * 80)
print(f"1. ROC AUC 0.730 = DECENT discovery ability!")
print(f"   (0.5 = random, 1.0 = perfect, 0.7 = acceptable)")
print(f"\n2. Accuracy 0.807 = GOOD at overall classification")
print(f"   (But precision 0.263 = many false 'you'll like this' predictions)")
print(f"\n3. YOUR SYSTEM WORKED because:")
print(f"   â€¢ Features guided you toward promising music")
print(f"   â€¢ You explored with an open mind")
print(f"   â€¢ First impressions (Gut Scores) captured magic")
print(f"\n4. 2026 IMPROVEMENT: Focus on discovery, not exact scores!")

print(f"\nðŸŽ§ NMF DISCOVERY RECOMMENDATIONS:")
print("=" * 80)

# Get top NMF albums by discovery chance
nmf_discovery_proba = clf.predict_proba(df_nmf[features])[:, 1]
df_nmf['discovery_chance'] = nmf_discovery_proba * 100

nmf_albums = df_nmf.groupby('Album Name').agg({
    'Artist Name(s)': 'first',
    'discovery_chance': 'mean',
    'Track Name': 'count'
}).reset_index()

nmf_albums.columns = ['Album', 'Artist', 'discovery_chance', 'tracks']
full_nmf = nmf_albums[nmf_albums['tracks'] >= 5]
full_nmf = full_nmf.sort_values('discovery_chance', ascending=False)

print(f"\nðŸ† TOP 5 NMF ALBUMS (highest discovery chance):")
for i, (_, row) in enumerate(full_nmf.head(5).iterrows(), 1):
    print(f"{i}. {row['Artist'][:25]:<25} - {row['Album'][:25]:<25}")
    print(f"   {row['discovery_chance']:.1f}% chance you'll love it (85+)")

print(f"\n" + "=" * 80)
print("âœ… BOTTOM LINE: Your system IS working for discovery!")
print("   The proof: You found great albums last year ðŸŽ‰")
print("=" * 80)



# === LET'S CHECK THE REAL ERROR METRICS ===

print("ðŸŽ¯ ACTUAL PREDICTION ERROR ANALYSIS")
print("=" * 60)

# Get predictions on test set
X_test = df_training.loc[test_indices, features]
y_test_actual = df_training.loc[test_indices, 'liked']

# Predict with our regression model
y_test_pred = rf_reg.predict(X_test)

# Calculate errors
errors = y_test_actual - y_test_pred
abs_errors = np.abs(errors)

print(f"\nðŸ“Š ERROR DISTRIBUTION:")
print(f"Mean Absolute Error (MAE): {np.mean(abs_errors):.2f} points")
print(f"Root Mean Squared Error (RMSE): {np.sqrt(np.mean(errors**2)):.2f} points")
print(f"Max Overprediction: {errors.min():.2f} points (model guessed too high)")
print(f"Max Underprediction: {errors.max():.2f} points (model guessed too low)")

print(f"\nðŸŽ¯ WHAT THIS MEANS:")
print(f"On average, predictions are off by {np.mean(abs_errors):.1f} points")
print(f"RMSE of {np.sqrt(np.mean(errors**2)):.1f} means bigger mistakes get penalized more")

print(f"\nðŸ” ERROR EXAMPLES:")
# Find some big misses
big_errors = pd.DataFrame({
    'Artist': df_training.loc[test_indices, 'Artist Name(s)'],
    'Album': df_training.loc[test_indices, 'Album Name'],
    'Actual': y_test_actual,
    'Predicted': y_test_pred,
    'Error': errors
}).sort_values('Error', key=abs, ascending=False)

print(f"\nTOP 5 BIGGEST MISSES:")
for i, row in big_errors.head(5).iterrows():
    print(f"  â€¢ {row['Artist'][:25]:<25} - {row['Album'][:20]:<20}")
    print(f"    Actual: {row['Actual']:5.1f} | Predicted: {row['Predicted']:5.1f} | Error: {row['Error']:+.1f}")

print(f"\nðŸ“ˆ WHERE ERRORS HAPPEN:")
# Check if errors are systematic
print(f"\nError by actual score range:")
score_bins = [30, 45, 60, 70, 85, 101]
for i in range(len(score_bins)-1):
    low, high = score_bins[i], score_bins[i+1]
    mask = (y_test_actual >= low) & (y_test_actual < high)
    if mask.any():
        avg_error = errors[mask].mean()
        avg_abs_error = abs_errors[mask].mean()
        print(f"  Scores {low:3d}-{high-1:3d}: Avg error = {avg_error:+.1f}, Avg abs error = {avg_abs_error:.1f}")


# === IMPLEMENT WEIGHTED TRAINING FOR 2026 RETOOLING ===

print("ðŸŽ¯ IMPLEMENTING WEIGHTED TRAINING")
print("=" * 60)

# Define importance weights based on your 2026 retooling strategy
def get_importance_weight(score):
    """
    Weight samples based on how important it is to predict them correctly
    Matches your 2026 mental effort allocation
    """
    if score >= 85:  # Elite / Top 100 territory
        return 3.0   # HIGHEST priority - flag potential favorites!
    elif score <= 40:  # Strong dislikes (30-40)
        return 2.5   # HIGH priority - avoid time wasters!
    elif 70 <= score <= 84:  # Great range
        return 1.8   # Above average importance
    elif 60 <= score <= 69:  # Good / Honorable mention
        return 1.3   # Slightly above average
    else:  # Mid range (45-59)
        return 1.0   # Normal importance - okay to be less precise

# Apply weights to training data
df_training['importance_weight'] = df_training['liked'].apply(get_importance_weight)
sample_weights = df_training['importance_weight'].values

print(f"ðŸ“Š WEIGHT DISTRIBUTION:")
weight_summary = df_training.groupby(df_training['liked'].apply(lambda x: 
    '85+' if x >= 85 else
    '70-84' if x >= 70 else
    '60-69' if x >= 60 else
    '45-59' if x >= 45 else
    '30-40')).agg({
        'importance_weight': ['count', 'mean', 'first']
    }).round(2)

print(weight_summary)

print(f"\nðŸŽ¯ 2026 RETOOLING ALIGNMENT:")
print(f"  85+ (Elite):       3.0x weight â†’ Must learn what makes favorites")
print(f"  30-40 (Dislikes):  2.5x weight â†’ Must learn what to avoid")
print(f"  70-84 (Great):     1.8x weight â†’ Important to recognize quality")
print(f"  60-69 (Good):      1.3x weight â†’ Worth getting right")
print(f"  45-59 (Mid):       1.0x weight â†’ Okay to be less precise")

# Album-aware split (to prevent memorization)
unique_albums = df_training[['Album Name', 'Artist Name(s)']].drop_duplicates()
albums_train, albums_test = train_test_split(unique_albums, test_size=0.2, random_state=42)

train_indices = df_training[
    df_training['Album Name'].isin(albums_train['Album Name']) &
    df_training['Artist Name(s)'].isin(albums_train['Artist Name(s)'])
].index

test_indices = df_training[
    df_training['Album Name'].isin(albums_test['Album Name']) &
    df_training['Artist Name(s)'].isin(albums_test['Artist Name(s)'])
].index

# Prepare weighted training data
X_train = df_training.loc[train_indices, features]
y_train = df_training.loc[train_indices, 'liked']
train_weights = df_training.loc[train_indices, 'importance_weight'].values

X_test = df_training.loc[test_indices, features]
y_test = df_training.loc[test_indices, 'liked']

print(f"\nðŸ”§ TRAINING WEIGHTED MODELS...")

# Train Random Forest with sample weights
rf_weighted = RandomForestRegressor(**rf_params, random_state=42)
rf_weighted.fit(X_train, y_train, sample_weight=train_weights)

# Train XGBoost with sample weights
xgb_weighted = XGBRegressor(**xgb_params, random_state=42)
xgb_weighted.fit(X_train, y_train, sample_weight=train_weights)

print("âœ… Weighted models trained")

# Make predictions
rf_pred_weighted = rf_weighted.predict(X_test)
xgb_pred_weighted = xgb_weighted.predict(X_test)
ensemble_pred_weighted = (0.8 * rf_pred_weighted) + (0.2 * xgb_pred_weighted)

# Calculate weighted errors
errors_weighted = y_test - ensemble_pred_weighted
abs_errors_weighted = np.abs(errors_weighted)

print(f"\nðŸ“Š WEIGHTED MODEL PERFORMANCE:")

# Calculate MAE by score category
print(f"\nðŸ” ERROR BY SCORE CATEGORY (Weighted vs Unweighted):")
print("-" * 80)

# For comparison, let's get unweighted model predictions too
rf_unweighted = RandomForestRegressor(**rf_params, random_state=42)
xgb_unweighted = XGBRegressor(**xgb_params, random_state=42)

rf_unweighted.fit(X_train, y_train)  # No weights!
xgb_unweighted.fit(X_train, y_train)

rf_pred_unweighted = rf_unweighted.predict(X_test)
xgb_pred_unweighted = xgb_unweighted.predict(X_test)
ensemble_pred_unweighted = (0.8 * rf_pred_unweighted) + (0.2 * xgb_pred_unweighted)
errors_unweighted = y_test - ensemble_pred_unweighted

# Compare by category
categories = {
    '85+': (y_test >= 85),
    '70-84': (y_test >= 70) & (y_test < 85),
    '60-69': (y_test >= 60) & (y_test < 70),
    '45-59': (y_test >= 45) & (y_test < 60),
    '30-40': (y_test <= 40)
}

print(f"{'Category':<8} {'Weighted MAE':<12} {'Unweighted MAE':<14} {'Improvement':<12}")
print("-" * 80)

for cat_name, mask in categories.items():
    if mask.any():
        weighted_mae = np.mean(abs_errors_weighted[mask])
        unweighted_mae = np.mean(np.abs(errors_unweighted[mask]))
        improvement = unweighted_mae - weighted_mae
        
        if improvement > 0:
            improvement_str = f"â†“ {improvement:.2f}"
        elif improvement < 0:
            improvement_str = f"â†‘ {abs(improvement):.2f}"
        else:
            improvement_str = "="
        
        print(f"{cat_name:<8} {weighted_mae:<12.2f} {unweighted_mae:<14.2f} {improvement_str:<12}")

# Overall metrics
overall_weighted_mae = np.mean(abs_errors_weighted)
overall_unweighted_mae = np.mean(np.abs(errors_unweighted))

print(f"\nðŸ“ˆ OVERALL IMPROVEMENT:")
print(f"  Weighted MAE:   {overall_weighted_mae:.2f}")
print(f"  Unweighted MAE: {overall_unweighted_mae:.2f}")
print(f"  Change:         {overall_unweighted_mae - overall_weighted_mae:+.2f}")

# Check elite prediction improvement
elite_mask = (y_test >= 85)
if elite_mask.any():
    elite_weighted_error = np.mean(abs_errors_weighted[elite_mask])
    elite_unweighted_error = np.mean(np.abs(errors_unweighted[elite_mask]))
    
    print(f"\nðŸŽ¯ ELITE PREDICTION (85+) IMPROVEMENT:")
    print(f"  Weighted error:   {elite_weighted_error:.2f}")
    print(f"  Unweighted error: {elite_unweighted_error:.2f}")
    print(f"  Improvement:      {elite_unweighted_error - elite_weighted_error:.2f} points!")

# Apply to NMF
print(f"\nðŸŽ§ APPLYING TO NMF PREDICTIONS...")
rf_nmf_pred = rf_weighted.predict(df_nmf[features])
xgb_nmf_pred = xgb_weighted.predict(df_nmf[features])
df_nmf['weighted_predicted_score'] = (0.8 * rf_nmf_pred) + (0.2 * xgb_nmf_pred)

# Compare with old predictions
print(f"\nðŸ”€ COMPARISON WITH OLD PREDICTIONS:")
if 'predicted_score' in df_nmf.columns:
    diff = df_nmf['weighted_predicted_score'] - df_nmf['predicted_score']
    print(f"  Average change: {diff.mean():+.2f} points")
    print(f"  Max increase:   {diff.max():+.2f} points")
    print(f"  Max decrease:   {diff.min():+.2f} points")
    
    # Show albums with biggest changes
    df_nmf['prediction_change'] = diff
    big_changes = df_nmf[['Artist Name(s)', 'Album Name', 'predicted_score', 
                          'weighted_predicted_score', 'prediction_change']].copy()
    big_changes = big_changes.sort_values('prediction_change', key=abs, ascending=False)
    
    print(f"\nðŸ“Š ALBUMS WITH BIGGEST PREDICTION CHANGES:")
    for _, row in big_changes.head(5).iterrows():
        change_str = f"+{row['prediction_change']:.1f}" if row['prediction_change'] > 0 else f"{row['prediction_change']:.1f}"
        print(f"  {row['Artist Name(s)'][:20]:<20} - {change_str:>5} points")
        print(f"    Old: {row['predicted_score']:.1f} â†’ New: {row['weighted_predicted_score']:.1f}")

print(f"\nâœ… WEIGHTED TRAINING COMPLETE!")
print(f"   Model now prioritizes 85+ and 30- predictions")
print(f"   Better aligned with your 2026 retooling strategy")





# =======================================================================================
# ðŸš€ THE DEFINITIVE 2026 NMF PREDICTION ENGINE v3.0
# =======================================================================================
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score  # Removed make_scorer from here
from sklearn.metrics import mean_squared_error, make_scorer  # Added make_scorer here
from collections import Counter
from fuzzywuzzy import process
from datetime import datetime


print("ðŸš€ EXECUTING THE DEFINITIVE 2026 PREDICTION PIPELINE")
print("=" * 60)

# --- 1. ADAPTIVE WEIGHTING FUNCTION ---
def calculate_adaptive_weights(X_train, y_train, X_pred, rf_model, xgb_model, features, n_bins=5):
    # Make predictions on training data
    rf_train_pred = rf_model.predict(X_train)
    xgb_train_pred = xgb_model.predict(X_train)
    
    # Calculate errors
    rf_errors = np.abs(rf_train_pred - y_train)
    xgb_errors = np.abs(xgb_train_pred - y_train)
    
    error_df = X_train.copy()
    error_df['rf_error'] = rf_errors
    error_df['xgb_error'] = xgb_errors
    
    # Get top features for binning
    feature_importance = pd.DataFrame({
        'feature': features, 
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    top_features = feature_importance['feature'].head(2).tolist()
    
    for feature in top_features:
        error_df[f'{feature}_bin'] = pd.qcut(error_df[feature], n_bins, labels=False, duplicates='drop')
    
    performance_by_bin = {}
    for feature in top_features:
        performance_by_bin[feature] = {}
        for bin_idx in range(n_bins):
            bin_data = error_df[error_df[f'{feature}_bin'] == bin_idx]
            if len(bin_data) > 0:
                rf_mean_error = bin_data['rf_error'].mean()
                xgb_mean_error = bin_data['xgb_error'].mean()
                total_error = rf_mean_error + xgb_mean_error
                rf_weight = xgb_mean_error / total_error if total_error > 0 else 0.5
                performance_by_bin[feature][bin_idx] = rf_weight
                
    weights = np.ones(len(X_pred)) * 0.5
    for feature in top_features:
        try:
            pred_bins = pd.qcut(X_pred[feature], n_bins, labels=False, duplicates='drop')
            for bin_idx in range(n_bins):
                if bin_idx in performance_by_bin[feature]:
                    bin_mask = (pred_bins == bin_idx)
                    if any(bin_mask):
                        rf_weight = performance_by_bin[feature][bin_idx]
                        # Blend weights: 0.3 floor to prevent extreme bias
                        scaled_weight = 0.3 + (rf_weight * 0.4)
                        weights[bin_mask] = scaled_weight
        except Exception as e:
            print(f"Warning: Could not calculate bins for {feature}: {e}")
            continue
    return weights

# --- 2. ARTIST SIMILARITY BOOST FUNCTION ---
def calculate_artist_similarity_boost(df_nmf, df_liked_similar, liked_artists_df, boost_factor=15):
    df_nmf_copy = df_nmf.copy()
    df_nmf_copy['Primary Artist'] = df_nmf_copy['Artist Name(s)'].str.split(',').str[0].str.strip()
    
    # Get set of liked artists
    liked_artists = set(liked_artists_df['Artist Name(s)'].str.split(',').str[0].str.strip())
    
    # Map similar artists
    similar_artists_dict = {}
    for _, row in df_liked_similar.iterrows():
        if pd.notna(row['Similar Artists']):
            similar_artists_dict[row['Artist']] = [s.strip() for s in row['Similar Artists'].split(',')]
            
    similar_to_liked_mapping = {}
    for liked_artist, similar_list in similar_artists_dict.items():
        for similar_artist in similar_list:
            if similar_artist not in similar_to_liked_mapping:
                similar_to_liked_mapping[similar_artist] = []
            similar_to_liked_mapping[similar_artist].append(liked_artist)
            
    def find_best_match(artist, artist_set, threshold=85):
        if artist in artist_set: return artist, 100
        matches = process.extractOne(artist, artist_set)
        return (matches[0], matches[1]) if matches and matches[1] >= threshold else (None, 0)
        
    df_nmf_copy['similarity_boost'] = 0
    df_nmf_copy['match_type'] = 'no_match'
    df_nmf_copy['matched_to'] = 'None'
    
    for i, row in df_nmf_copy.iterrows():
        artist = row['Primary Artist']
        
        # Check direct match
        match, score = find_best_match(artist, liked_artists)
        if match:
            df_nmf_copy.at[i, 'similarity_boost'] = boost_factor
            df_nmf_copy.at[i, 'match_type'] = 'direct_liked'
            df_nmf_copy.at[i, 'matched_to'] = match
            continue
            
        # Check similar match
        match, score = find_best_match(artist, set(similar_to_liked_mapping.keys()))
        if match:
            df_nmf_copy.at[i, 'similarity_boost'] = boost_factor * 0.7
            df_nmf_copy.at[i, 'match_type'] = 'similar_to_liked'
            df_nmf_copy.at[i, 'matched_to'] = ", ".join(similar_to_liked_mapping[match][:2])
            
    return df_nmf_copy

# --- 3. EXECUTION ---
# Train models with 2026 weights
X_train = df_training.loc[train_indices, features]
y_train = df_training.loc[train_indices, 'liked']
train_weights = df_training.loc[train_indices, 'importance_weight'].values

print("Training weighted models...")
rf_final = RandomForestRegressor(**rf_params, random_state=42)
rf_final.fit(X_train, y_train, sample_weight=train_weights)

xgb_final = XGBRegressor(**xgb_params, random_state=42)
xgb_final.fit(X_train, y_train, sample_weight=train_weights)

# Calculate adaptive weights for NMF
print("Calculating adaptive ensemble weights...")
nmf_weights = calculate_adaptive_weights(X_train, y_train, df_nmf[features], rf_final, xgb_final, features)

# Generate predictions
rf_nmf = rf_final.predict(df_nmf[features])
xgb_nmf = xgb_final.predict(df_nmf[features])
df_nmf['base_prediction'] = (nmf_weights * rf_nmf) + ((1 - nmf_weights) * xgb_nmf)

# Apply Similarity Boost
print("Applying artist similarity boost...")
df_nmf = calculate_artist_similarity_boost(df_nmf, df_liked_similar, df_liked)

# Final Score = Base Prediction + Similarity Boost
df_nmf['final_predicted_score'] = df_nmf['base_prediction'] + df_nmf['similarity_boost']
# Clip to 100
df_nmf['final_predicted_score'] = df_nmf['final_predicted_score'].clip(upper=100)

# --- 4. ALBUM AGGREGATION ---
nmf_results = df_nmf.groupby('Album Name').agg({
    'Artist Name(s)': 'first',
    'final_predicted_score': ['mean', 'std', 'count'],
    'match_type': 'first',
    'matched_to': 'first'
}).reset_index()

nmf_results.columns = ['Album', 'Artist', 'Score', 'Consistency', 'Tracks', 'Match Type', 'Matched To']
# Consistency: Lower std dev is better (more consistent tracks)
nmf_results['Consistency'] = nmf_results['Consistency'].fillna(0)

# Filter for full albums (5+ tracks) and sort
final_recommendations = nmf_results[nmf_results['Tracks'] >= 5].sort_values('Score', ascending=False)

print("\nâœ… 2026 NMF PREDICTIONS COMPLETE")
print(f"Top Recommendation: {final_recommendations.iloc[0]['Album']} by {final_recommendations.iloc[0]['Artist']} (Score: {final_recommendations.iloc[0]['Score']:.1f})")



# --- 5. SAVE RECOMMENDATIONS (2025 STYLE) ---
import os

# Create predictions directory if it doesn't exist
if not os.path.exists('predictions'):
    os.makedirs('predictions')

# Get the most common release date from NMF dataset
try:
    nmf_release_date = df_nmf['Release Date'].mode().iloc[0]
    # Format date as MM-DD-YY
    date_str = datetime.strptime(nmf_release_date, '%Y-%m-%d').strftime('%m-%d-%y')
except Exception as e:
    print(f"Warning: Could not parse release date, using today's date instead. Error: {e}")
    date_str = datetime.now().strftime('%m-%d-%y')

filename = f"predictions/{date_str}_Album_Recommendations.csv"

# Prepare the final dataframe for export
# We'll include the key metrics you liked from last year
export_df = final_recommendations.copy()
export_df.columns = ['Album', 'Artist', 'Predicted_Score', 'Consistency', 'Track_Count', 'Match_Type', 'Matched_To']

# Save to CSV
export_df.round(2).to_csv(filename, index=False)

print(f"\nðŸ’¾ SUCCESS: Recommendations saved to {filename}")
print("-" * 60)
print(f"Top 10 Albums for {date_str}:")
print(export_df[['Artist', 'Album', 'Predicted_Score']].head(10).to_string(index=False))






# Folder where the predictions are saved
predictions_folder = "predictions/"

# Get the latest predictions file
files = [f for f in os.listdir(predictions_folder) if f.endswith("_Album_Recommendations.csv")]

if files:
    # Find the most recently created file
    thisweek = max(files, key=lambda f: os.path.getmtime(os.path.join(predictions_folder, f)))
    print(f"Loaded latest file: {thisweek}")
    
    model_output_df = pd.read_csv(os.path.join(predictions_folder, thisweek))
    
    # FIX: Use 'Predicted_Score' instead of 'avg_score'
    score_col = 'Predicted_Score' if 'Predicted_Score' in model_output_df.columns else 'Score'
    model_output_df = model_output_df.sort_values(by=score_col, ascending=False)

    # Display all rows
    pd.set_option("display.max_rows", None)
    # Using display() is better for notebooks to show the nice table
    display(model_output_df)
else:
    print("No prediction files found in the predictions folder.")
    thisweek = None
    model_output_df = None

thisweek  # Stores the filename for reference






# My Last.fm API key
LASTFM_API_KEY = "74a510ecc9fc62bf3e0edc6adc2e99f9"

# Function to get similar artists using Last.fm API
def get_similar_artists(artist: str, api_key: str, limit: int = 5) -> dict:
    base_url = "http://ws.audioscrobbler.com/2.0/"
    params = {
        'method': 'artist.getsimilar',
        'artist': artist,
        'api_key': api_key,
        'limit': limit,
        'format': 'json'
    }
    
    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()
        data = response.json()
        
        if 'similarartists' in data and 'artist' in data['similarartists']:
            similar_artists = [artist['name'] for artist in data['similarartists']['artist']]
            return {'Artist': artist, 'Similar Artists': ", ".join(similar_artists[:limit]), 'status': 'success'}
    except Exception as e:
        return {'Artist': artist, 'Similar Artists': None, 'status': f'error: {str(e)}'}
    
    return {'Artist': artist, 'Similar Artists': None, 'status': 'no_results'}

# Function to get album art using Apple Music API
def get_album_art(artist: str, album: str) -> dict:
    try:
        url = f"https://itunes.apple.com/search?term={artist} {album}&entity=album"
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
        
        if 'results' in data and len(data['results']) > 0:
            album_art = data['results'][0].get('artworkUrl100', '').replace("100x100", "600x600")
            return {'Artist': artist, 'Album Name': album, 'Album Art': album_art, 'status': 'success'}
    except Exception as e:
        return {'Artist': artist, 'Album Name': album, 'Album Art': None, 'status': f'error: {str(e)}'}
    
    return {'Artist': artist, 'Album Name': album, 'Album Art': None, 'status': 'no_results'}

# Main function to update album data
def update_album_data(input_file: str, album_art_file: str, similar_artists_file: str, api_key: str) -> None:
    print(f"\nStarting data fetch at {datetime.now().strftime('%H:%M:%S')}")
    df_input = pd.read_csv(input_file)
    album_pairs = df_input[['Primary Artist', 'Album Name']].drop_duplicates()
    recommended_artists = df_input[df_input['playlist_origin'] == 'df_nmf']['Primary Artist'].unique()
    
    try:
        existing_album_art = pd.read_csv(album_art_file)
    except FileNotFoundError:
        existing_album_art = pd.DataFrame(columns=['Artist', 'Album Name', 'Album Art'])
    
    album_pairs = album_pairs.merge(
        existing_album_art,
        left_on=['Primary Artist', 'Album Name'],
        right_on=['Artist', 'Album Name'],
        how='left',
        indicator=True
    )
    album_pairs = album_pairs[album_pairs['_merge'] == 'left_only'].drop(columns=['_merge', 'Album Art'])
    
    df_album_art = pd.DataFrame(columns=['Artist', 'Album Name', 'Album Art'])
    df_similar = pd.DataFrame(columns=['Artist', 'Similar Artists'])
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        future_to_album = {
            executor.submit(get_album_art, row['Primary Artist'], row['Album Name']): (row['Primary Artist'], row['Album Name'])
            for _, row in album_pairs.iterrows()
        }
        for future in as_completed(future_to_album):
            result = future.result()
            if result['status'] == 'success' and result['Album Art']:
                df_album_art = pd.concat([df_album_art, pd.DataFrame([result])], ignore_index=True)
            sleep(0.25)
    
    updated_album_art = pd.concat([existing_album_art, df_album_art], ignore_index=True)
    updated_album_art.to_csv(album_art_file, index=False)
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        future_to_artist = {
            executor.submit(get_similar_artists, artist, api_key): artist
            for artist in recommended_artists
        }
        for future in as_completed(future_to_artist):
            result = future.result()
            if result['status'] == 'success' and result['Similar Artists']:
                df_similar = pd.concat([df_similar, pd.DataFrame([result])], ignore_index=True)
            sleep(0.25)
    
    df_similar.to_csv(similar_artists_file, index=False)
    print(f"\nFinished at {datetime.now().strftime('%H:%M:%S')}")

# Usage
if __name__ == "__main__":
    input_file = "data/df_nmf_later.csv"  
    album_art_file = "data/nmf_album_covers.csv"  
    similar_artists_file = "data/nmf_similar_artists.csv"  
    update_album_data(input_file, album_art_file, similar_artists_file, LASTFM_API_KEY)





def generate_spotify_links(batch_size=None, overwrite=False):
    
    output_file = 'data/nmf_album_links.csv'
    
    # Check if the output file already exists
    if os.path.exists(output_file) and not overwrite:
        print(f"Output file {output_file} already exists.")
        print("Loading existing links file instead of regenerating...")
        try:
            existing_links = pd.read_csv(output_file)
            print(f"Loaded {len(existing_links)} existing album links.")
            return existing_links
        except Exception as e:
            print(f"Error loading existing file: {e}")
            print("Continuing with link generation...")
    
    print("Starting Spotify link generation...")
    
    # Initialize Spotify client with your credentials
    client_id = "71faef9605da4db495b691d96a0daa4b"
    client_secret = "832e40da22e049bba93f29d9dbeb2e62"
    
    try:
        print("Authenticating with Spotify...")
        sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(
            client_id=client_id,
            client_secret=client_secret
        ))
        print("Spotify authentication successful")
    except Exception as e:
        print(f"Failed to authenticate with Spotify: {e}")
        return None
    
    # Read the NMF data
    try:
        print("Reading NMF data...")
        df = pd.read_csv('data/nmf.csv')
        print(f"Loaded {len(df)} tracks")
        
        # Print the column names to verify
        print("Available columns:", df.columns.tolist())
    except Exception as e:
        print(f"Error reading NMF data: {e}")
        return None
    
    try:
        # Get unique albums
        print("Extracting unique albums...")
        albums = df.drop_duplicates(subset=['Album Name'], keep='first')
        print(f"Found {len(albums)} unique albums")
    except Exception as e:
        print(f"Error extracting albums: {e}")
        return None
    
    # Function to get album ID from track URI with rate limiting
    def get_album_id(track_uri):
        if pd.isna(track_uri):
            print("Warning: Found NaN URI")
            return None
            
        print(f"Processing track: {track_uri}")
        try:
            # Extract just the ID part from the URI (spotify:track:ID_HERE)
            if isinstance(track_uri, str) and 'spotify:track:' in track_uri:
                track_id = track_uri.split(':')[-1]
            else:
                track_id = track_uri  # If it's already an ID
            
            print(f"Extracted track ID: {track_id}")
                
            # Add delay to respect rate limits
            time.sleep(0.1)  # 100ms delay between requests
            track_info = sp.track(track_id)
            album_id = track_info['album']['id']
            print(f"Found album ID: {album_id}")
            return album_id
        except spotipy.exceptions.SpotifyException as e:
            print(f"Spotify API error: {e}")
            if hasattr(e, 'http_status') and e.http_status == 429:  # Too Many Requests
                print("Rate limit hit, waiting longer...")
                time.sleep(5)  # Wait 5 seconds before retrying
                try:
                    track_info = sp.track(track_id)
                    return track_info['album']['id']
                except:
                    print(f"Still failed after retry for track {track_id}")
                    return None
            else:
                print(f"Error getting album ID for track {track_id}: {e}")
                return None
        except Exception as e:
            print(f"Unexpected error for track URI {track_uri}: {e}")
            return None
    
    # Check if we actually have the Track URI column
    track_id_column = None
    for possible_name in ['Track URI', 'track_uri', 'Track ID', 'track_id']:
        if possible_name in albums.columns:
            track_id_column = possible_name
            print(f"Found track identifier column: {track_id_column}")
            break
    
    if track_id_column is None:
        print("ERROR: Could not find a track ID or URI column. Available columns are:")
        print(albums.columns.tolist())
        return None
    
    # Determine which albums to process
    if batch_size is not None:
        albums_to_process = albums.head(batch_size)
        print(f"Processing a batch of {len(albums_to_process)} out of {len(albums)} total albums...")
    else:
        albums_to_process = albums
        print(f"Processing all {len(albums)} albums...")
    
    # Instead of using apply, let's process one by one for better error tracking
    albums['Album ID'] = None
    
    for i, (idx, row) in enumerate(albums_to_process.iterrows()):
        print(f"\nProcessing album {i+1}/{len(albums_to_process)}: {row['Album Name']}")
        album_id = get_album_id(row[track_id_column])
        albums.at[idx, 'Album ID'] = album_id
    
    # Filter out any failed lookups
    processed_albums = albums.dropna(subset=['Album ID'])
    
    processed_albums['Spotify URL'] = 'open.spotify.com/album/' + processed_albums['Album ID'].astype(str)
    
    # Select and reorder columns for output
    output_columns = ['Album Name', 'Artist Name(s)', 'Album ID', 'Spotify URL']
    # Make sure all required columns exist
    output_columns = [col for col in output_columns if col in processed_albums.columns]
    
    output_df = processed_albums[output_columns]
    
    # Save to CSV
    output_df.to_csv(output_file, index=False)
    
    print(f"Successfully generated album links for {len(output_df)} albums")
    return output_df

# Run the function
# Pass overwrite=True to regenerate links even if the file exists
result = generate_spotify_links(overwrite=True)  # Force regenerate all links
print("Function completed")





# Ensure the 'graphics/' directory exists
os.makedirs('graphics', exist_ok=True)

# Load the current notebook with 'utf-8' encoding
notebook_filename = 'Music Taste Machine Learning Data Prep.ipynb'
with open(notebook_filename, 'r', encoding='utf-8') as f:
    notebook_content = nbformat.read(f, as_version=4)

# Export the notebook as HTML
html_exporter = HTMLExporter()
html_data, resources = html_exporter.from_notebook_node(notebook_content)

# Save the HTML to the 'graphics/' folder
output_filename = 'graphics/Music_Taste_Machine_Learning_Data_Prep.html'
with open(output_filename, 'w', encoding='utf-8') as f:
    f.write(html_data)

print(f"HTML version saved to {output_filename}")


# Save clean HTML version (no outputs) to graphics folder
from nbconvert import HTMLExporter
import nbformat
import os

# Create graphics folder if it doesn't exist
os.makedirs('graphics', exist_ok=True)

# Read notebook, export without outputs, save
with open('2026 Music Taste Machine Learning Data Prep.ipynb', 'r', encoding='utf-8') as f:
    nb = nbformat.read(f, as_version=4)
    html, _ = HTMLExporter(exclude_output=True).from_notebook_node(nb)
    with open('graphics/2026_Music_Taste_Data_Prep_CLEAN_New_Years4.html', 'w', encoding='utf-8') as out:
        out.write(html)

print("âœ… Clean HTML saved to: graphics/2026_Music_Taste_Data_Prep_CLEAN_New_Years4.html")


import os
import glob
import sys

def verify_app_data():
    required_directories = [
        'data',
        'predictions',
        'feedback'
    ]

    required_data_files = [
        'data/nuked_albums.csv',
        'data/nmf_album_covers.csv',
        'data/nmf_album_links.csv',
        'data/nmf_similar_artists.csv',
        'data/liked_artists_only_similar.csv',
        'data/df_cleaned_pre_standardized.csv'
    ]

    missing_items = []

    print("\n--- Verifying Streamlit App Data Dependencies ---")

    # Check directories
    for d in required_directories:
        if not os.path.exists(d):
            missing_items.append(f"Missing directory: {d}")
        else:
            print(f"Found directory: {d}")

    # Check specific data files
    for f in required_data_files:
        if not os.path.exists(f):
            missing_items.append(f"Missing data file: {f}")
        else:
            print(f"Found data file: {f}")

    # Check for at least one prediction file
    prediction_files = glob.glob('predictions/*_Album_Recommendations.csv')
    if not prediction_files:
        missing_items.append("No prediction files found in 'predictions/' (e.g., 'predictions/MM-DD-YY_Album_Recommendations.csv')")
    else:
        print(f"Found {len(prediction_files)} prediction files in 'predictions/'")

    if missing_items:
        print("\n--- Verification FAILED ---")
        for item in missing_items:
            print(f"ERROR: {item}")
        print("\nPlease ensure your Jupyter Notebook has been run to generate all necessary data files and directories.")
        sys.exit(1) # Exit with an error code
    else:
        print("\n--- Verification SUCCESS: All required data dependencies are present! ---")
        return True

if __name__ == '__main__':
    verify_app_data()


# --- PRE-EXPORT VERIFICATION CELL ---
def verify_export_columns(df):
    # These are the columns your Streamlit app expects based on your 2026 refactor
    required_columns = [
        'Album', 
        'Artist', 
        'Predicted_Score', 
        'Consistency', 
        'Track_Count', 
        'Match_Type', 
        'Matched_To'
    ]
    
    missing = [col for col in required_columns if col not in df.columns]
    
    if missing:
        print("âŒ ERROR: Missing critical columns!")
        for col in missing:
            print(f"   - {col}")
        print("\nFix your column names before exporting to avoid Streamlit errors.")
    else:
        print("âœ… SUCCESS: All critical columns are present. Ready for export!")
        print(f"Columns: {list(df.columns)}")

# Run the check
verify_export_columns(export_df)

