


# Data Manipulation Libraries
import pandas as pd
import numpy as np

# Model Selection and Tuning
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Preprocessing
from sklearn.preprocessing import MinMaxScaler, MultiLabelBinarizer, StandardScaler

# Regression Models
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso

# Model Evaluation Metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score






df_liked = pd.read_csv("data/liked.csv")
df_fav_albums = pd.read_csv("data/liked_albums.csv")
df_not_liked = pd.read_csv("data/did_not_like.csv")
df_nmf = pd.read_csv("data/nmf.csv")








# Load similar artist recommendations
df_fav_adjacent = pd.read_csv('data/artist_recommendations.csv')

# Check the first few rows
df_fav_adjacent.head()


df_liked.head()








import urllib.parse
artist_name = "Kina Grannis, Imaginary Future"
encoded_artist = urllib.parse.quote(artist_name)



df_fav_albums.head()


df_not_liked.head()


df_nmf.head()





df_liked.columns





# Assign liked scores before combining
df_liked['liked'] = 100
df_fav_albums['liked'] = 65
df_not_liked['liked'] = 30
df_nmf['liked'] = 0

# Add playlist_origin column before combining
df_liked['playlist_origin'] = 'df_liked'
df_fav_albums['playlist_origin'] = 'df_fav_albums'
df_not_liked['playlist_origin'] = 'df_not_liked'
df_nmf['playlist_origin'] = 'df_nmf'





df_liked['liked'].head()


df_fav_albums['liked'].head()


df_not_liked['liked'].head()


df_nmf['liked'].head()





df = pd.concat([df_liked, df_fav_albums, df_not_liked, df_nmf], ignore_index=True)


metadata_columns = ['Track Name', 'Album Name', 'Artist Name(s)']
metadata_df = df[metadata_columns].copy()


# Remove duplicates: Keep the highest 'liked' score (100 > 65)
df = df.sort_values(by='liked', ascending=False)  # Ensures 100-rated songs come first
df = df.drop_duplicates(subset=['Track Name', 'Artist Name(s)'], keep='first')


# Count how many songs by each artist are liked (liked >= 45) and track playlist_origin
artist_song_count = df[df['liked'] >= 45].groupby('Artist Name(s)')['liked'].count()

# Add a column for the count of liked songs per artist
df['artist_song_count'] = df['Artist Name(s)'].apply(lambda x: artist_song_count.get(x, 0))

# Create a 'favorite_artist' column: 1 if they have at least one liked song, else 0
df['favorite_artist'] = df['artist_song_count'].apply(lambda x: 1 if x > 0 else 0)

# Now, drop or keep the `artist_song_count` column based on what you need for modeling
df.drop(columns=['artist_song_count'], inplace=True)



df.columns #Checking to remind myself what is all available to drop, keep seperate as metadata, etc.





df.drop(columns=['Track ID', 'Added By', 'Added At', 'Time Signature'], inplace=True)





 df.isna().sum()


# Fill missing genres with "Unknown"
df['Genres'] = df['Genres'].fillna("Unknown")


# Check if the same rows have missing values across these columns
df[df[['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo']].isna().any(axis=1)]


# Drop low-quality rows (missing audio features)
df.dropna(subset=['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechiness', 
                  'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo'], inplace=True)






top_labels = pd.concat([df_liked['Record Label'], df_fav_albums['Record Label']]).value_counts().head(20).index
top_labels


# Step 1: Identify the top 20 labels from both df_liked and df_fav_albums
top_labels = pd.concat([df_liked['Record Label'], df_fav_albums['Record Label']]).value_counts().head(20).index

# Step 2: One-hot encode the 'Record Label' column for the top 20 labels
for label in top_labels:
    df[label] = df['Record Label'].apply(lambda x: 1 if label in str(x) else 0)

# Step 3: Drop the original 'Record Label' column
df.drop(columns=['Record Label'], inplace=True)






 df.isna().sum()


df.columns





# Combine genres from both 'df_liked' and 'df_fav_albums' DataFrames
combined_genres = pd.concat([df_liked['Genres'], df_fav_albums['Genres']])

# Split, stack, and count the genres across both DataFrames
top_genres = combined_genres.str.split(',', expand=True).stack().value_counts().head(30).index

# Ensure "Unknown" is not processed as a valid genre
df[['Genres_1', 'Genres_2', 'Genres_3']] = df['Genres'].str.split(',', expand=True, n=2)

# Replace "Unknown" with NaN
df[['Genres_1', 'Genres_2', 'Genres_3']] = df[['Genres_1', 'Genres_2', 'Genres_3']].replace('Unknown', pd.NA)

# Create a function to check if a genre is in top_genres, ensuring we handle NAType
def genre_bump(genre_column):
    if pd.isna(genre_column):  # Explicitly check for NAType
        return False
    return genre_column in top_genres  # This will work since it's a string comparison

# Apply the bump function to each of the genre columns
df['Bump_Genre_1'] = df['Genres_1'].apply(genre_bump)
df['Bump_Genre_2'] = df['Genres_2'].apply(genre_bump)
df['Bump_Genre_3'] = df['Genres_3'].apply(genre_bump)

# Count the number of bumps (genres that match the top genres)
df['Genre_Bump_Count'] = df[['Bump_Genre_1', 'Bump_Genre_2', 'Bump_Genre_3']].sum(axis=1)

# If any of the bumps are True (i.e., genre matches and not "Unknown"), bump the song
df['Genre_Bump'] = (df['Genre_Bump_Count'] > 0).astype(int)

# Assign a score based on the number of genre matches (using 20-point multiplier)
df['Genre_Bump_Score'] = df['Genre_Bump_Count'] * 20  # Multiply by 20 for each match

# Show the top 20 rows with the adjusted score
df[['Track Name', 'Album Name', 'Artist Name(s)', 'Genres_1', 'Genres_2', 'Genres_3', 
    'Genre_Bump_Count', 'Genre_Bump_Score', 'liked']].head(20)





top_genres


df.columns





# Define numeric columns explicitly (excluding 'artist_song_count')
numeric_cols = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 
                'Speechiness', 'Acousticness', 'Instrumentalness', 
                'Liveness', 'Valence', 'Tempo']

# Ensure numeric columns exist in the dataframe before applying transformations
numeric_cols = [col for col in numeric_cols if col in df.columns]

# Ensure numeric columns don't contain NaNs before scaling
df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')
df[numeric_cols] = df[numeric_cols].fillna(0)  # Replace any remaining NaNs with 0

# Standardize numerical features
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])


#Viewing the numeric columns after standardization
df[numeric_cols]











# Drop unnecessary columns
df_cleaned = df.drop(columns=['Track Name', 'Album Name', 'Artist Name(s)', 
                              'Genres', 'Genres_1', 'Genres_2', 'Genres_3', 
                              'Bump_Genre_1', 'Bump_Genre_2', 'Bump_Genre_3',
                              'Genre_Bump_Count', 'Genre_Bump', 'Genre_Bump_Score',
                              'liked', 'Release Date', 'playlist_origin',
                              'Duration (ms)'])

# Prepare the feature matrix and target variable
X = df_cleaned
y = df['liked']  # target variable

# The data is now ready for regression modeling



# Separate the NMF data from the rest of the data
df_nmf_separated = df_cleaned[df['playlist_origin'] == 'df_nmf']
df_training_data = df_cleaned[df['playlist_origin'] != 'df_nmf']






from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Linear Regression
lr = LinearRegression()
lr.fit(X, y)

# Predictions
y_pred_lr = lr.predict(X)

# Metrics
mse_lr = mean_squared_error(y, y_pred_lr)
r2_lr = r2_score(y, y_pred_lr)

print(f"Linear Regression MSE: {mse_lr}")
print(f"Linear Regression R2: {r2_lr}")






from sklearn.linear_model import Ridge

# Ridge Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X, y)

# Predictions
y_pred_ridge = ridge.predict(X)

# Metrics
mse_ridge = mean_squared_error(y, y_pred_ridge)
r2_ridge = r2_score(y, y_pred_ridge)

print(f"Ridge Regression MSE: {mse_ridge}")
print(f"Ridge Regression R2: {r2_ridge}")






from sklearn.linear_model import Lasso

# Lasso Regression
lasso = Lasso(alpha=0.1)
lasso.fit(X, y)

# Predictions
y_pred_lasso = lasso.predict(X)

# Metrics
mse_lasso = mean_squared_error(y, y_pred_lasso)
r2_lasso = r2_score(y, y_pred_lasso)

print(f"Lasso Regression MSE: {mse_lasso}")
print(f"Lasso Regression R2: {r2_lasso}")





from sklearn.ensemble import RandomForestRegressor

# Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100)
rf.fit(X, y)

# Predictions
y_pred_rf = rf.predict(X)

# Metrics
mse_rf = mean_squared_error(y, y_pred_rf)
r2_rf = r2_score(y, y_pred_rf)

print(f"Random Forest MSE: {mse_rf}")
print(f"Random Forest R2: {r2_rf}")






from sklearn.ensemble import GradientBoostingRegressor

# Gradient Boosting Regressor
gb = GradientBoostingRegressor(n_estimators=100)
gb.fit(X, y)

# Predictions
y_pred_gb = gb.predict(X)

# Metrics
mse_gb = mean_squared_error(y, y_pred_gb)
r2_gb = r2_score(y, y_pred_gb)

print(f"Gradient Boosting MSE: {mse_gb}")
print(f"Gradient Boosting R2: {r2_gb}")






# Get feature importances for Random Forest and Gradient Boosting
rf_feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
gb_feature_importances = pd.Series(gb.feature_importances_, index=X.columns).sort_values(ascending=False)

# Top 15 features for Random Forest
print("Top 15 Random Forest Features:")
print(rf_feature_importances.head(15))

# Top 15 features for Gradient Boosting
print("Top 15 Gradient Boosting Features:")
print(gb_feature_importances.head(15))



# Focus on the bump-related columns
bump_columns = ['Bump_Genre_1', 'Bump_Genre_2', 'Bump_Genre_3', 'Genre_Bump_Count', 'Genre_Bump', 'Genre_Bump_Score']

# Include 'liked' in the correlation check
df_bumps = df[bump_columns + ['liked']]

# Compute the correlation matrix for bump-related columns
bump_correlation = df_bumps.corr()

# Get the correlation values with 'liked'
liked_bump_correlation = bump_correlation['liked'].sort_values(ascending=False)

# Show the top correlated bump features with 'liked'
print(liked_bump_correlation)






from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd

# Keep 'liked' only for the training data, ignore for NMF songs
df_cleaned = df.drop(columns=['Track Name', 'Album Name', 'Artist Name(s)', 
                              'Genres', 'Genres_1', 'Genres_2', 'Genres_3', 
                              'Bump_Genre_1', 'Bump_Genre_2', 'Bump_Genre_3',
                               'Genre_Bump', 'Genre_Bump_Score',
                              'Release Date', 'playlist_origin'])  # Exclude non-numeric columns

# Separate the NMF data from the rest of the data
df_nmf_separated = df[df['playlist_origin'] == 'df_nmf']
df_training_data = df[df['playlist_origin'] != 'df_nmf']

# Prepare the feature matrix and target variable for training (keep only numeric columns)
X_train = df_training_data.select_dtypes(include=['number']).drop(columns=['liked'])  # Remove 'liked' from training features
y_train = df_training_data['liked']  # 'liked' is the target for training

# Train the Random Forest model on the training data
rf = RandomForestRegressor(n_estimators=100)
rf.fit(X_train, y_train)

# Prepare the feature matrix for prediction using NMF data (only numeric columns)
X_nmf = df_nmf_separated.select_dtypes(include=['number']).drop(columns=['liked'])  # No 'liked' column in the prediction set

# Predict the 'liked' scores for NMF data (without interfering with the 0s)
y_pred_nmf = rf.predict(X_nmf)

# Add the predicted scores to the NMF dataset
df_nmf_separated['Predicted_Liked_Score'] = y_pred_nmf

# Group by album name only and aggregate predicted scores
aggregated_album_scores = df_nmf_separated.groupby('Album Name').agg(
    AggregatePredictedAverage=('Predicted_Liked_Score', 'mean'),  # Take the mean of predicted scores
    Artists=('Artist Name(s)', 'first'),  # Get the first artist for the album (you could also choose a more robust method if needed)
    ReleaseDate=('Release Date', 'first')  # You could add other fun data like release date
).reset_index()

# Display the head of the aggregated results
print(aggregated_album_scores.head(20))

# Optionally, save the aggregated scores to a CSV
aggregated_album_scores.to_csv('recommended_albums.csv', index=False)

# If you want to evaluate the Random Forest model on the training data
y_pred_rf_train = rf.predict(X_train)
mse_rf = mean_squared_error(y_train, y_pred_rf_train)
r2_rf = r2_score(y_train, y_pred_rf_train)

print(f"Random Forest MSE (Training): {mse_rf}")
print(f"Random Forest R2 (Training): {r2_rf}")



import matplotlib.pyplot as plt

# Visualize the distribution of predicted 'liked' scores for NMF data
plt.figure(figsize=(10, 6))
plt.hist(df_nmf_separated['Predicted_Liked_Score'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Predicted Liked Scores (NMF)', fontsize=16)
plt.xlabel('Predicted Liked Score', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.grid(True)
plt.show()






from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)

print(classification_report(y_test, y_pred_rf))



from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)

y_pred_lr = lr.predict(X_test)

print(classification_report(y_test, y_pred_lr))


from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier(n_estimators=100, random_state=42)
gbc.fit(X_train, y_train)

y_pred_gbc = gbc.predict(X_test)

print(classification_report(y_test, y_pred_gbc))



from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

y_pred_knn = knn.predict(X_test)

print(classification_report(y_test, y_pred_knn))

