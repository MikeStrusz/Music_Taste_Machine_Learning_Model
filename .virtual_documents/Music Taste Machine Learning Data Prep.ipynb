


# Data Manipulation Libraries
import pandas as pd
import numpy as np

# Model Selection and Tuning
from sklearn.model_selection import train_test_split, RandomizedSearchCV

# Preprocessing
from sklearn.preprocessing import MinMaxScaler, MultiLabelBinarizer, StandardScaler

# Regression Models
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso

# Model Evaluation Metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score






df_liked = pd.read_csv("data/liked.csv") #Liked playlist on Spotify
df_fav_albums = pd.read_csv("data/liked_albums.csv") #Albums I've Liked in Recent Years
df_not_liked = pd.read_csv("data/did_not_like.csv") #Albums I've not liked in Recent Years
df_nmf = pd.read_csv("data/nmf.csv") #The most recent New Music Friday Playlist
df_liked_similar = pd.read_csv("data/liked_artists_only_similar.csv") #Lastfm pull of similar artists my recently played artists
df_nmf_similar = pd.read_csv("data/nmf_artist_adjacent.csv") #Lastfm pull of similar artists to this weeks NMF artists


df_liked.head()


# Liked Albums in Recent Years
df_fav_albums.head()


# Albums Not Liked in Recent Years
df_not_liked.head()


# New Music Friday Playlist
df_nmf.head()


# Similar Artists to Recently Played Artists (Last.fm)
df_liked_similar.head()


# Similar Artists to NMF Artists (Last.fm)
df_nmf_similar.head()





df_liked.columns


df_liked_similar.columns





# Check the original shape sizes before filtering
print(f"Original df_liked shape: {df_liked.shape}")
print(f"Original df_fav_albums shape: {df_fav_albums.shape}")
print(f"Original df_not_liked shape: {df_not_liked.shape}")

# Now apply the filtering for the most popular song per album
df_liked = df_liked.loc[df_liked.groupby('Album Name')['Popularity'].idxmax()]
df_fav_albums = df_fav_albums.loc[df_fav_albums.groupby('Album Name')['Popularity'].idxmax()]
df_not_liked = df_not_liked.loc[df_not_liked.groupby('Album Name')['Popularity'].idxmax()]

# Check the shape again after filtering
print(f"Filtered df_liked shape: {df_liked.shape}")
print(f"Filtered df_fav_albums shape: {df_fav_albums.shape}")
print(f"Filtered df_not_liked shape: {df_not_liked.shape}")





# Assign liked scores before combining
df_liked['liked'] = 100
df_fav_albums['liked'] = 65
df_not_liked['liked'] = 30
df_nmf['liked'] = 0

# Add playlist_origin column before combining
df_liked['playlist_origin'] = 'df_liked'
df_fav_albums['playlist_origin'] = 'df_fav_albums'
df_not_liked['playlist_origin'] = 'df_not_liked'
df_nmf['playlist_origin'] = 'df_nmf'
df_liked_similar['source'] = 'liked_similar'
df_nmf_similar['source'] = 'nmf_similar'





df_liked[['liked', 'playlist_origin']].head()


df_fav_albums[['liked', 'playlist_origin']].head()


df_not_liked[['liked', 'playlist_origin']].head()


df_nmf[['liked', 'playlist_origin']].head()


df_liked_similar[['Artist', 'Similar Artists', 'source']].head()


df_nmf_similar[['Artist', 'Similar Artists', 'source']].head()





df = pd.concat([df_liked, df_fav_albums, df_not_liked, df_nmf], ignore_index=True)


#How Large is the Dataset, Now?
df.shape


# Remove duplicates: Keep the highest 'liked' score (100 > 65)
df = df.sort_values(by='liked', ascending=False)  # Ensures 100-rated songs come first
df = df.drop_duplicates(subset=['Track Name', 'Artist Name(s)'], keep='first')
df.shape


df.columns #Checking to remind myself what is all available to drop, keep seperate as metadata, etc.





df.drop(columns=['Track ID', 'Added By', 'Added At', 'Time Signature'], inplace=True)





 df.isna().sum()


#Drop all Missing Genres
df = df.dropna(subset=['Genres']).reset_index(drop=True)
df.shape


df['Record Label'] = df['Record Label'].fillna('Unknown')





def target_encode(df, column, target, smoothing=1):
    # Separate out df_nmf to ensure it's never used in encoding
    df_train = df[df['playlist_origin'] != 'df_nmf'].copy()

    mean_target = df_train[target].mean()
    label_means = df_train.groupby(column)[target].mean()
    label_counts = df_train[column].value_counts()

    smoothed_values = (label_means * label_counts + mean_target * smoothing) / (label_counts + smoothing)

    # Map with a fallback to the overall mean
    df[column + '_encoded'] = df[column].map(smoothed_values).fillna(mean_target)

    return df

# Target encode only on the training data (excludes df_nmf)
df = target_encode(df, 'Record Label', 'liked', smoothing=10)
df[['Record Label', 'Record Label_encoded', 'liked']].head()






 df.isna().sum()


df.columns





# Filter out 'df_nmf' from training data (so it doesn't influence target encoding)
df_train = df[df['playlist_origin'] != 'df_nmf'].copy()

# Calculate the overall mean of the target variable
global_mean = df_train['liked'].mean()

# Calculate the mean target value for each genre (using only non-'df_nmf' records)
genre_means = df_train.groupby('Genres')['liked'].mean()

# Count occurrences of each genre
genre_counts = df_train['Genres'].value_counts()

# Apply smoothing to avoid overfitting, especially for rare genres
smoothing_factor = 10
smoothed_values = (genre_means * genre_counts + global_mean * smoothing_factor) / (genre_counts + smoothing_factor)

# Apply encoding with fallback to global mean
df['Genre_Encoded'] = df['Genres'].map(smoothed_values).fillna(global_mean)

# Optional: Drop the original 'Genres' column to reduce dimensionality
df.drop(columns=['Genres'], inplace=True)

# Check the top rows of the updated dataframe
print(df[['Track Name', 'Album Name', 'Artist Name(s)', 'Genre_Encoded', 'liked', 'playlist_origin']].head(20))


df.columns





# Clean artist names in main dataframe
df['Artist Name(s)'] = df['Artist Name(s)'].str.replace(r',(\S)', r', \1', regex=True)

# Clean liked_similar dataframe
df_liked_similar['Artist'] = df_liked_similar['Artist'].str.strip().str.lower()
df_liked_similar['Similar Artists'] = df_liked_similar['Similar Artists'].str.strip().str.lower()

# Clean nmf_similar dataframe
df_nmf_similar['Artist'] = df_nmf_similar['Artist'].str.strip().str.lower()
df_nmf_similar['Similar Artists'] = df_nmf_similar['Similar Artists'].str.strip().str.lower()



import networkx as nx
# Create a graph for liked artists and NMF artists
G_artists = nx.Graph()
# Add edges based on similar artists (from both df_liked_similar and df_nmf_similar)
for _, row in df_liked_similar.iterrows():
    G_artists.add_edge(row['Artist'], row['Similar Artists'])
for _, row in df_nmf_similar.iterrows():
    G_artists.add_edge(row['Artist'], row['Similar Artists'])  


# Get list of artists you've liked (exclude df_nmf playlist_origin)
liked_artists = df[df['playlist_origin'] != 'df_nmf']['Artist Name(s)'].str.strip().str.lower().unique()

# Let's verify our liked artists
print(f"Number of liked artists: {len(liked_artists)}")
print("Sample of liked artists:", liked_artists[:5])  # Show first 5 to verify

# NEW UPDATED FUNCTION that handles multiple artists
def artist_degree_detailed(artist, liked_artists, graph):
    artist_list = [a.strip().lower() for a in artist.split(',')]
    
    # Check if any artists are in the graph
    artists_in_graph = [a for a in artist_list if a in graph]
    if not artists_in_graph:
        return 'not_in_graph'
    
    # For artists in graph, get their degrees to liked artists
    all_degrees = []
    for single_artist in artists_in_graph:
        degrees = [nx.shortest_path_length(graph, source=liked_artist, target=single_artist)
                  for liked_artist in liked_artists 
                  if liked_artist in graph and nx.has_path(graph, liked_artist, single_artist)]
        if degrees:
            all_degrees.extend(degrees)
    
    if not all_degrees:
        return 'in_graph_no_path'
    return min(all_degrees)

# Apply new detailed function
df_nmf['Artist_Degree_Detailed'] = df_nmf['Artist Name(s)'].apply(
    lambda x: artist_degree_detailed(x, liked_artists, G_artists)
)

# Show new distribution
print("Detailed Degree Statistics:")
print(df_nmf['Artist_Degree_Detailed'].value_counts())

# Show some examples of each category
for category in df_nmf['Artist_Degree_Detailed'].unique():
    print(f"\nExamples of {category}:")
    print(df_nmf[df_nmf['Artist_Degree_Detailed'] == category][['Artist Name(s)', 'Track Name']].head(3))


# Check some stats about our data
print(f"Number of liked artists: {len(liked_artists)}")
print(f"Number of nodes in graph: {len(G_artists.nodes())}")
print(f"Number of edges in graph: {len(G_artists.edges())}")

# Check if a few random liked artists are in the graph
sample_artists = liked_artists[:3]  # First 3 liked artists
for artist in sample_artists:
    print(f"Is '{artist}' in graph? {artist in G_artists}")


# Check for some stats about the degrees
print("\nDegree Statistics:")
print(df_nmf['Artist_Degree'].value_counts().sort_index())

# Look at some examples across different degrees
for degree in sorted(df_nmf['Artist_Degree'].unique()):
    print(f"\nArtists with degree {degree}:")
    print(df_nmf[df_nmf['Artist_Degree'] == degree][['Artist Name(s)', 'Track Name']].head(3))


# Get Lucy Dacus' neighbors
neighbors_lucy_dacus = list(G_artists.neighbors(lucy_dacus))
print(f"Neighbors of Lucy Dacus: {neighbors_lucy_dacus}")



# Keep existing degree calculation
df_nmf['Artist_Degree'] = df_nmf['Artist Name(s)'].apply(lambda x: artist_degree(x, liked_artists, G_artists))

# Add new detailed analysis
df_nmf['Artist_Degree_Detailed'] = df_nmf['Artist Name(s)'].apply(
    lambda x: artist_degree_detailed(x, liked_artists, G_artists)
)

# Show both distributions
print("Original Degree Distribution:")
print(df_nmf['Artist_Degree'].value_counts().sort_index())

print("\nDetailed Degree Distribution:")
print(df_nmf['Artist_Degree_Detailed'].value_counts())

# Compare for a few specific examples
print("\nComparison for specific tracks:")
print(df_nmf[['Track Name', 'Artist Name(s)', 'Artist_Degree', 'Artist_Degree_Detailed']].head(10))


print(G_artists.nodes)



df.columns


import seaborn as sns

def clean_and_add_features(df, df_liked_similar, df_nmf_similar):
    """
    Clean dataframe and add network features properly
    """
    # Remove duplicate columns from merges
    columns_to_drop = ['source_x', 'source_y', 'Similar_Artists_x', 
                      'Similar_Artists_y', 'source', 'Similar_Artists']
    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])
    
    # Create artist network
    G_artists = nx.Graph()
    
    # Add edges from both similar artists dataframes
    for df_similar in [df_liked_similar, df_nmf_similar]:
        for _, row in df_similar.iterrows():
            artist = row['Artist'].lower().strip()
            similar = row['Similar_Artists'].lower().strip()
            G_artists.add_edge(artist, similar)
    
    # Get liked artists
    liked_artists = set(df[df['liked'] == 1]['Artist Name(s)'].str.lower().str.strip())
    
    def calculate_artist_features(artist):
        artist = artist.lower().strip()
        if artist not in G_artists:
            return pd.Series({
                'artist_min_degree': 999,  # High number for artists not in network
                'artist_avg_degree': 999,
                'artist_total_connections': 0,
                'artist_liked_connections': 0
            })
        
        # Calculate degrees to liked artists
        degrees = []
        for liked_artist in liked_artists:
            if liked_artist in G_artists and nx.has_path(G_artists, artist, liked_artist):
                degrees.append(nx.shortest_path_length(G_artists, artist, liked_artist))
        
        # Get direct connections
        neighbors = set(G_artists.neighbors(artist))
        liked_neighbors = len(neighbors.intersection(liked_artists))
        
        return pd.Series({
            'artist_min_degree': min(degrees) if degrees else 999,
            'artist_avg_degree': np.mean(degrees) if degrees else 999,
            'artist_total_connections': len(neighbors),
            'artist_liked_connections': liked_neighbors
        })
    
    # Add network features
    network_features = df['Artist Name(s)'].apply(calculate_artist_features)
    df = pd.concat([df, network_features], axis=1)
    
    # Create visualizations
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Artist Network Features Distribution')
    
    # Plot distributions
    for idx, col in enumerate(['artist_min_degree', 'artist_avg_degree', 
                             'artist_total_connections', 'artist_liked_connections']):
        row = idx // 2
        col_idx = idx % 2
        sns.histplot(data=df[df[col] < 999], x=col, ax=axes[row, col_idx])
        axes[row, col_idx].set_title(col.replace('_', ' ').title())
    
    plt.tight_layout()
    
    return df, G_artists





# Define numeric columns explicitly (excluding 'artist_song_count')
numeric_cols = ['Danceability', 'Energy', 'Key', 'Loudness', 'Mode', 
                'Speechiness', 'Acousticness', 'Instrumentalness', 
                'Liveness', 'Valence', 'Tempo']

# Ensure numeric columns exist in the dataframe before applying transformations
numeric_cols = [col for col in numeric_cols if col in df.columns]

# Ensure numeric columns don't contain NaNs before scaling
df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')
df[numeric_cols] = df[numeric_cols].fillna(0)  # Replace any remaining NaNs with 0

# Standardize numerical features
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])


#Viewing the numeric columns after standardization
df[numeric_cols]











# Drop unnecessary columns
df_cleaned = df.drop(columns=['Track Name', 'Album Name', 'Artist Name(s)', 
                              'Genres', 'Genres_1', 'Genres_2', 'Genres_3', 
                              'Popularity', 'Bump_Genre_2', 'Bump_Genre_3',
                              'Genre_Bump_Count', 'Genre_Bump', 'Genre_Bump_Score',
                              'liked', 'Release Date', 'playlist_origin',
                              'Duration (ms)'])

# Prepare the feature matrix and target variable'
X = df_cleaned
y = df['liked']  # target variable

# The data is now ready for regression modeling



# Separate the NMF data from the rest of the data
df_nmf_separated = df_cleaned[df['playlist_origin'] == 'df_nmf']
df_training_data = df_cleaned[df['playlist_origin'] != 'df_nmf']






from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Linear Regression
lr = LinearRegression()
lr.fit(X, y)

# Predictions
y_pred_lr = lr.predict(X)

# Metrics
mse_lr = mean_squared_error(y, y_pred_lr)
r2_lr = r2_score(y, y_pred_lr)

print(f"Linear Regression MSE: {mse_lr}")
print(f"Linear Regression R2: {r2_lr}")






from sklearn.linear_model import Ridge

# Ridge Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X, y)

# Predictions
y_pred_ridge = ridge.predict(X)

# Metrics
mse_ridge = mean_squared_error(y, y_pred_ridge)
r2_ridge = r2_score(y, y_pred_ridge)

print(f"Ridge Regression MSE: {mse_ridge}")
print(f"Ridge Regression R2: {r2_ridge}")






from sklearn.linear_model import Lasso

# Lasso Regression
lasso = Lasso(alpha=0.1)
lasso.fit(X, y)

# Predictions
y_pred_lasso = lasso.predict(X)

# Metrics
mse_lasso = mean_squared_error(y, y_pred_lasso)
r2_lasso = r2_score(y, y_pred_lasso)

print(f"Lasso Regression MSE: {mse_lasso}")
print(f"Lasso Regression R2: {r2_lasso}")





from sklearn.ensemble import RandomForestRegressor

# Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100)
rf.fit(X, y)

# Predictions
y_pred_rf = rf.predict(X)

# Metrics
mse_rf = mean_squared_error(y, y_pred_rf)
r2_rf = r2_score(y, y_pred_rf)

print(f"Random Forest MSE: {mse_rf}")
print(f"Random Forest R2: {r2_rf}")






from sklearn.ensemble import GradientBoostingRegressor

# Gradient Boosting Regressor
gb = GradientBoostingRegressor(n_estimators=100)
gb.fit(X, y)

# Predictions
y_pred_gb = gb.predict(X)

# Metrics
mse_gb = mean_squared_error(y, y_pred_gb)
r2_gb = r2_score(y, y_pred_gb)

print(f"Gradient Boosting MSE: {mse_gb}")
print(f"Gradient Boosting R2: {r2_gb}")






# Get feature importances for Random Forest and Gradient Boosting
rf_feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
gb_feature_importances = pd.Series(gb.feature_importances_, index=X.columns).sort_values(ascending=False)

# Top 15 features for Random Forest
print("Top 15 Random Forest Features:")
print(rf_feature_importances.head(15))

# Top 15 features for Gradient Boosting
print("Top 15 Gradient Boosting Features:")
print(gb_feature_importances.head(15))



# Focus on the bump-related columns
bump_columns = ['Bump_Genre_1', 'Bump_Genre_2', 'Bump_Genre_3', 'Genre_Bump_Count', 'Genre_Bump', 'Genre_Bump_Score']

# Include 'liked' in the correlation check
df_bumps = df[bump_columns + ['liked']]

# Compute the correlation matrix for bump-related columns
bump_correlation = df_bumps.corr()

# Get the correlation values with 'liked'
liked_bump_correlation = bump_correlation['liked'].sort_values(ascending=False)

# Show the top correlated bump features with 'liked'
print(liked_bump_correlation)






from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd

# Keep 'liked' only for the training data, ignore for NMF songs
df_cleaned = df.drop(columns=['Track Name', 'Album Name', 'Artist Name(s)', 
                              'Genres', 'Genres_1', 'Genres_2', 'Genres_3', 
                              'Bump_Genre_2', 'Bump_Genre_3', 'Popularity',
                               'Genre_Bump', 'Genre_Bump_Score',
                              'Release Date', 'playlist_origin'])  # Exclude non-numeric columns

# Separate the NMF data from the rest of the data
df_nmf_separated = df[df['playlist_origin'] == 'df_nmf']
df_training_data = df[df['playlist_origin'] != 'df_nmf']

# Prepare the feature matrix and target variable for training (keep only numeric columns)
X_train = df_training_data.select_dtypes(include=['number']).drop(columns=['liked'])  # Remove 'liked' from training features
y_train = df_training_data['liked']  # 'liked' is the target for training

# Train the Random Forest model on the training data
rf = RandomForestRegressor(n_estimators=100)
rf.fit(X_train, y_train)

# Prepare the feature matrix for prediction using NMF data (only numeric columns)
X_nmf = df_nmf_separated.select_dtypes(include=['number']).drop(columns=['liked'])  # No 'liked' column in the prediction set

# Predict the 'liked' scores for NMF data (without interfering with the 0s)
y_pred_nmf = rf.predict(X_nmf)

# Add the predicted scores to the NMF dataset
df_nmf_separated['Predicted_Liked_Score'] = y_pred_nmf

# Group by album name only and aggregate predicted scores
aggregated_album_scores = df_nmf_separated.groupby('Album Name').agg(
    AggregatePredictedAverage=('Predicted_Liked_Score', 'mean'),  # Take the mean of predicted scores
    Artists=('Artist Name(s)', 'first'),  # Get the first artist for the album (you could also choose a more robust method if needed)
    ReleaseDate=('Release Date', 'first')  # You could add other fun data like release date
).reset_index()

# Display the head of the aggregated results
print(aggregated_album_scores.head(20))

# Optionally, save the aggregated scores to a CSV
aggregated_album_scores.to_csv('recommended_albums.csv', index=False)

# If you want to evaluate the Random Forest model on the training data
y_pred_rf_train = rf.predict(X_train)
mse_rf = mean_squared_error(y_train, y_pred_rf_train)
r2_rf = r2_score(y_train, y_pred_rf_train)

print(f"Random Forest MSE (Training): {mse_rf}")
print(f"Random Forest R2 (Training): {r2_rf}")



import matplotlib.pyplot as plt

# Visualize the distribution of predicted 'liked' scores for NMF data
plt.figure(figsize=(10, 6))
plt.hist(df_nmf_separated['Predicted_Liked_Score'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Predicted Liked Scores (NMF)', fontsize=16)
plt.xlabel('Predicted Liked Score', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.grid(True)
plt.show()






from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)

print(classification_report(y_test, y_pred_rf))



from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)

y_pred_lr = lr.predict(X_test)

print(classification_report(y_test, y_pred_lr))


from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier(n_estimators=100, random_state=42)
gbc.fit(X_train, y_train)

y_pred_gbc = gbc.predict(X_test)

print(classification_report(y_test, y_pred_gbc))



from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

y_pred_knn = knn.predict(X_test)

print(classification_report(y_test, y_pred_knn))

