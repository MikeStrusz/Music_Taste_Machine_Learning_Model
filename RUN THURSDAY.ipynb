{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826242c6-c59e-42e0-9d2f-adc2631ada21",
   "metadata": {},
   "source": [
    "# RUN THURSDAY.ipynb\n",
    "# ====================================================================\n",
    "# üìä GUT SCORE INTEGRATION PIPELINE\n",
    "# Run this on Thursday to update training data with your latest ratings\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13c7a42-daa4-4248-b7e4-d1f2bf333e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóìÔ∏è RUN THURSDAY: Clean Output Version\n",
      "============================================================\n",
      "üìÖ 2026-01-16 20:52:02\n",
      "============================================================\n",
      "\n",
      "üìä Loading gut scores from MASTER file...\n",
      "‚úÖ Found 21 total rated albums in master file\n",
      "\n",
      "üìÅ Loading training data...\n",
      "  Current: 10,284 tracks\n",
      "  Existing gut-scored tracks: 165\n",
      "\n",
      "üéµ Processing NEW gut-scored albums...\n",
      "  Processing...\n",
      "  ‚ùå Could not find tracks for: A$AP Rocky - Don't Be Dumb\n",
      "\n",
      "============================================================\n",
      "üìä WEEKLY REPORT\n",
      "============================================================\n",
      "‚úÖ ADDED THIS WEEK:\n",
      "------------------------------------------------------------\n",
      "  ‚Ä¢ Julianna Barwick;Mary Lat - Tragic Magic              ‚Üí 61.0\n",
      "  ‚Ä¢ Tyler Ramsey;Carl Broemel - Celestun                  ‚Üí 54.0\n",
      "  ‚Ä¢ The Sha La Das            - Your Picture              ‚Üí 53.0\n",
      "  ‚Ä¢ A$AP Rocky                - Don't Be Dumb             ‚Üí 57.0\n",
      "  ‚Ä¢ Robbie Williams;Tony Iomm - BRITPOP                   ‚Üí 44.0\n",
      "  ‚Ä¢ Jana Horn                 - Jana Horn                 ‚Üí 50.0\n",
      "  ‚Ä¢ Courtney Marie Andrews    - Valentine                 ‚Üí 65.0\n",
      "------------------------------------------------------------\n",
      "  Tracks added: 49\n",
      "  Albums added: 7\n",
      "\n",
      "üìà OVERALL TOTALS:\n",
      "------------------------------------------------------------\n",
      "  Total rated albums: 21\n",
      "  Gut-scored tracks: 214\n",
      "  Total training tracks: 10,333\n",
      "\n",
      "============================================================\n",
      "‚úÖ THURSDAY PROCESSING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ====================================================================\n",
    "# üéØ THURSDAY: CLEAN OUTPUT VERSION\n",
    "# Only shows NEW albums being processed\n",
    "# ====================================================================\n",
    "\n",
    "print(\"üóìÔ∏è RUN THURSDAY: Clean Output Version\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìÖ \" + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- STEP 1: LOAD GUT SCORES FROM MASTER FILE ---\n",
    "print(\"\\nüìä Loading gut scores from MASTER file...\")\n",
    "\n",
    "master_file = 'feedback/master_gut_scores.csv'\n",
    "\n",
    "if not os.path.exists(master_file):\n",
    "    print(f\"‚ùå Master file not found: {master_file}\")\n",
    "    exit()\n",
    "\n",
    "master_df = pd.read_csv(master_file)\n",
    "master_df = master_df[master_df['gut_score'].notna()]\n",
    "\n",
    "if len(master_df) == 0:\n",
    "    print(\"‚ùå No gut scores found!\")\n",
    "    exit()\n",
    "\n",
    "total_albums = len(master_df)\n",
    "print(f\"‚úÖ Found {total_albums} total rated albums in master file\")\n",
    "\n",
    "# --- STEP 2: LOAD TRAINING DATA ---\n",
    "print(\"\\nüìÅ Loading training data...\")\n",
    "training_file = 'data/2026_training_complete_with_features.csv'\n",
    "\n",
    "if not os.path.exists(training_file):\n",
    "    print(f\"‚ùå Training file not found: {training_file}\")\n",
    "    exit()\n",
    "\n",
    "df_training = pd.read_csv(training_file)\n",
    "print(f\"  Current: {len(df_training):,} tracks\")\n",
    "\n",
    "# Count existing gut-scored tracks\n",
    "if 'source_type' in df_training.columns:\n",
    "    existing_gut = df_training[df_training['source_type'] == 'gut_score_rated']\n",
    "    existing_count = len(existing_gut)\n",
    "    print(f\"  Existing gut-scored tracks: {existing_count:,}\")\n",
    "else:\n",
    "    existing_count = 0\n",
    "\n",
    "# --- STEP 3: FIND AND ADD ONLY NEW TRACKS ---\n",
    "print(\"\\nüéµ Processing NEW gut-scored albums...\")\n",
    "\n",
    "# Get archives (newest first)\n",
    "archive_files = sorted(glob.glob('data/archived_nmf_with_features/*.csv'), reverse=True)\n",
    "\n",
    "if not archive_files:\n",
    "    print(\"‚ùå No archive files found!\")\n",
    "    exit()\n",
    "\n",
    "tracks_added = 0\n",
    "new_albums = 0\n",
    "new_albums_list = []\n",
    "\n",
    "for _, fb_row in master_df.iterrows():\n",
    "    artist = fb_row['Artist']\n",
    "    album = fb_row['Album']\n",
    "    score = fb_row['gut_score']\n",
    "    \n",
    "    # Skip if already in training as gut_score_rated\n",
    "    already_in_training = df_training[\n",
    "        (df_training['Album Name'] == album) &\n",
    "        (df_training['Artist Name(s)'].str.contains(artist, na=False)) &\n",
    "        (df_training['source_type'] == 'gut_score_rated')\n",
    "    ]\n",
    "    \n",
    "    if len(already_in_training) > 0:\n",
    "        continue  # SILENTLY skip - don't print anything!\n",
    "    \n",
    "    # ---- ONLY REACH HERE FOR NEW ALBUMS ----\n",
    "    if new_albums == 0:  # First new album\n",
    "        print(\"  Processing...\")\n",
    "    \n",
    "    new_albums += 1\n",
    "    new_albums_list.append((artist, album, score))\n",
    "    \n",
    "    # Search archives\n",
    "    found_tracks = None\n",
    "    for archive_file in archive_files:\n",
    "        try:\n",
    "            archive_df = pd.read_csv(archive_file)\n",
    "            album_tracks = archive_df[\n",
    "                (archive_df['Album Name'] == album) &\n",
    "                (archive_df['Artist Name(s)'].str.contains(artist, na=False))\n",
    "            ].copy()\n",
    "            \n",
    "            if len(album_tracks) > 0:\n",
    "                found_tracks = album_tracks\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if found_tracks is None:\n",
    "        print(f\"  ‚ùå Could not find tracks for: {artist} - {album}\")\n",
    "        continue\n",
    "    \n",
    "    # Add gut score\n",
    "    found_tracks['liked'] = score\n",
    "    found_tracks['source_type'] = 'gut_score_rated'\n",
    "    found_tracks['gut_score_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Add to training\n",
    "    df_training = pd.concat([df_training, found_tracks], ignore_index=True)\n",
    "    tracks_added += len(found_tracks)\n",
    "\n",
    "# --- STEP 4: DUPLICATE HANDLING (SILENT) ---\n",
    "if tracks_added > 0:\n",
    "    # Silent duplicate handling\n",
    "    def clean_artist_name(artist_str):\n",
    "        if pd.isna(artist_str): return \"\"\n",
    "        artist = str(artist_str).strip()\n",
    "        separators = [' feat. ', ' featuring ', ' ft. ', ' with ', ' & ', ' and ', ';', ',']\n",
    "        for sep in separators:\n",
    "            if sep in artist.lower():\n",
    "                artist = artist.split(sep)[0].strip()\n",
    "        return artist\n",
    "    \n",
    "    df_training['artist_clean'] = df_training['Artist Name(s)'].apply(clean_artist_name)\n",
    "    df_training = df_training.sort_values('liked', ascending=False)\n",
    "    df_training = df_training.drop_duplicates(\n",
    "        subset=['Album Name', 'artist_clean', 'Track Name'],\n",
    "        keep='first'\n",
    "    )\n",
    "    df_training = df_training.drop(columns=['artist_clean'], errors='ignore')\n",
    "\n",
    "# --- STEP 5: SAVE AND REPORT ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä WEEKLY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if tracks_added > 0:\n",
    "    # Create backup\n",
    "    backup_dir = 'data/backups'\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    backup_file = f\"{backup_dir}/training_backup_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "    pd.read_csv(training_file).to_csv(backup_file, index=False)\n",
    "    \n",
    "    # Save updated training\n",
    "    df_training.to_csv(training_file, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ ADDED THIS WEEK:\")\n",
    "    print(\"-\" * 60)\n",
    "    for artist, album, score in new_albums_list:\n",
    "        print(f\"  ‚Ä¢ {artist[:25]:<25} - {album[:25]:<25} ‚Üí {score}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  Tracks added: {tracks_added:,}\")\n",
    "    print(f\"  Albums added: {new_albums}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No new gut scores to add this week\")\n",
    "\n",
    "# Final stats\n",
    "print(f\"\\nüìà OVERALL TOTALS:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Total rated albums: {total_albums}\")\n",
    "print(f\"  Gut-scored tracks: {len(df_training[df_training['source_type'] == 'gut_score_rated']):,}\")\n",
    "print(f\"  Total training tracks: {len(df_training):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ THURSDAY PROCESSING COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139bb5c1-6e93-4e06-a94f-9378add17e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóìÔ∏è RUN THURSDAY: Enhanced with Release Dates\n",
      "============================================================\n",
      "üìÖ 2026-01-16 20:52:04\n",
      "============================================================\n",
      "\n",
      "üìä Loading gut scores from MASTER file...\n",
      "‚úÖ Found 21 total rated albums in master file\n",
      "\n",
      "üìÅ Loading training data...\n",
      "  Current: 10,333 tracks\n",
      "  Existing gut-scored tracks: 214\n",
      "\n",
      "üóÇÔ∏è  Scanning archive files...\n",
      "  Found 2 archive weeks\n",
      "\n",
      "üéµ Processing NEW gut-scored albums...\n",
      "  ‚ùå Could not find tracks for: A$AP Rocky - Don't Be Dumb\n",
      "\n",
      "============================================================\n",
      "üìä WEEKLY REPORT\n",
      "============================================================\n",
      "‚ÑπÔ∏è No new gut scores to add this week\n",
      "\n",
      "üìà OVERALL TOTALS:\n",
      "------------------------------------------------------------\n",
      "  Total gut-scored tracks: 214\n",
      "  Total rated albums: 21\n",
      "  Total training tracks: 10,333\n",
      "\n",
      "============================================================\n",
      "‚úÖ THURSDAY PROCESSING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# üéØ THURSDAY: ENHANCED WITH RELEASE DATES\n",
    "# Shows which NMF week each album came from\n",
    "# ====================================================================\n",
    "\n",
    "print(\"üóìÔ∏è RUN THURSDAY: Enhanced with Release Dates\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìÖ \" + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- STEP 1: LOAD GUT SCORES FROM MASTER FILE ---\n",
    "print(\"\\nüìä Loading gut scores from MASTER file...\")\n",
    "\n",
    "master_file = 'feedback/master_gut_scores.csv'\n",
    "\n",
    "if not os.path.exists(master_file):\n",
    "    print(f\"‚ùå Master file not found: {master_file}\")\n",
    "    exit()\n",
    "\n",
    "master_df = pd.read_csv(master_file)\n",
    "master_df = master_df[master_df['gut_score'].notna()]\n",
    "\n",
    "if len(master_df) == 0:\n",
    "    print(\"‚ùå No gut scores found!\")\n",
    "    exit()\n",
    "\n",
    "total_albums = len(master_df)\n",
    "print(f\"‚úÖ Found {total_albums} total rated albums in master file\")\n",
    "\n",
    "# --- STEP 2: LOAD TRAINING DATA ---\n",
    "print(\"\\nüìÅ Loading training data...\")\n",
    "training_file = 'data/2026_training_complete_with_features.csv'\n",
    "\n",
    "if not os.path.exists(training_file):\n",
    "    print(f\"‚ùå Training file not found: {training_file}\")\n",
    "    exit()\n",
    "\n",
    "df_training = pd.read_csv(training_file)\n",
    "print(f\"  Current: {len(df_training):,} tracks\")\n",
    "\n",
    "# Count existing gut-scored tracks\n",
    "if 'source_type' in df_training.columns:\n",
    "    existing_gut = df_training[df_training['source_type'] == 'gut_score_rated']\n",
    "    existing_count = len(existing_gut)\n",
    "    print(f\"  Existing gut-scored tracks: {existing_count:,}\")\n",
    "else:\n",
    "    existing_count = 0\n",
    "\n",
    "# --- STEP 3: MAP ARCHIVE FILES TO DATES ---\n",
    "print(\"\\nüóÇÔ∏è  Scanning archive files...\")\n",
    "archive_files = sorted(glob.glob('data/archived_nmf_with_features/*.csv'), reverse=True)\n",
    "\n",
    "if not archive_files:\n",
    "    print(\"‚ùå No archive files found!\")\n",
    "    exit()\n",
    "\n",
    "# Create mapping: filename -> human readable date\n",
    "archive_date_map = {}\n",
    "for archive_file in archive_files:\n",
    "    filename = os.path.basename(archive_file)\n",
    "    # Extract date: \"2026-01-11_nmf_complete.csv\" -> \"2026-01-11\"\n",
    "    date_str = filename.split('_')[0]\n",
    "    \n",
    "    try:\n",
    "        # Convert to readable format: \"Jan 11, 2026\"\n",
    "        date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        readable_date = date_obj.strftime('%b %d, %Y')\n",
    "        archive_date_map[archive_file] = readable_date\n",
    "    except:\n",
    "        archive_date_map[archive_file] = date_str\n",
    "\n",
    "print(f\"  Found {len(archive_files)} archive weeks\")\n",
    "\n",
    "# --- STEP 4: PROCESS NEW ALBUMS WITH RELEASE DATES ---\n",
    "print(\"\\nüéµ Processing NEW gut-scored albums...\")\n",
    "\n",
    "tracks_added = 0\n",
    "new_albums = 0\n",
    "new_albums_info = []  # Store (artist, album, score, release_date, track_count)\n",
    "\n",
    "for _, fb_row in master_df.iterrows():\n",
    "    artist = fb_row['Artist']\n",
    "    album = fb_row['Album']\n",
    "    score = fb_row['gut_score']\n",
    "    \n",
    "    # Skip if already in training\n",
    "    already_in_training = df_training[\n",
    "        (df_training['Album Name'] == album) &\n",
    "        (df_training['Artist Name(s)'].str.contains(artist, na=False)) &\n",
    "        (df_training['source_type'] == 'gut_score_rated')\n",
    "    ]\n",
    "    \n",
    "    if len(already_in_training) > 0:\n",
    "        continue\n",
    "    \n",
    "    # Search archives\n",
    "    found_tracks = None\n",
    "    found_archive = None\n",
    "    release_date = \"Unknown\"\n",
    "    \n",
    "    for archive_file in archive_files:\n",
    "        try:\n",
    "            archive_df = pd.read_csv(archive_file)\n",
    "            album_tracks = archive_df[\n",
    "                (archive_df['Album Name'] == album) &\n",
    "                (archive_df['Artist Name(s)'].str.contains(artist, na=False))\n",
    "            ].copy()\n",
    "            \n",
    "            if len(album_tracks) > 0:\n",
    "                found_tracks = album_tracks\n",
    "                found_archive = archive_file\n",
    "                release_date = archive_date_map.get(archive_file, \"Unknown\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if found_tracks is None:\n",
    "        print(f\"  ‚ùå Could not find tracks for: {artist} - {album}\")\n",
    "        continue\n",
    "    \n",
    "    new_albums += 1\n",
    "    new_albums_info.append({\n",
    "        'artist': artist,\n",
    "        'album': album, \n",
    "        'score': score,\n",
    "        'release_date': release_date,\n",
    "        'tracks': len(found_tracks)\n",
    "    })\n",
    "    \n",
    "    # Add gut score\n",
    "    found_tracks['liked'] = score\n",
    "    found_tracks['source_type'] = 'gut_score_rated'\n",
    "    found_tracks['gut_score_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    found_tracks['nmf_release_date'] = release_date\n",
    "    \n",
    "    # Add to training\n",
    "    df_training = pd.concat([df_training, found_tracks], ignore_index=True)\n",
    "    tracks_added += len(found_tracks)\n",
    "\n",
    "# --- STEP 5: SILENT DUPLICATE HANDLING ---\n",
    "if tracks_added > 0:\n",
    "    def clean_artist_name(artist_str):\n",
    "        if pd.isna(artist_str): return \"\"\n",
    "        artist = str(artist_str).strip()\n",
    "        separators = [' feat. ', ' featuring ', ' ft. ', ' with ', ' & ', ' and ', ';', ',']\n",
    "        for sep in separators:\n",
    "            if sep in artist.lower():\n",
    "                artist = artist.split(sep)[0].strip()\n",
    "        return artist\n",
    "    \n",
    "    df_training['artist_clean'] = df_training['Artist Name(s)'].apply(clean_artist_name)\n",
    "    df_training = df_training.sort_values('liked', ascending=False)\n",
    "    df_training = df_training.drop_duplicates(\n",
    "        subset=['Album Name', 'artist_clean', 'Track Name'],\n",
    "        keep='first'\n",
    "    )\n",
    "    df_training = df_training.drop(columns=['artist_clean'], errors='ignore')\n",
    "\n",
    "# --- STEP 6: ENHANCED REPORT WITH RELEASE DATES ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä WEEKLY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if tracks_added > 0:\n",
    "    # Create backup\n",
    "    backup_dir = 'data/backups'\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    backup_file = f\"{backup_dir}/training_backup_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "    pd.read_csv(training_file).to_csv(backup_file, index=False)\n",
    "    \n",
    "    # Save updated training\n",
    "    df_training.to_csv(training_file, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ ADDED THIS WEEK ({len(new_albums_info)} albums):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Group by release date\n",
    "    albums_by_date = {}\n",
    "    for info in new_albums_info:\n",
    "        date = info['release_date']\n",
    "        if date not in albums_by_date:\n",
    "            albums_by_date[date] = []\n",
    "        albums_by_date[date].append(info)\n",
    "    \n",
    "    # Show by release date (most recent first)\n",
    "    for date in sorted(albums_by_date.keys(), reverse=True):\n",
    "        print(f\"\\nüìÖ {date}:\")\n",
    "        for info in albums_by_date[date]:\n",
    "            print(f\"  ‚Ä¢ {info['artist'][:22]:<22} - {info['album'][:22]:<22}\")\n",
    "            print(f\"      Score: {info['score']} | Tracks: {info['tracks']}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  Total tracks added: {tracks_added:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No new gut scores to add this week\")\n",
    "\n",
    "# Final stats with breakdown\n",
    "print(f\"\\nüìà OVERALL TOTALS:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Get all gut-scored tracks\n",
    "if 'nmf_release_date' in df_training.columns:\n",
    "    gut_scored = df_training[df_training['source_type'] == 'gut_score_rated']\n",
    "    \n",
    "    if len(gut_scored) > 0:\n",
    "        # Count by release date\n",
    "        date_counts = gut_scored['nmf_release_date'].value_counts()\n",
    "        \n",
    "        print(f\"  Gut-scored tracks by release week:\")\n",
    "        for date, count in date_counts.head(5).items():  # Show top 5\n",
    "            print(f\"    {date}: {count:,} tracks\")\n",
    "        \n",
    "        if len(date_counts) > 5:\n",
    "            print(f\"    ... and {len(date_counts) - 5} more weeks\")\n",
    "    \n",
    "    print(f\"  Total gut-scored tracks: {len(gut_scored):,}\")\n",
    "else:\n",
    "    gut_scored = df_training[df_training['source_type'] == 'gut_score_rated']\n",
    "    print(f\"  Total gut-scored tracks: {len(gut_scored):,}\")\n",
    "\n",
    "print(f\"  Total rated albums: {total_albums}\")\n",
    "print(f\"  Total training tracks: {len(df_training):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ THURSDAY PROCESSING COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "058b147e-84ad-4eaa-a209-e772ee5a56c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üï∞Ô∏è CELL 3: HISTORICAL ALBUM PROCESSOR (FIXED VERSION)\n",
      "============================================================\n",
      "üìä Total gut scores in master: 21\n",
      "\n",
      "üéØ Identifying NEW albums (including historical)...\n",
      "Found 1 albums needing processing\n",
      "\n",
      "üîç Searching for album data (with regex escaping)...\n",
      "\n",
      "============================================================\n",
      "üìä SEARCH RESULTS\n",
      "============================================================\n",
      "‚úÖ Ready to add (has audio features): 1 albums\n",
      "‚ö†Ô∏è  Needs audio features: 0 albums\n",
      "‚ùå Not found in any source: 0 albums\n",
      "\n",
      "‚ö†Ô∏è  Albums with special characters (1):\n",
      "----------------------------------------\n",
      " 1. A$AP Rocky           - Don't Be Dumb       \n",
      "     Special chars: '$'\n",
      "\n",
      "üéµ Albums ready for immediate addition:\n",
      "----------------------------------------\n",
      " 1. A$AP Rocky             - Don't Be Dumb          ‚ö†Ô∏è\n",
      "     Score: 57.0 | Tracks: 17 | Source: 2026-01-16_nmf_...\n",
      "\n",
      "üíæ Auto-adding 1 albums to training...\n",
      "  ‚úì Added: A$AP Rocky                - Don't Be Dumb            \n",
      "\n",
      "‚úÖ Successfully added 1 albums to training!\n",
      "üìà New totals: 231 gut-scored tracks out of 10,350 total\n",
      "\n",
      "============================================================\n",
      "üéØ RECOMMENDED NEXT STEPS:\n",
      "============================================================\n",
      "1. Add 'Historical Rating' page to Streamlit app\n",
      "2. Check artist/album names for special characters ($, ., *, etc.)\n",
      "3. Consider fuzzy matching for artist names with variations\n",
      "4. Create batch Spotify API fetcher for albums needing features\n",
      "\n",
      "‚úÖ CELL 3 COMPLETE (with regex escaping fix)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# üï∞Ô∏è CELL 3: HISTORICAL ALBUM PROCESSOR (FIXED FOR SPECIAL CHARACTERS)\n",
    "# For re-rating old albums and adding to training\n",
    "# ====================================================================\n",
    "\n",
    "print(\"üï∞Ô∏è CELL 3: HISTORICAL ALBUM PROCESSOR (FIXED VERSION)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import re\n",
    "\n",
    "def safe_contains(series, text):\n",
    "    \"\"\"Safely check if series contains text, escaping regex special chars\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return pd.Series([False] * len(series))\n",
    "    \n",
    "    escaped_text = re.escape(str(text))\n",
    "    return series.str.contains(escaped_text, na=False, regex=True)\n",
    "\n",
    "def find_album_tracks_safely(df, artist, album):\n",
    "    \"\"\"Safely find album tracks accounting for special characters and variations\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Try exact match first\n",
    "    mask = (\n",
    "        (df['Album Name'] == album) &\n",
    "        safe_contains(df['Artist Name(s)'], artist)\n",
    "    )\n",
    "    exact_matches = df[mask]\n",
    "    \n",
    "    if len(exact_matches) > 0:\n",
    "        return exact_matches\n",
    "    \n",
    "    # Try fuzzy matching for album name\n",
    "    for album_col in ['Album Name', 'Album']:\n",
    "        if album_col in df.columns:\n",
    "            for artist_col in ['Artist Name(s)', 'Artist']:\n",
    "                if artist_col in df.columns:\n",
    "                    # Check for partial matches\n",
    "                    album_mask = df[album_col].str.contains(re.escape(album), na=False, regex=True, case=False)\n",
    "                    artist_mask = safe_contains(df[artist_col], artist)\n",
    "                    \n",
    "                    matches = df[album_mask & artist_mask]\n",
    "                    if len(matches) > 0:\n",
    "                        return matches\n",
    "    \n",
    "    return pd.DataFrame()  # Empty if not found\n",
    "\n",
    "# Load ALL gut scores (including from historical ratings)\n",
    "master_file = 'feedback/master_gut_scores.csv'\n",
    "master_df = pd.read_csv(master_file)\n",
    "master_df = master_df[master_df['gut_score'].notna()]\n",
    "\n",
    "print(f\"üìä Total gut scores in master: {len(master_df)}\")\n",
    "\n",
    "# Load training to see what's already there\n",
    "training_file = 'data/2026_training_complete_with_features.csv'\n",
    "df_training = pd.read_csv(training_file)\n",
    "\n",
    "# Identify NEW albums not yet in training (including historical)\n",
    "print(\"\\nüéØ Identifying NEW albums (including historical)...\")\n",
    "\n",
    "new_albums_to_process = []\n",
    "missing_albums = []\n",
    "\n",
    "for _, fb_row in master_df.iterrows():\n",
    "    artist = fb_row['Artist']\n",
    "    album = fb_row['Album']\n",
    "    score = fb_row['gut_score']\n",
    "    \n",
    "    # Check if already in training as gut_score_rated\n",
    "    already_in_training = df_training[\n",
    "        (df_training['Album Name'] == album) &\n",
    "        safe_contains(df_training['Artist Name(s)'], artist) &\n",
    "        (df_training['source_type'] == 'gut_score_rated')\n",
    "    ]\n",
    "    \n",
    "    if len(already_in_training) > 0:\n",
    "        continue  # Already processed\n",
    "    \n",
    "    new_albums_to_process.append({\n",
    "        'artist': artist,\n",
    "        'album': album,\n",
    "        'score': score,\n",
    "        'source_file': fb_row.get('source_file', 'unknown'),\n",
    "        'raw_artist': artist,  # Keep original for debugging\n",
    "        'raw_album': album     # Keep original for debugging\n",
    "    })\n",
    "\n",
    "print(f\"Found {len(new_albums_to_process)} albums needing processing\")\n",
    "\n",
    "# Search for these albums in ALL possible sources\n",
    "print(\"\\nüîç Searching for album data (with regex escaping)...\")\n",
    "\n",
    "# 1. Recent archives (with full features)\n",
    "archive_files = sorted(glob.glob('data/archived_nmf_with_features/*.csv'), reverse=True)\n",
    "# 2. Old prediction files (metadata only)\n",
    "prediction_files = sorted(glob.glob('predictions/*_Album_Recommendations.csv'), reverse=True)\n",
    "\n",
    "processed_count = 0\n",
    "ready_for_features = []\n",
    "needs_features = []\n",
    "special_char_albums = []\n",
    "\n",
    "for album_info in new_albums_to_process:\n",
    "    artist = album_info['artist']\n",
    "    album = album_info['album']\n",
    "    \n",
    "    # Check for special characters\n",
    "    special_chars = re.findall(r'[^\\w\\s\\-\\.\\']', artist + album)\n",
    "    if special_chars:\n",
    "        special_char_albums.append({\n",
    "            'artist': artist,\n",
    "            'album': album,\n",
    "            'chars': special_chars\n",
    "        })\n",
    "    \n",
    "    found = False\n",
    "    \n",
    "    # Search in recent archives (has features)\n",
    "    for archive_file in archive_files:\n",
    "        try:\n",
    "            archive_df = pd.read_csv(archive_file)\n",
    "            tracks = find_album_tracks_safely(archive_df, artist, album)\n",
    "            \n",
    "            if len(tracks) > 0:\n",
    "                # Found with features - can add directly\n",
    "                ready_for_features.append({\n",
    "                    'artist': artist,\n",
    "                    'album': album,\n",
    "                    'score': album_info['score'],\n",
    "                    'tracks': tracks,\n",
    "                    'source': 'archive',\n",
    "                    'source_file': archive_file,\n",
    "                    'special_chars': special_chars\n",
    "                })\n",
    "                found = True\n",
    "                break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if not found:\n",
    "        # Search in old predictions (metadata only)\n",
    "        for pred_file in prediction_files:\n",
    "            try:\n",
    "                pred_df = pd.read_csv(pred_file)\n",
    "                \n",
    "                # Handle column name variations\n",
    "                album_col = 'Album' if 'Album' in pred_df.columns else 'Album Name'\n",
    "                artist_col = 'Artist' if 'Artist' in pred_df.columns else 'Artist Name(s)'\n",
    "                \n",
    "                # Use safe search\n",
    "                if album_col in pred_df.columns and artist_col in pred_df.columns:\n",
    "                    album_mask = pred_df[album_col].str.contains(re.escape(album), na=False, regex=True, case=False)\n",
    "                    artist_mask = safe_contains(pred_df[artist_col], artist)\n",
    "                    \n",
    "                    album_rows = pred_df[album_mask & artist_mask]\n",
    "                    \n",
    "                    if len(album_rows) > 0:\n",
    "                        # Found metadata but needs audio features\n",
    "                        needs_features.append({\n",
    "                            'artist': artist,\n",
    "                            'album': album,\n",
    "                            'score': album_info['score'],\n",
    "                            'source': 'prediction',\n",
    "                            'source_file': pred_file,\n",
    "                            'special_chars': special_chars,\n",
    "                            'prediction_data': album_rows.iloc[0].to_dict()\n",
    "                        })\n",
    "                        found = True\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    if not found:\n",
    "        missing_albums.append({\n",
    "            'artist': artist,\n",
    "            'album': album,\n",
    "            'special_chars': special_chars\n",
    "        })\n",
    "\n",
    "# Report findings\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä SEARCH RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"‚úÖ Ready to add (has audio features): {len(ready_for_features)} albums\")\n",
    "print(f\"‚ö†Ô∏è  Needs audio features: {len(needs_features)} albums\")\n",
    "print(f\"‚ùå Not found in any source: {len(missing_albums)} albums\")\n",
    "\n",
    "if special_char_albums:\n",
    "    print(f\"\\n‚ö†Ô∏è  Albums with special characters ({len(special_char_albums)}):\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, item in enumerate(special_char_albums):\n",
    "        chars_str = ', '.join([f\"'{c}'\" for c in item['chars'][:3]])\n",
    "        if len(item['chars']) > 3:\n",
    "            chars_str += f\" (+{len(item['chars']) - 3} more)\"\n",
    "        print(f\"{i+1:2d}. {item['artist'][:20]:<20} - {item['album'][:20]:<20}\")\n",
    "        print(f\"     Special chars: {chars_str}\")\n",
    "\n",
    "if ready_for_features:\n",
    "    print(f\"\\nüéµ Albums ready for immediate addition:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, item in enumerate(ready_for_features[:10]):\n",
    "        filename = os.path.basename(item['source_file'])\n",
    "        char_note = \"‚ö†Ô∏è\" if item['special_chars'] else \"\"\n",
    "        print(f\"{i+1:2d}. {item['artist'][:22]:<22} - {item['album'][:22]:<22} {char_note}\")\n",
    "        print(f\"     Score: {item['score']} | Tracks: {len(item['tracks'])} | Source: {filename[:15]}...\")\n",
    "    if len(ready_for_features) > 10:\n",
    "        print(f\"     ... and {len(ready_for_features) - 10} more\")\n",
    "\n",
    "if needs_features:\n",
    "    print(f\"\\nüîß Albums needing feature fetching:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, item in enumerate(needs_features[:5]):\n",
    "        char_note = \"‚ö†Ô∏è\" if item['special_chars'] else \"\"\n",
    "        print(f\"{i+1:2d}. {item['artist'][:22]:<22} - {item['album'][:22]:<22} {char_note}\")\n",
    "        print(f\"     Score: {item['score']} | Source: prediction file\")\n",
    "\n",
    "if missing_albums:\n",
    "    print(f\"\\n‚ùå Albums not found (need investigation):\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, missing in enumerate(missing_albums[:5]):\n",
    "        char_note = \"‚ö†Ô∏è\" if missing['special_chars'] else \"\"\n",
    "        print(f\"{i+1:2d}. {missing['artist'][:25]:<25} - {missing['album'][:25]:<25} {char_note}\")\n",
    "        if missing['special_chars']:\n",
    "            chars_str = ', '.join([f\"'{c}'\" for c in missing['special_chars'][:3]])\n",
    "            print(f\"     Special chars: {chars_str}\")\n",
    "    if len(missing_albums) > 5:\n",
    "        print(f\"     ... and {len(missing_albums) - 5} more\")\n",
    "\n",
    "# Auto-add ready albums if we found any\n",
    "if ready_for_features:\n",
    "    print(f\"\\nüíæ Auto-adding {len(ready_for_features)} albums to training...\")\n",
    "    \n",
    "    for item in ready_for_features:\n",
    "        tracks = item['tracks'].copy()\n",
    "        tracks['liked'] = item['score']\n",
    "        tracks['source_type'] = 'gut_score_rated'\n",
    "        tracks['gut_score_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        tracks['historical_source'] = 're_rated'\n",
    "        tracks['special_chars_note'] = str(item['special_chars']) if item['special_chars'] else ''\n",
    "        \n",
    "        df_training = pd.concat([df_training, tracks], ignore_index=True)\n",
    "        print(f\"  ‚úì Added: {item['artist'][:25]:<25} - {item['album'][:25]:<25}\")\n",
    "    \n",
    "    # Save updated training\n",
    "    df_training.to_csv(training_file, index=False)\n",
    "    print(f\"\\n‚úÖ Successfully added {len(ready_for_features)} albums to training!\")\n",
    "    \n",
    "    # Show updated stats\n",
    "    updated_training = pd.read_csv(training_file)\n",
    "    gut_scored = updated_training[updated_training['source_type'] == 'gut_score_rated']\n",
    "    print(f\"üìà New totals: {len(gut_scored):,} gut-scored tracks out of {len(updated_training):,} total\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ RECOMMENDED NEXT STEPS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Add 'Historical Rating' page to Streamlit app\")\n",
    "print(\"2. Check artist/album names for special characters ($, ., *, etc.)\")\n",
    "print(\"3. Consider fuzzy matching for artist names with variations\")\n",
    "print(\"4. Create batch Spotify API fetcher for albums needing features\")\n",
    "\n",
    "print(\"\\n‚úÖ CELL 3 COMPLETE (with regex escaping fix)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944cd287-2516-4ddf-981b-62f4f79045d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for Hidden Gems...\n",
      "  Checking 2026-01-16...\n",
      "  Checking 2026-01-09...\n",
      "  Checking 2025-10-17...\n",
      "  Checking 2025-10-10...\n",
      "  Checking 2025-09-26...\n",
      "  Checking 2025-09-19...\n",
      "  Checking 2025-09-12...\n",
      "  Checking 2025-09-05...\n",
      "  Checking 2025-08-29...\n",
      "  Checking 2025-08-22...\n",
      "  Checking 2025-08-15...\n",
      "  Checking 2025-08-08...\n",
      "  Checking 2025-08-01...\n",
      "  Checking 2025-07-25...\n",
      "  Checking 2025-07-18...\n",
      "  Checking 2025-07-11...\n",
      "  Checking 2025-06-27...\n",
      "  Checking 2025-06-20...\n",
      "  Checking 2025-06-13...\n",
      "  Checking 2025-06-06...\n",
      "  Checking 2025-05-30...\n",
      "  Checking 2025-05-23...\n",
      "  Checking 2025-05-16...\n",
      "  Checking 2025-05-09...\n",
      "  Checking 2025-05-02...\n",
      "  Checking 2025-04-25...\n",
      "  Checking 2025-04-18...\n",
      "  Checking 2025-04-11...\n",
      "    Warning: No score column in predictions\\04-11-25_Album_Recommendations.csv\n",
      "  Checking 2025-04-04...\n",
      "  Checking 2025-03-28...\n",
      "  Checking 2025-03-21...\n",
      "  Checking 2025-03-14...\n",
      "  Checking 2025-03-07...\n",
      "  Checking 2025-02-28...\n",
      "  Checking 2025-02-21...\n",
      "  Checking 2025-02-14...\n",
      "‚úÖ Found 210 Hidden Gems! Saved to data/hidden_gems_cache.csv\n",
      "üìä Also saved 20 random samples to data/hidden_gems_sample.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "def get_album_history(artist, album):\n",
    "    \"\"\"Check if album appears in training data\"\"\"\n",
    "    try:\n",
    "        training_file = 'data/2026_training_complete_with_features.csv'\n",
    "        training_df = pd.read_csv(training_file)\n",
    "        \n",
    "        album_tracks = training_df[\n",
    "            (training_df['Album Name'] == album) &\n",
    "            (training_df['Artist Name(s)'].str.contains(artist, na=False))\n",
    "        ]\n",
    "        \n",
    "        if len(album_tracks) == 0:\n",
    "            return \"Never Rated\"\n",
    "        \n",
    "        source_type = album_tracks.iloc[0]['source_type']\n",
    "        history_map = {\n",
    "            'top_100_ranked': \"Top 100\",\n",
    "            'honorable_mention': \"Honorable Mention\",\n",
    "            'mid': \"Mid Albums\",\n",
    "            'not_liked': \"Not Liked\",\n",
    "            'gut_score_rated': \"Gut Scored\"\n",
    "        }\n",
    "        \n",
    "        return history_map.get(source_type, \"Unknown\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking history for {artist} - {album}: {e}\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "def update_hidden_gems():\n",
    "    \"\"\"Find all Never Rated albums with predicted score ‚â•75\"\"\"\n",
    "    print(\"üîç Searching for Hidden Gems...\")\n",
    "    \n",
    "    # Get all prediction files (last 12 months)\n",
    "    prediction_files = []\n",
    "    max_date_obj = None\n",
    "    \n",
    "    for file in glob.glob('predictions/*_Album_Recommendations.csv'):\n",
    "        try:\n",
    "            date_str = os.path.basename(file).split('_')[0]\n",
    "            date_obj = datetime.strptime(date_str, '%m-%d-%y')\n",
    "            \n",
    "            if max_date_obj is None or date_obj > max_date_obj:\n",
    "                max_date_obj = date_obj\n",
    "                \n",
    "            prediction_files.append((date_obj, file))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not prediction_files:\n",
    "        print(\"‚ùå No prediction files found!\")\n",
    "        return 0\n",
    "    \n",
    "    # Sort by date\n",
    "    prediction_files.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Get cutoff date (12 months ago)\n",
    "    cutoff_date = max_date_obj - timedelta(days=30*12)\n",
    "    \n",
    "    all_hidden_gems = []\n",
    "    \n",
    "    for date_obj, file in prediction_files:\n",
    "        if date_obj < cutoff_date:\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Checking {date_obj.strftime('%Y-%m-%d')}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load predictions\n",
    "            df = pd.read_csv(file)\n",
    "            \n",
    "            # Skip if empty\n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Standardize columns\n",
    "            if 'Album' in df.columns:\n",
    "                df['Album Name'] = df['Album']\n",
    "            elif 'Album Name' in df.columns:\n",
    "                df['Album'] = df['Album Name']\n",
    "            \n",
    "            if 'Artist Name(s)' in df.columns:\n",
    "                df['Artist'] = df['Artist Name(s)']\n",
    "            \n",
    "            if 'Predicted_Score' in df.columns:\n",
    "                df['avg_score'] = df['Predicted_Score']\n",
    "            elif 'avg_score' not in df.columns:\n",
    "                print(f\"    Warning: No score column in {file}\")\n",
    "                continue\n",
    "            \n",
    "            # Check each album\n",
    "            for _, row in df.iterrows():\n",
    "                score = row.get('Predicted_Score', row.get('avg_score', 0))\n",
    "                \n",
    "                if pd.isna(score) or score < 75:\n",
    "                    continue\n",
    "                \n",
    "                artist = row.get('Artist', '')\n",
    "                album_name = row.get('Album', '')\n",
    "                \n",
    "                if not artist or not album_name:\n",
    "                    continue\n",
    "                \n",
    "                history = get_album_history(artist, album_name)\n",
    "                \n",
    "                if history == \"Never Rated\":\n",
    "                    all_hidden_gems.append({\n",
    "                        'Artist': artist,\n",
    "                        'Album': album_name,\n",
    "                        'Predicted_Score': float(score),\n",
    "                        'Source_Week': date_obj.strftime('%Y-%m-%d'),\n",
    "                        'Genres': row.get('Genres', ''),\n",
    "                        'Label': row.get('Label', '')\n",
    "                    })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"    Error loading {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    if all_hidden_gems:\n",
    "        gems_df = pd.DataFrame(all_hidden_gems)\n",
    "        \n",
    "        # Remove duplicates (keep highest score if multiple entries)\n",
    "        gems_df = gems_df.sort_values('Predicted_Score', ascending=False)\n",
    "        gems_df = gems_df.drop_duplicates(subset=['Artist', 'Album'], keep='first')\n",
    "        \n",
    "        # Save to CSV\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        gems_df.to_csv('data/hidden_gems_cache.csv', index=False)\n",
    "        print(f\"‚úÖ Found {len(gems_df)} Hidden Gems! Saved to data/hidden_gems_cache.csv\")\n",
    "        \n",
    "        # Also save a sample of 20 for quick access\n",
    "        if len(gems_df) >= 20:\n",
    "            sample_gems = gems_df.sample(n=20, random_state=42)\n",
    "            sample_gems.to_csv('data/hidden_gems_sample.csv', index=False)\n",
    "            print(f\"üìä Also saved 20 random samples to data/hidden_gems_sample.csv\")\n",
    "        else:\n",
    "            # If less than 20, just save all as sample\n",
    "            gems_df.to_csv('data/hidden_gems_sample.csv', index=False)\n",
    "            print(f\"üìä Saved {len(gems_df)} samples to data/hidden_gems_sample.csv\")\n",
    "    else:\n",
    "        print(\"‚ùå No Hidden Gems found!\")\n",
    "    \n",
    "    return len(all_hidden_gems) if all_hidden_gems else 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    update_hidden_gems()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
